<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PG 读取大量数据因临时空间不足导致任务失败</title>
    <url>/2022/10/30/PG-%E8%AF%BB%E5%8F%96%E5%A4%A7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%9B%A0%E4%B8%B4%E6%97%B6%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%E5%AF%BC%E8%87%B4%E4%BB%BB%E5%8A%A1%E5%A4%B1%E8%B4%A5/</url>
    <content><![CDATA[<p>记录 flinkx 1.11 同步 PG 数据量过亿时，由于生成的同步 SQL 中包含的排序子句，临时空间不够导致任务失败。</p>
<span id="more"></span>

<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>业务方有一个同步 PG 的作业，数据量级大致是 1.5+ 亿条。任务执行失败，报错</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="number">2022</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">14</span>:<span class="number">31</span>:<span class="number">37.178</span> <span class="selector-attr">[flink-akka.actor.default-dispatcher-21]</span> INFO  org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.runtime</span><span class="selector-class">.executiongraph</span><span class="selector-class">.ExecutionGraph</span>  - Source: postgresqlreader (<span class="number">1</span>/<span class="number">1</span>) (<span class="number">77</span>bd8cb9299189456dceee9609b5d03a) switched from RUNNING to FAILED on org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.runtime</span><span class="selector-class">.jobmaster</span><span class="selector-class">.slotpool</span>.SingleLogicalSlot@<span class="number">55</span>bedd88.</span><br><span class="line">java<span class="selector-class">.lang</span><span class="selector-class">.IllegalArgumentException</span>: <span class="built_in">open</span>() failed<span class="selector-class">.ERROR</span>: temporary file size exceeds temp_file_limit (<span class="number">15728640</span>kB)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.rdb</span><span class="selector-class">.inputformat</span><span class="selector-class">.JdbcInputFormat</span><span class="selector-class">.openInternal</span>(JdbcInputFormat<span class="selector-class">.java</span>:<span class="number">154</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.inputformat</span><span class="selector-class">.BaseRichInputFormat</span><span class="selector-class">.open</span>(BaseRichInputFormat<span class="selector-class">.java</span>:<span class="number">183</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.functions</span><span class="selector-class">.source</span><span class="selector-class">.DtInputFormatSourceFunction</span><span class="selector-class">.run</span>(DtInputFormatSourceFunction<span class="selector-class">.java</span>:<span class="number">124</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.StreamSource</span><span class="selector-class">.run</span>(StreamSource<span class="selector-class">.java</span>:<span class="number">100</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.StreamSource</span><span class="selector-class">.run</span>(StreamSource<span class="selector-class">.java</span>:<span class="number">63</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span>.SourceStreamTask<span class="variable">$LegacySourceFunctionThread</span><span class="selector-class">.run</span>(SourceStreamTask<span class="selector-class">.java</span>:<span class="number">201</span>)</span><br><span class="line">Caused by: org<span class="selector-class">.postgresql</span><span class="selector-class">.util</span><span class="selector-class">.PSQLException</span>: ERROR: temporary file size exceeds temp_file_limit (<span class="number">15728640</span>kB)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.core</span><span class="selector-class">.v3</span><span class="selector-class">.QueryExecutorImpl</span><span class="selector-class">.receiveErrorResponse</span>(QueryExecutorImpl<span class="selector-class">.java</span>:<span class="number">2433</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.core</span><span class="selector-class">.v3</span><span class="selector-class">.QueryExecutorImpl</span><span class="selector-class">.processResults</span>(QueryExecutorImpl<span class="selector-class">.java</span>:<span class="number">2178</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.core</span><span class="selector-class">.v3</span><span class="selector-class">.QueryExecutorImpl</span><span class="selector-class">.execute</span>(QueryExecutorImpl<span class="selector-class">.java</span>:<span class="number">306</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeInternal</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">441</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.execute</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">365</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeWithFlags</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">307</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeCachedSql</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">293</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeWithFlags</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">270</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeQuery</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">224</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.rdb</span><span class="selector-class">.inputformat</span><span class="selector-class">.JdbcInputFormat</span><span class="selector-class">.executeQuery</span>(JdbcInputFormat<span class="selector-class">.java</span>:<span class="number">868</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.rdb</span><span class="selector-class">.inputformat</span><span class="selector-class">.JdbcInputFormat</span><span class="selector-class">.openInternal</span>(JdbcInputFormat<span class="selector-class">.java</span>:<span class="number">149</span>)</span><br><span class="line">	... <span class="number">5</span> common frames omitted</span><br></pre></td></tr></table></figure>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>这看起来就是 PG 数据库的问题，需要增加配置<code>temp_file_limit</code>。不过业务说 1.0 上可以执行，不放心用 2.0 的配置在 1.0 上手动提交了一个作业，居然可以。<br>从 2.0 的执行日志中获取了执行的 SQL</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> &quot;ts&quot;,&quot;asm_name&quot;,&quot;sequence&quot;,&quot;process_name&quot;,&quot;thread_name&quot;,&quot;pid&quot;,&quot;tid&quot;,&quot;energy_raw&quot;,&quot;uniform_time_ms&quot;,&quot;curr_time&quot; <span class="keyword">FROM</span> &quot;ctp_metrics27&quot; <span class="keyword">WHERE</span> <span class="number">1</span><span class="operator">=</span><span class="number">1</span>   <span class="keyword">and</span> &quot;curr_time&quot; <span class="operator">&gt;=</span> <span class="string">&#x27;2022-10-27 00:00:00.000000&#x27;</span> <span class="keyword">and</span> &quot;curr_time&quot; <span class="operator">&lt;</span> <span class="string">&#x27;2022-10-28 00:00:00.000000&#x27;</span> <span class="keyword">order</span> <span class="keyword">by</span> curr_time</span><br></pre></td></tr></table></figure>
<p>注意到，这个 sql 的最后多了 <code>order by</code>，这是个多余的操作，而且为了做排序，必定会使用到临时空间，考虑到数据量级，空间不足就是理应会发生的了。<br>看看这个 <code>order by</code> 是在哪里添加的——<br>flinkx 中同步关系型数据库的逻辑，简单的概括就是<code>InputFormat</code>的<code>open</code>方法中创建一个数据库连接，执行 SQL，得到结果集，其中执行的 SQL 的生成分成两步</p>
<ol>
<li>在调用方法<code>com.dtstack.flinkx.rdb.datareader.JdbcDataReader#readData</code>处理配置时，调用<code>com.dtstack.flinkx.rdb.datareader.QuerySqlBuilder#buildSql</code>生成一个 SQL 模板，其中包含了增量&#x2F;恢复、切分条件的占位符</li>
<li>在调用方法<code>com.dtstack.flinkx.rdb.inputformat.JdbcInputFormat#openInternal</code>打开数据库时，调用<code>com.dtstack.flinkx.rdb.inputformat.JdbcInputFormat#buildQuerySql</code>将模板中的占位符替换掉，得到最终的 SQL</li>
</ol>
<p>PG 特殊的地方在于它将父类的方法<code>com.dtstack.flinkx.postgresql.reader.PostgresqlQuerySqlBuilder#buildQuerySql</code>给覆盖掉了，覆盖后的方法与父类的差异多了一行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> String <span class="title function_">buildQuerySql</span><span class="params">()</span>&#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  sb.append(buildOrderSql()); <span class="comment">// 多了这一行</span></span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//com.dtstack.flinkx.rdb.datareader.QuerySqlBuilder#buildOrderSql</span></span><br><span class="line"><span class="keyword">protected</span> String <span class="title function_">buildOrderSql</span><span class="params">()</span>&#123;</span><br><span class="line">  String column;</span><br><span class="line">  <span class="keyword">if</span>(isIncrement)&#123;</span><br><span class="line">    column = incrementColumn;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span>(isRestore)&#123;</span><br><span class="line">    column = restoreColumn;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    column = orderByColumn;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> StringUtils.isEmpty(column) ? <span class="string">&quot;&quot;</span> : String.format(<span class="string">&quot; order by %s&quot;</span>, column);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，当任务是增量任务时，会添加<code>order by 增量字段</code>。</p>
<h2 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h2><p>这段逻辑是没有必要的，去掉即可。</p>
]]></content>
      <tags>
        <tag>flinkx</tag>
        <tag>问题</tag>
      </tags>
  </entry>
  <entry>
    <title>flinkx 1.11 中使用远程 jar 遇到的问题与处理</title>
    <url>/2022/10/30/flinkx-1-11-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%A8%8B-jar-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>记录 flinkx 1.11 验证使用远程 jar 启动任务过程中遇到的问题和处理。</p>
<span id="more"></span>

<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>随着任务不断迁移，datahub api 需要提交的任务也越来越多，带来的问题包括但不限于如下</p>
<ol>
<li>客户端压力大，导致作业提交失败，之前发生过提交子进程内存耗尽</li>
<li>大量任务同时上传文件到 HDFS，导致上传变慢，失败后重试，甚至上传失败</li>
</ol>
<p>之前优化了 jar 重复上传的问题，上周例会上龙哥也做了一件类似的事，将 flink lib 的目录下的 jar 上传到 hdfs，任务使用远程 jar ，这样可以省去本地上传的过程。由于 flinkx 作业几乎不会改动到 flink 的 jar 包，使用远程 jar 可以节省至少一半上传数据量。<br>接下来记录下验证过程中碰到的一个问题</p>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>虽然配置了远程的 jar 但是要把本地的 lib 目录清空了才不会上传。但是清空之后有个问题，任务启动失败报错有个类找不到。挨个尝试，最终发现要在本地 lib 目录中，保留以 Hadoop shaded jar 才行。<br><img src="/2022/10/30/flinkx-1-11-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%A8%8B-jar-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%84%E7%90%86/image_1666956868228_0.png" alt="image.png"><br>之所以会上传是因为 flinkx 在创建 yarndescriptor 时候，将 flink lib 目录中的 jar 添加到 ship files 中了，在执行 start app master 这个方法的时候 会把 ship files 中的文件上传。<br>后面发现添加配置项 <code>yarn.per-job-cluster.include-user-jar : FIRST</code> 后任务可以执行了，带来的变化是<br><img src="/2022/10/30/flinkx-1-11-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%A8%8B-jar-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%84%E7%90%86/image_1666956956724_0.png" alt="image.png"><br>但是这里有个问题，按类路径里的顺序 往后不是也能从 flinkx-release_1.11.0.jar 这个包里找到需要的类么？第二天过来，按照类加载的顺序，看了这两个 jar ：<code>flinkx-hdfs-writer-release_1.11.0.jar</code> 和 <code>flinkx-release_1.11.0.jar</code>，发现问题了</p>
<ol>
<li>不设置 FIRST ，会从 <code>flinkx-hdfs-writer-release_1.11.0.jar</code> 这个 jar 中加载到类，<code>org.apache.commons.cli.Option</code>。 这个类是 commons-cli 1.2 版本的，它的实现中没有 builder 这个方法。</li>
<li>设置了 FIRST，会从 <code>flinkx-release_1.11.0.jar</code> 这个 jar 中加载 Option 类。这个 jar 中的 commons-cli 是 1.3.1 版本的，是有这个方法的。</li>
</ol>
<p>之前为什么会 “挨个尝试，最终发现要在本地 lib 目录中，保留 flink-shaded-hadoop-2-uber-2.6.5-10.0.jar 这个 jar” 也是这个原因，这个 jar 里面的 Option 类是对的。本地 lib 中 jar 会出现在 classpath 的前面，如下，所以会加载到正确的类。</p>
]]></content>
      <tags>
        <tag>flinkx</tag>
      </tags>
  </entry>
  <entry>
    <title>flinkx 1.7 jar 上传逻辑</title>
    <url>/2022/10/30/flinkx-1-7-jar-%E4%B8%8A%E4%BC%A0%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>之前在使用 flinkx 1.7 时，将插件的 jar 在 flink 的 lib 目录中也存放了一份，这样每次提交作业，用不上的 jar 也会上传，记录如何优化这个上传过程。</p>
<span id="more"></span>

<p>flinkx 1.7 任务启动过程中要上传的 jar 分两部分</p>
<ol>
<li>flink lib 目录下的 jar，又细分成两部分 flink-dist 和 非 flink-dist，以及<del>具体的 flinkx 插件 jar 包</del></li>
<li>flinkx 插件目录下的 jar，又细分成 flinkx.jar （对应 flinkx-core 模块）和具体插件的 jar</li>
</ol>
<p>在具体的上传过程中，<br>flink-dist 单独上传</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Path</span> <span class="variable">remotePathJar</span> <span class="operator">=</span> setupSingleLocalResource(</span><br><span class="line">  <span class="string">&quot;flink.jar&quot;</span>,</span><br><span class="line">  fs,</span><br><span class="line">  appId,</span><br><span class="line">  flinkJarPath,</span><br><span class="line">  localResources,</span><br><span class="line">  homeDir,</span><br><span class="line">  <span class="string">&quot;&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>其他的 jar 作业 user jar 一起上传</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (jobGraph != <span class="literal">null</span>) &#123;</span><br><span class="line">  <span class="comment">// add the user code jars from the provided JobGraph</span></span><br><span class="line">  <span class="comment">// 这里所说的 user code jars 就是 flinkx plugin 目录下的 flinkx.jar</span></span><br><span class="line">  <span class="keyword">for</span> (org.apache.flink.core.fs.Path path : jobGraph.getUserJars()) &#123;</span><br><span class="line">    userJarFiles.add(<span class="keyword">new</span> <span class="title class_">File</span>(path.toUri()));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) &#123;</span><br><span class="line">  userClassPaths = uploadAndRegisterFiles(</span><br><span class="line">    userJarFiles,</span><br><span class="line">    fs,</span><br><span class="line">    homeDir,</span><br><span class="line">    appId,</span><br><span class="line">    paths,</span><br><span class="line">    localResources,</span><br><span class="line">    envShipFileList);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  userClassPaths = Collections.emptyList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>userJarFiles</code> 的初始化是在<code>com.dtstack.flinkx.launcher.perjob.PerJobClusterClientBuilder#createPerJobClusterDescriptor</code>，这里将 flink lib 目录下除 flink-dist jar 之外的所有 jar 放到了 <code>userJarFiles</code> 中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;URL&gt; classpaths = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"><span class="keyword">if</span> (flinkJarPath != <span class="literal">null</span>) &#123;</span><br><span class="line">  File[] jars = <span class="keyword">new</span> <span class="title class_">File</span>(flinkJarPath).listFiles();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (File file : jars)&#123;</span><br><span class="line">    <span class="keyword">if</span> (file.toURI().toURL().toString().contains(<span class="string">&quot;flink-dist&quot;</span>))&#123;</span><br><span class="line">      clusterDescriptor.setLocalJarPath(<span class="keyword">new</span> <span class="title class_">Path</span>(file.toURI().toURL().toString()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      classpaths.add(file.toURI().toURL());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;The Flink jar path is null&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">clusterDescriptor.setProvidedUserJarFiles(classpaths);</span><br></pre></td></tr></table></figure>

<p>这个过程中存在问题是，所有的插件 jar 在 flink lib 目录下都存在了一份，提交的作业时不管有没有使用到都会上传。</p>
<p>理想的状态是，任务使用了哪些插件，就上传这些插件对应的 jar。</p>
<p>任务使用了哪些插件，可以从任务配置文件中解析得到。这个解析过程，在<code>com.dtstack.flinkx.launcher.Launcher#analyzeUserClasspath</code>方法中已经完成，保存在<code>clusterSpecification.classpaths</code> 中。并且在后续获取 <code>PackagedProgram</code> 和 <code>JobGraph</code> 对象时，又保存到了各自对象的 <code>classpaths</code> 变量中。</p>
<p>一个处理的方法是：</p>
<ol>
<li>将插件包从 flink lib 目录下删除</li>
<li>上面上传 <code>userJarFiles</code> 表示的 jar 包之前，从 <code>jobGraph</code> 中获取了 <code>flinkx.jar</code>，可以做一个类似的操作，即：将 <code>jobGraph.classpath</code> 中的 jar （就是任务需要的插件包）一并添加到 <code>userJarFiles</code> 中，待后续上传。</li>
<li>还有个方法，同事给出的，是在 <code>com.dtstack.flinkx.launcher.perjob.PerJobClusterClientBuilder#createPerJobClusterDescriptor</code>方法中设置 classpaths 时，从 clusterSpefication 中获取解析出的插件包添加到 classpaths 中。</li>
</ol>
<p>如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;URL&gt; classpaths = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">clusterSpecification.getClasspaths().forEach(jar-&gt;classpaths.add(jar));</span><br><span class="line"><span class="keyword">if</span> (flinkJarPath != <span class="literal">null</span>) &#123;</span><br><span class="line">  File[] jars = <span class="keyword">new</span> <span class="title class_">File</span>(flinkJarPath).listFiles();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (File file : jars)&#123;</span><br><span class="line">    <span class="keyword">if</span> (file.toURI().toURL().toString().contains(<span class="string">&quot;flink-dist&quot;</span>))&#123;</span><br><span class="line">      clusterDescriptor.setLocalJarPath(<span class="keyword">new</span> <span class="title class_">Path</span>(file.toURI().toURL().toString()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      classpaths.add(file.toURI().toURL());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;The Flink jar path is null&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">clusterDescriptor.setProvidedUserJarFiles(classpaths);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>flinkx</tag>
      </tags>
  </entry>
  <entry>
    <title>hive 读取超时</title>
    <url>/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/</url>
    <content><![CDATA[<p>介绍 JDBC 连接 Hive Server 的大致过程，以及如何设置读取超时。</p>
<span id="more"></span>

<p>业务反馈 hive 读取作业执行失败<br><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220707143134989_1657247857902_0.png" alt="image-20220707143134989.png"><br>查看了这些失败作业，发现有些许作业是读超时了，异常栈如下</p>
<pre><code>Caused by: java.net.SocketTimeoutException: Read timed out
at java.net.SocketInputStream.socketRead0(Native Method)
at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
at java.net.SocketInputStream.read(SocketInputStream.java:171)
at java.net.SocketInputStream.read(SocketInputStream.java:141)
at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
... 30 more
</code></pre>
<p>方法<code>java.net.SocketInputStream#socketRead0</code>的定义是</p>
<figure class="highlight aspectj"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">native</span> <span class="function"><span class="keyword">int</span> <span class="title">socketRead0</span><span class="params">(FileDescriptor fd,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">byte</span> b[], <span class="keyword">int</span> off, <span class="keyword">int</span> len,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">int</span> timeout)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException</span>;</span><br></pre></td></tr></table></figure>
<p>其中，参数<code>timeout</code>的定义是<code>the read timeout in ms</code>。<code>timeout</code>在 141 行调用，由下面的方法传入，即<code>impl.getTimeout()</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">read</span><span class="params">(<span class="type">byte</span> b[], <span class="type">int</span> off, <span class="type">int</span> length)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="keyword">return</span> read(b, off, length, impl.getTimeout());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里<code>impl</code>是类<code>AbstractPlainSocketImpl</code>的一个实例，方法<code>getTimeout()</code>返回成员变量<code>timeout</code>，该成员在方法<code>java.net.AbstractPlainSocketImpl#setOption</code>中设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setOption</span><span class="params">(<span class="type">int</span> opt, Object val)</span> <span class="keyword">throws</span> SocketException &#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  <span class="keyword">case</span> SO_TIMEOUT:</span><br><span class="line">  <span class="keyword">if</span> (val == <span class="literal">null</span> || (!(val <span class="keyword">instanceof</span> Integer)))</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">SocketException</span>(<span class="string">&quot;Bad parameter for SO_TIMEOUT&quot;</span>);</span><br><span class="line">  <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> ((Integer) val).intValue();</span><br><span class="line">  <span class="keyword">if</span> (tmp &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(<span class="string">&quot;timeout &lt; 0&quot;</span>);</span><br><span class="line">  timeout = tmp;</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法有 3 处调用<br><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220708105432620_1657264951019_0.png" alt="image-20220708105432620.png"><br>如何知道哪个调用是与 hive 有关系呢？不太好找，因为有太多的调用方了。换个思路，从 hive 入手，知道肯定会执行到这里，所以打个断点，当执行到断点时，查看调用栈，就知道执行过程了。<br><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220708144551916_1657264974671_0.png" alt="image-20220708144551916.png"><br>这里大致流程是<br><code>com.dtstack.flinkx.hive.format.HiveInputFormat#openInternal</code>拿到查询 SQL<code>querySql</code>并执行查询<br>查询是通过方法<code>com.dtstack.flinkx.hive.format.HiveInputFormat#executeQuery</code>实现，首先获取数据库连接<code>dbConn</code><br>数据库连接通过方法<code>com.dtstack.flinkx.rdb.inputformat.JdbcInputFormat#getConnection</code> 获取，内部有个重试，调用<code>DriverManager.getConnection()</code><br><code>DriverManager</code>内部遍历注册的 drivers ，并连接，见方法<code>java.sql.DriverManager#getConnection(java.lang.String, java.util.Properties, java.lang.Class&lt;?&gt;)</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(DriverInfo aDriver : registeredDrivers) &#123;</span><br><span class="line">  <span class="comment">// If the caller does not have permission to load the driver then</span></span><br><span class="line">  <span class="comment">// skip it.</span></span><br><span class="line">  <span class="keyword">if</span>(isDriverAllowed(aDriver.driver, callerCL)) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      println(<span class="string">&quot;    trying &quot;</span> + aDriver.driver.getClass().getName());</span><br><span class="line">      <span class="type">Connection</span> <span class="variable">con</span> <span class="operator">=</span> aDriver.driver.connect(url, info); <span class="comment">//连接</span></span><br><span class="line">      <span class="keyword">if</span> (con != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Success!</span></span><br><span class="line">        println(<span class="string">&quot;getConnection returning &quot;</span> + aDriver.driver.getClass().getName());</span><br><span class="line">        <span class="keyword">return</span> (con);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SQLException ex) &#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    println(<span class="string">&quot;    skipping: &quot;</span> + aDriver.getClass().getName());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Hive 场景下，注册的 driver 是 <code>HiveDriver</code>，connect 方法中就是创建一个<code>HiveConnection</code> 对象</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Connection <span class="title function_">connect</span><span class="params">(String url, Properties info)</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  <span class="keyword">return</span> acceptsURL(url) ? <span class="keyword">new</span> <span class="title class_">HiveConnection</span>(url, info) : <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对象<code>HiveConnection</code>的构造方法如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">HiveConnection</span><span class="params">(String uri, Properties info)</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  setupLoginTimeout(); </span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    connParams = Utils.parseURL(uri);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ZooKeeperHiveClientException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">SQLException</span>(e);</span><br><span class="line">  &#125;</span><br><span class="line">  jdbcUriString = connParams.getJdbcUriString();</span><br><span class="line">  <span class="comment">// extract parsed connection parameters:</span></span><br><span class="line">  <span class="comment">// JDBC URL: jdbc:hive2://&lt;host&gt;:&lt;port&gt;/dbName;sess_var_list?hive_conf_list#hive_var_list</span></span><br><span class="line">  <span class="comment">// each list: &lt;key1&gt;=&lt;val1&gt;;&lt;key2&gt;=&lt;val2&gt; and so on</span></span><br><span class="line">  <span class="comment">// sess_var_list -&gt; sessConfMap</span></span><br><span class="line">  <span class="comment">// hive_conf_list -&gt; hiveConfMap</span></span><br><span class="line">  <span class="comment">// hive_var_list -&gt; hiveVarMap</span></span><br><span class="line">  host = connParams.getHost();</span><br><span class="line">  port = connParams.getPort();</span><br><span class="line">  sessConfMap = connParams.getSessionVars();</span><br><span class="line">  hiveConfMap = connParams.getHiveConfs();</span><br><span class="line"></span><br><span class="line">  hiveVarMap = connParams.getHiveVars();</span><br><span class="line">  <span class="keyword">for</span> (Map.Entry&lt;Object, Object&gt; kv : info.entrySet()) &#123;</span><br><span class="line">    <span class="keyword">if</span> ((kv.getKey() <span class="keyword">instanceof</span> String)) &#123;</span><br><span class="line">      <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> (String) kv.getKey();</span><br><span class="line">      <span class="keyword">if</span> (key.startsWith(HIVE_VAR_PREFIX)) &#123;</span><br><span class="line">        hiveVarMap.put(key.substring(HIVE_VAR_PREFIX.length()), info.getProperty(key));</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.startsWith(HIVE_CONF_PREFIX)) &#123;</span><br><span class="line">        hiveConfMap.put(key.substring(HIVE_CONF_PREFIX.length()), info.getProperty(key));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  isEmbeddedMode = connParams.isEmbeddedMode();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isEmbeddedMode) &#123;</span><br><span class="line">    <span class="type">EmbeddedThriftBinaryCLIService</span> <span class="variable">embeddedClient</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">EmbeddedThriftBinaryCLIService</span>();</span><br><span class="line">    embeddedClient.init(<span class="keyword">new</span> <span class="title class_">HiveConf</span>());</span><br><span class="line">    client = embeddedClient;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// extract user/password from JDBC connection properties if its not supplied in the</span></span><br><span class="line">    <span class="comment">// connection URL</span></span><br><span class="line">    <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_USER)) &#123;</span><br><span class="line">      sessConfMap.put(JdbcConnectionParams.AUTH_USER, info.getProperty(JdbcConnectionParams.AUTH_USER));</span><br><span class="line">      <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_PASSWD)) &#123;</span><br><span class="line">        sessConfMap.put(JdbcConnectionParams.AUTH_PASSWD, info.getProperty(JdbcConnectionParams.AUTH_PASSWD));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_TYPE)) &#123;</span><br><span class="line">      sessConfMap.put(JdbcConnectionParams.AUTH_TYPE, info.getProperty(JdbcConnectionParams.AUTH_TYPE));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// open the client transport</span></span><br><span class="line">    openTransport(); <span class="comment">// 从这里开始</span></span><br><span class="line">    <span class="comment">// set up the client</span></span><br><span class="line">    client = <span class="keyword">new</span> <span class="title class_">TCLIService</span>.Client(<span class="keyword">new</span> <span class="title class_">TBinaryProtocol</span>(transport));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// add supported protocols</span></span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V1);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V2);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V3);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V4);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V5);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V6);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V7);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// open client session</span></span><br><span class="line">  openSession();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Wrap the client with a thread-safe proxy to serialize the RPC calls</span></span><br><span class="line">  client = newSynchronizedClient(client);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用方法<code>setupLoginTimeout()</code>设置<code>loginTimeout</code>，内部获取的是<code>java.sql.DriverManager#loginTimeout</code>，这个变量是类的 static 成员，就是参考资料中提到的多个 JDBC driver 驱动存在时，会被覆盖导致问题<br>调用<code>org.apache.hive.jdbc.Utils#parseURL</code>解析 jdbc url 连接， <code>jdbc:hive2://&lt;host1&gt;:&lt;port1&gt;,&lt;host2&gt;:&lt;port2&gt;/dbName;sess_var_list?hive_conf_list#hive_var_list</code>，其中几个 list 是以<code>;</code>分隔的键值对，分别对应 URI 的 path、query和fragment，内部也是用 URI 类来提取的各个部分的<br>调用方法<code>org.apache.hive.jdbc.HiveConnection#openTransport</code>打开客户端连接，判断是个二进制协议还是 http 协议，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">openTransport</span><span class="params">()</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  <span class="type">int</span> <span class="variable">numRetries</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> <span class="variable">maxRetries</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    maxRetries = Integer.parseInt(sessConfMap.get(JdbcConnectionParams.RETRIES));</span><br><span class="line">  &#125; <span class="keyword">catch</span>(NumberFormatException e) &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      assumeSubject =</span><br><span class="line">        JdbcConnectionParams.AUTH_KERBEROS_AUTH_TYPE_FROM_SUBJECT.equals(sessConfMap</span><br><span class="line">                                                                         .get(JdbcConnectionParams.AUTH_KERBEROS_AUTH_TYPE));</span><br><span class="line">      transport = isHttpTransportMode() ? createHttpTransport() : createBinaryTransport(); <span class="comment">//判断是个二进制协议还是 http 协议，</span></span><br><span class="line">      <span class="keyword">if</span> (!transport.isOpen()) &#123;</span><br><span class="line">        transport.open();</span><br><span class="line">      &#125;</span><br><span class="line">      logZkDiscoveryMessage(<span class="string">&quot;Connected to &quot;</span> + connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (TTransportException e) &#123;</span><br><span class="line">      <span class="comment">// We&#x27;ll retry till we exhaust all HiveServer2 nodes from ZooKeeper</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里是二进制，调用方法<code>org.apache.hive.jdbc.HiveConnection#createBinaryTransport</code>，内部调用<code>org.apache.hive.jdbc.HiveConnection#createUnderlyingTransport</code>创建底层的 transport</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> TTransport <span class="title function_">createUnderlyingTransport</span><span class="params">()</span> <span class="keyword">throws</span> TTransportException &#123;</span><br><span class="line">  <span class="type">TTransport</span> <span class="variable">transport</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">  <span class="comment">// Note: Thrift returns an SSL socket that is already bound to the specified host:port</span></span><br><span class="line">  <span class="comment">// Therefore an open called on this would be a no-op later</span></span><br><span class="line">  <span class="comment">// Hence, any TTransportException related to connecting with the peer are thrown here.</span></span><br><span class="line">  <span class="comment">// Bubbling them up the call hierarchy so that a retry can happen in openTransport,</span></span><br><span class="line">  <span class="comment">// if dynamic service discovery is configured.</span></span><br><span class="line">  <span class="keyword">if</span> (isSslConnection()) &#123;</span><br><span class="line">    <span class="comment">// get SSL socket</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// get non-SSL socket transport</span></span><br><span class="line">    transport = HiveAuthUtils.getSocketTransport(host, port, loginTimeout);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> transport;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里获取 transport 的时候，传了<code>loginTimeout</code><br>方法<code>org.apache.hadoop.hive.common.auth.HiveAuthUtils#getSocketTransport</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> TTransport <span class="title function_">getSocketTransport</span><span class="params">(String host, <span class="type">int</span> port, <span class="type">int</span> loginTimeout)</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TSocket</span>(host, port, loginTimeout);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>继续看 <code>TSocket</code> 的初始化</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">TSocket</span><span class="params">(String host, <span class="type">int</span> port, <span class="type">int</span> timeout)</span> &#123;</span><br><span class="line">  <span class="built_in">this</span>.socket_ = <span class="literal">null</span>;</span><br><span class="line">  <span class="built_in">this</span>.host_ = <span class="literal">null</span>;</span><br><span class="line">  <span class="built_in">this</span>.port_ = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">this</span>.timeout_ = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">this</span>.host_ = host;</span><br><span class="line">  <span class="built_in">this</span>.port_ = port;</span><br><span class="line">  <span class="built_in">this</span>.timeout_ = timeout;</span><br><span class="line">  <span class="built_in">this</span>.initSocket(); <span class="comment">//</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">initSocket</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="built_in">this</span>.socket_ = <span class="keyword">new</span> <span class="title class_">Socket</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.socket_.setSoLinger(<span class="literal">false</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">this</span>.socket_.setTcpNoDelay(<span class="literal">true</span>);</span><br><span class="line">    <span class="built_in">this</span>.socket_.setKeepAlive(<span class="literal">true</span>);</span><br><span class="line">    <span class="built_in">this</span>.socket_.setSoTimeout(<span class="built_in">this</span>.timeout_); <span class="comment">//</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> (SocketException var2) &#123;</span><br><span class="line">    LOGGER.error(<span class="string">&quot;Could not configure socket.&quot;</span>, var2);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>java.net.Socket#setSoTimeout</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">setSoTimeout</span><span class="params">(<span class="type">int</span> timeout)</span> <span class="keyword">throws</span> SocketException &#123;</span><br><span class="line">  <span class="keyword">if</span> (isClosed())</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">SocketException</span>(<span class="string">&quot;Socket is closed&quot;</span>);</span><br><span class="line">  <span class="keyword">if</span> (timeout &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(<span class="string">&quot;timeout can&#x27;t be negative&quot;</span>);</span><br><span class="line"></span><br><span class="line">  getImpl().setOption(SocketOptions.SO_TIMEOUT, <span class="keyword">new</span> <span class="title class_">Integer</span>(timeout)); <span class="comment">//</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里设置的是 socket 的读取超时，实际上 timeout_，也用于 socket 连接超时<br>最后看下，<code>java.sql.DriverManager#loginTimeout</code>是如何设置的，类似的，添加断点，发现是在方法<code>com.dtstack.flinkx.util.ClassUtil#forName(java.lang.String, java.lang.ClassLoader)</code>中设置，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">forName</span><span class="params">(String clazz, ClassLoader classLoader)</span>  &#123;</span><br><span class="line">  <span class="keyword">synchronized</span> (LOCK_STR)&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Class.forName(clazz, <span class="literal">true</span>, classLoader);</span><br><span class="line">      DriverManager.setLoginTimeout(<span class="number">10</span>); <span class="comment">//10s</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220708110105135_1657265225155_0.png" alt="image-20220708110105135.png"></p>
]]></content>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>mongo 任务执行成功但数据量不对</title>
    <url>/2022/10/30/mongo-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E6%88%90%E5%8A%9F%E4%BD%86%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B8%8D%E5%AF%B9/</url>
    <content><![CDATA[<p>记录 flinkx 中 mongo 同步任务由于错误设置问题导致的任务应当失败而未失败的问题。</p>
<span id="more"></span>

<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>业务反馈了一个同步 hive 数据到 mongo 的作业，同步结束后，mongo 里面的数据量与 hive 的对不上，任务运行日志中有 118 条记录写失败了。</p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>初看这个任务有一点奇怪：flinkx 中有一个数据量的检测机制，如果写失败了，错误记录数或者错误记录比例超过一定阈值，作业就失败了。这个作业有记录写失败了，为什么还是成功的呢？带着这个疑惑，重新看了下错误检测机制，分为两个部分：初始化和校验</p>
<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>在<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#open</code> 内部会调用方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#initStatisticsAccumulator</code>初始化累加器（比如错误记录数 <code>errCounter</code>），具体是通过 flink runtime 创建计数器，并将这些计数器添加到指标组中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">initStatisticsAccumulator</span><span class="params">()</span>&#123;</span><br><span class="line">  errCounter = context.getLongCounter(Metrics.NUM_ERRORS);</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  outputMetric = <span class="keyword">new</span> <span class="title class_">BaseMetric</span>(context);</span><br><span class="line">  outputMetric.addMetric(Metrics.NUM_ERRORS, errCounter);</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  startTime = System.currentTimeMillis();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后调用方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#initAccumulatorCollector</code>初始话计数器收集器<code>accumulatorCollector</code>，内部会启动一个定时任务，执行方法<code>com.dtstack.flinkx.metrics.AccumulatorCollector#collectAccumulatorWithApi</code>从 JM 获取计数器的值。</p>
<h2 id="校验"><a href="#校验" class="headerlink" title="校验"></a>校验</h2><p>校验过程有两处：每一行数据写之前；以及所有数据写完。<br>写之前是在方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#writeSingleRecord</code>中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">writeSingleRecord</span><span class="params">(Row row)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span>(errorLimiter != <span class="literal">null</span>) &#123;</span><br><span class="line">    errorLimiter.acquire();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    writeSingleRecordInternal(row);</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里先调用方法<code>com.dtstack.flinkx.writer.ErrorLimiter#acquire</code>获取错误的情况，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">acquire</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">String</span> <span class="variable">errorDataStr</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">  <span class="keyword">if</span>(errorData != <span class="literal">null</span>)&#123;</span><br><span class="line">    errorDataStr = errorData.toString() + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">long</span> <span class="variable">errors</span> <span class="operator">=</span> accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS);</span><br><span class="line">  <span class="keyword">if</span>(maxErrors != <span class="literal">null</span> &amp;&amp; !maxErrors.equals(<span class="number">0</span>))&#123;</span><br><span class="line">    Preconditions.checkArgument(errors &lt;= maxErrors, <span class="string">&quot;WritingRecordError: error writing record [&quot;</span> + errors + <span class="string">&quot;] exceed limit [&quot;</span> + maxErrors</span><br><span class="line">                                + <span class="string">&quot;]\n&quot;</span> + errorDataStr + errMsg);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span>(maxErrorRatio != <span class="literal">null</span>)&#123;</span><br><span class="line">    <span class="type">long</span> <span class="variable">numRead</span> <span class="operator">=</span> accumulatorCollector.getAccumulatorValue(Metrics.NUM_READS);</span><br><span class="line">    <span class="keyword">if</span>(numRead &gt;= <span class="number">1</span>) &#123;</span><br><span class="line">      errorRatio = (<span class="type">double</span>) errors / numRead;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Preconditions.checkArgument(errorRatio &lt;= maxErrorRatio, <span class="string">&quot;WritingRecordError: error writing record ratio [&quot;</span> + errorRatio + <span class="string">&quot;] exceed limit [&quot;</span> + maxErrorRatio</span><br><span class="line">                                + <span class="string">&quot;]\n&quot;</span> + errorDataStr + errMsg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从计数器收集器<code>accumulatorCollector</code>中获取错误数<code>errors</code>和错误比例（错误数&#x2F;读取数）<code>errorRatio = (**double**) errors / numRead</code>。如果超过最大错误数<code>maxErrors</code>或者最大错误比例<code>maxErrorRatio</code>，抛出异常。<br>由于上面的检测是在写之前，对于最后一次写，还是有可能失败的，所以写完之后，又有一次检测，也是调用方法<code>com.dtstack.flinkx.writer.ErrorLimiter#acquire</code>。具体是在方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#checkErrorLimit</code>中，由<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#close</code>方法调用。<br>至此，错误检测机制说明白了。那为什么没有生效呢？可能的原因就是检测条件不成立，即<code>maxErrors</code>为 null 或者 0；<code>maxErrorRatio</code> 为 null。顺着这个思路找到这两个值初始化的源头，<code>com.dtstack.flinkx.mongodb.writer.MongodbWriter#writeData</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> DataStreamSink&lt;?&gt; writeData(List&lt;DataStream&lt;Row&gt;&gt; dataSets) &#123;</span><br><span class="line">  <span class="type">MongodbOutputFormatBuilder</span> <span class="variable">builder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MongodbOutputFormatBuilder</span>();</span><br><span class="line"></span><br><span class="line">  builder.setMongodbConfig(mongodbConfig);</span><br><span class="line">  builder.setColumns(columns);</span><br><span class="line">  builder.setMonitorUrls(monitorUrls);</span><br><span class="line">  builder.setErrors(errors);</span><br><span class="line">  builder.setDirtyPath(dirtyPath);</span><br><span class="line">  builder.setDirtyHadoopConfig(dirtyHadoopConfig);</span><br><span class="line">  builder.setSrcCols(srcCols);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> createOutput(dataSets, builder.finish());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看到这里处理了 <code>errors</code>，没有处理<code>errorRatio</code>。而<code>errors</code>是从配置中获取的，取值为 0。如此两个检测条件都不成立，所以任务没有失败。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;errorLimit&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;percentage&quot;</span><span class="punctuation">:</span> <span class="number">0.01</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;record&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<h1 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h1><p>在方法<code>com.dtstack.flinkx.mongodb.writer.MongodbWriter#writeData</code>中设置<code>errorRatio</code> 或者在创建配置时将<code>errorLimit.record</code>设置为 1。</p>
]]></content>
      <tags>
        <tag>flinkx</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>连不上 HS2</title>
    <url>/2022/10/30/%E8%BF%9E%E4%B8%8D%E4%B8%8A-HS2/</url>
    <content><![CDATA[<p>介绍 JDBC 连接 Hive Server 过程中，通过 ZK 实现高可用的逻辑。</p>
<span id="more"></span>

<p>作业执行失败报错如下  </p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">Tried <span class="attribute">all</span> existing HiveServer2 uris <span class="selector-tag">from</span> ZooKeeper.</span><br></pre></td></tr></table></figure>

<p>目前 HS2 通过 ZK 做了 HA，从报错日志看，将 ZK 里面配置的 HS2 依次连接了一遍，都失败了。过往的经验是有两个可能的问题</p>
<p>jdbc 连接串（ <code>jdbc:hive2://10.1.183.246,10.1.178.113,10.1.166.18:2181/usercenter;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sgp -com-hive;hive.server2.proxy.user=sg_ib_dw?mapred.job.queue.name=root.ib.sgoffline</code> ）有问题<br>HS2 地址中使用的是主机名，从主机名解析获取 IP 地址失败</p>
<p>第一个问题通过 beeline 连接给定的 jdbc 连接串验证，可以连上，说明不是这个问题；  </p>
<p>第二个问题可以从 ZK 中获取 HS2 地址，通过 ping 或者 telnet 命令验证，验证通过，说明也不是这个问题  </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取 HS2 地址</span></span><br><span class="line">[zk: 10.1.166.18:2181(CONNECTED) 0] ls /sgp-com-hive</span><br><span class="line">[serverUri=10-94-12-217.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn-1;sequence=0000001812, serverUri=10-94-104-44.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn-1;sequence=0000001795, serverUri=10-94-106-212.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn-1;sequence=0000001771]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">telnet 验证</span></span><br><span class="line">[service@10-94-48-163.datahub-api.sgp lib]$ telnet 10-94-12-217.sgp-com-hiveserver2.sgp 10000</span><br><span class="line">Trying 10.94.12.217...</span><br><span class="line">Connected to 10-94-12-217.sgp-com-hiveserver2.sgp.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">^]</span><br><span class="line"><span class="meta prompt_">telnet&gt; </span><span class="language-bash">quit</span></span><br><span class="line">Connection closed.</span><br></pre></td></tr></table></figure>

<p>接下来，看下 hive jdbc 是如何创建连接的，从异常栈中是看到相关的类是  <code>org.apache.hive.jdbc.HiveConnection#HiveConnection</code> ，之前在hive-读取超时中有描述 jdbc 打开 hive 连接的大致流程，接下来看下其中遍历 ZK 中配置的 HS2，依次尝试连接，直到成功或者失败的过程。  </p>
<p>从方法 <code>org.apache.hive.jdbc.Utils#parseURL</code> 开始解析 jdbc url 连接， <code>jdbc:hive2://&lt;host1&gt;:&lt;port1&gt;,&lt;host2&gt;:&lt;port2&gt;/dbName;sess_var_list?hive_conf_list#hive_var_list</code> ，其中几个 list 是以 <code>;</code> 分隔的键值对，分别对应 URI 的 path、query和fragment，内部也是用 URI 类来提取的各个部分的。最后会调用 <code>org.apache.hive.jdbc.Utils#configureConnParams</code>  处理 <code>connParams</code> ，从 ZK 中获取真实的主机和端口，并替换 <code>dummyAuthorityString</code> 。  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> JdbcConnectionParams <span class="title function_">parseURL</span><span class="params">(String uri)</span> <span class="keyword">throws</span> JdbcUriParseException,</span><br><span class="line">SQLException, ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="type">JdbcConnectionParams</span> <span class="variable">connParams</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JdbcConnectionParams</span>();</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="comment">// Extract host, port</span></span><br><span class="line">  <span class="keyword">if</span> (connParams.isEmbeddedMode()) &#123;</span><br><span class="line">    <span class="comment">// In case of embedded mode we were supplied with an empty authority.</span></span><br><span class="line">    <span class="comment">// So we never substituted the authority with a dummy one.</span></span><br><span class="line">    connParams.setHost(jdbcURI.getHost());</span><br><span class="line">    connParams.setPort(jdbcURI.getPort());</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Configure host, port and params from ZooKeeper if used,</span></span><br><span class="line">    <span class="comment">// and substitute the dummy authority with a resolved one</span></span><br><span class="line">    configureConnParams(connParams); <span class="comment">//就是这里</span></span><br><span class="line">    <span class="comment">// We check for invalid host, port while configuring connParams with configureConnParams()</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">authorityStr</span> <span class="operator">=</span> connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort();</span><br><span class="line">    LOG.info(<span class="string">&quot;Resolved authority: &quot;</span> + authorityStr);</span><br><span class="line">    uri = uri.replace(dummyAuthorityString, authorityStr);</span><br><span class="line">    connParams.setJdbcUriString(uri);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> connParams;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法 <code>org.apache.hive.jdbc.Utils#configureConnParams</code> 内部会判断服务发现模式是不是 <code>zooKeeper</code> （在 jdbc url 的 session var 中有给出，即： <code>serviceDiscoveryMode=zooKeeper;</code> ），执行 <code>org.apache.hive.jdbc.ZooKeeperHiveClientHelper#configureConnParams</code> 从 ZK 获取配置  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">configureConnParams</span><span class="params">(JdbcConnectionParams connParams)</span></span><br><span class="line">  <span class="keyword">throws</span> JdbcUriParseException, ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="type">String</span> <span class="variable">serviceDiscoveryMode</span> <span class="operator">=</span></span><br><span class="line">    connParams.getSessionVars().get(JdbcConnectionParams.SERVICE_DISCOVERY_MODE);</span><br><span class="line">  <span class="keyword">if</span> ((serviceDiscoveryMode != <span class="literal">null</span>)</span><br><span class="line">      &amp;&amp; (JdbcConnectionParams.SERVICE_DISCOVERY_MODE_ZOOKEEPER</span><br><span class="line">          .equalsIgnoreCase(serviceDiscoveryMode))) &#123;</span><br><span class="line">    <span class="comment">// Set ZooKeeper ensemble in connParams for later use</span></span><br><span class="line">    connParams.setZooKeeperEnsemble(joinStringArray(connParams.getAuthorityList(), <span class="string">&quot;,&quot;</span>));</span><br><span class="line">    <span class="comment">// Configure using ZooKeeper</span></span><br><span class="line">    ZooKeeperHiveClientHelper.configureConnParams(connParams);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看方法 <code>org.apache.hive.jdbc.ZooKeeperHiveClientHelper#configureConnParams</code> 内部实现<br>从指定空间（在 jdbc url 的 session var 中指定：即 <code>zooKeeperNamespace=sgp-com-hive</code> ）中获取 HS2 列表 <code>serverHosts</code><br>将 <code>serverHosts</code> 中已经尝试连接过的 HS2 剔除掉（连接失败的 HS2 会记录到^^拒绝连接列表^^中，下文中有描述）<br>如果没有可用的 HS2，就抛出前面任务失败的异常 <code>Tried all existing HiveServer2 uris from ZooKeeper.</code><br>从可用的 HS2 随机选择一个，获取对应 znode 内容  <code>serverConfStr</code> ，即对应 HS2 的配置：</p>
<figure class="highlight subunit"><table><tr><td class="code"><pre><span class="line">[zk: 10.1.166.18:2181(CONNECTED) 0] ls /sgp-com-hive</span><br><span class="line">[serverUri=10<span class="string">-94</span><span class="string">-12</span><span class="string">-217</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001812, serverUri=10<span class="string">-94</span><span class="string">-104</span><span class="string">-44</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001795, serverUri=10<span class="string">-94</span><span class="string">-106</span><span class="string">-212</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001771]</span><br><span class="line">[zk: 10.1.166.18:2181(CONNECTED) 1] get /sgp-com-hive/serverUri=10<span class="string">-94</span><span class="string">-12</span><span class="string">-217</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001812</span><br><span class="line">hive.server2.authentication=CUSTOM;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=10<span class="string">-94</span><span class="string">-12</span><span class="string">-217</span>.sgp-com-hiveserver2.sgp;hive.server2.thrift.port=10000;hive.server2.use.SSL=false</span><br></pre></td></tr></table></figure>

<p>重新整理后如下  </p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">hive.server2.authentication</span>=CUSTOM<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.transport.mode</span>=binary<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.thrift.sasl.qop</span>=auth<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.thrift.bind.host</span>=<span class="number">10</span>-<span class="number">94</span>-<span class="number">12</span>-<span class="number">217</span>.sgp-com-hiveserver2.sgp<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.thrift.port</span>=<span class="number">10000</span><span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.use.SSL</span>=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>调用方法 <code>applyConfs</code>  应用配置 <code>serverConfStr</code> ，即将上面的配置依次设置到 <code>connParams</code> 中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">configureConnParams</span><span class="params">(JdbcConnectionParams connParams)</span></span><br><span class="line">  <span class="keyword">throws</span> ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="comment">//省略...</span></span><br><span class="line">  serverHosts = zooKeeperClient.getChildren().forPath(<span class="string">&quot;/&quot;</span> + zooKeeperNamespace);</span><br><span class="line">  <span class="comment">// Remove the znodes we&#x27;ve already tried from this list</span></span><br><span class="line">  serverHosts.removeAll(connParams.getRejectedHostZnodePaths());</span><br><span class="line">  <span class="keyword">if</span> (serverHosts.isEmpty()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ZooKeeperHiveClientException</span>(</span><br><span class="line">      <span class="string">&quot;Tried all existing HiveServer2 uris from ZooKeeper.&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Now pick a server node randomly</span></span><br><span class="line">  serverNode = serverHosts.get(randomizer.nextInt(serverHosts.size()));</span><br><span class="line">  connParams.setCurrentHostZnodePath(serverNode);</span><br><span class="line">  <span class="comment">// Read config string from the znode for this server node</span></span><br><span class="line">  <span class="type">String</span> <span class="variable">serverConfStr</span> <span class="operator">=</span></span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">String</span>(</span><br><span class="line">    zooKeeperClient.getData().forPath(<span class="string">&quot;/&quot;</span> + zooKeeperNamespace + <span class="string">&quot;/&quot;</span> + serverNode),</span><br><span class="line">    Charset.forName(<span class="string">&quot;UTF-8&quot;</span>));</span><br><span class="line">  applyConfs(serverConfStr, connParams);</span><br><span class="line">  <span class="comment">//省略 ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>支持连接参数 <code>connParams</code> 初始化完成，接下来调用方法 <code>org.apache.hive.jdbc.HiveConnection#openTransport</code> 打开与 ThriftServer 的连接，在一个 while true 的循环中尝试连接，处理异常。看下其中处理打开异常的部分：   </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">openTransport</span><span class="params">()</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      transport = isHttpTransportMode() ? createHttpTransport() : createBinaryTransport();</span><br><span class="line">      <span class="keyword">if</span> (!transport.isOpen()) &#123;</span><br><span class="line">        transport.open();</span><br><span class="line">      &#125;</span><br><span class="line">      logZkDiscoveryMessage(<span class="string">&quot;Connected to &quot;</span> + connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (TTransportException e) &#123;</span><br><span class="line">      <span class="comment">// We&#x27;ll retry till we exhaust all HiveServer2 nodes from ZooKeeper</span></span><br><span class="line">      <span class="keyword">if</span> (isZkDynamicDiscoveryMode()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          Utils.updateConnParamsFromZooKeeper(connParams); <span class="comment">//这里</span></span><br><span class="line">          <span class="comment">//...</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在通过 ZK 做动态发现模式下（方法 <code>isZkDynamicDiscoveryMode()</code> ）,调用方法 <code>org.apache.hive.jdbc.Utils#updateConnParamsFromZooKeeper</code> 重新获取一个 HS2<br>首先，将当前尝试连接的节点节点添加到^^拒绝节点列表^^中<br>暂存当前使用 HS2 的 host 和 port<br>调用前面介绍的方法 <code>org.apache.hive.jdbc.ZooKeeperHiveClientHelper#configureConnParams</code> 重新获取一个 HS2<br>使用新获取的 HS2 的 host 和 port 替换旧的 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">updateConnParamsFromZooKeeper</span><span class="params">(JdbcConnectionParams connParams)</span></span><br><span class="line">  <span class="keyword">throws</span> ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="comment">// Add current host to the rejected list</span></span><br><span class="line">  connParams.getRejectedHostZnodePaths().add(connParams.getCurrentHostZnodePath());</span><br><span class="line">  <span class="type">String</span> <span class="variable">oldServerHost</span> <span class="operator">=</span> connParams.getHost();</span><br><span class="line">  <span class="type">int</span> <span class="variable">oldServerPort</span> <span class="operator">=</span> connParams.getPort();</span><br><span class="line">  <span class="comment">// Update connection params (including host, port) from ZooKeeper</span></span><br><span class="line">  ZooKeeperHiveClientHelper.configureConnParams(connParams);</span><br><span class="line">  connParams.setJdbcUriString(connParams.getJdbcUriString().replace(</span><br><span class="line">    oldServerHost + <span class="string">&quot;:&quot;</span> + oldServerPort, connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort()));</span><br><span class="line">  LOG.info(<span class="string">&quot;Selected HiveServer2 instance with uri: &quot;</span> + connParams.getJdbcUriString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，通过 ZK 来实现 HS2 高可用的流程就结束了。  </p>
<p>看来问题还是在连接 HS2 出现了异常，即 <code>transport.open();</code> 这里，内部的代码没有细看，大致用了 SASL 做认证，其中会从  <code>sessConfMap</code>  中获取用户和密码，会不会是认证出现了问题了？  </p>
<p>用户名和密码是在创建对象 <code>HiveConnection</code> 是，从传入变量 <code>info</code> 中拿到后设置到 <code>sessConfMap</code> 中的  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_USER)) &#123;</span><br><span class="line">  sessConfMap.put(JdbcConnectionParams.AUTH_USER, info.getProperty(JdbcConnectionParams.AUTH_USER));</span><br><span class="line">  <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_PASSWD)) &#123;</span><br><span class="line">    sessConfMap.put(JdbcConnectionParams.AUTH_PASSWD, info.getProperty(JdbcConnectionParams.AUTH_PASSWD));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结合前面的异常栈，这个 <code>info</code> 就是 <code>com.dtstack.flinkx.hive.format.HiveInputFormat#executeQuery</code> 这里的 <code>properties</code> ，这个 <code>properties</code> 是从配置文件中获取的，检查后，发现配置是 null  </p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;increColumn&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;splitPk&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;startLocation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<p>这些问题大致明白了——没有获取到用户名和密码导致 HS2 连接失败。  </p>
]]></content>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>streamis-作业执行过程源码</title>
    <url>/2022/10/30/streamis-%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E6%BA%90%E7%A0%81/</url>
    <content><![CDATA[<p>记录 streamis 作业执行流程</p>
<span id="more"></span>

<h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><pre class="mermaid">flowchart TD
构建单次作业 --> 提交创建引擎
提交创建引擎 --> 获取引擎信息
获取引擎信息 --> 更新作业状态</pre>

<ol>
<li><p>构建一次性作业</p>
<p>访问接口<code>/streamis/streamJobManager/job/execute</code>，从数据库中获取 job 信息，封装到<code>StreamisTransformJob</code>实例 中，再经过一系列的转换（<code>Transform</code>）处理（添加 labels、source、作业配置、launchConfig）后得到一个<code>LaunchJob</code>实例。这个实例，会有 linkis 的 <code>SimpleOnceJobBuilder</code> 转成一个 <code>SubmittableSimpleOnceJob</code> 对象，封装了 linkis 客户端和引擎创建 action：<code>CreateEngineConnAction</code>，此外还会将作业要执行的 sql 作为资源上传到 HDFS 便于引擎执行时获取。</p>
</li>
<li><p>提交创建引擎</p>
<p>提交上文创建的 <code>SubmittableSimpleOnceJob</code>，通过 linkis 客户端执行请求<code>CreateEngineConnAction</code>，然后在一个循环中等待引擎就绪，得到一个<code>engineConnId</code>。linkis 启动引擎的过程中，会创建一个<code>FlinkCodeOnceExecutor</code>实例的执行器，这个执行器内部会存储 yarn application id 和 作业所在 node manager 的地址。保存<code>engineConnId</code>和<code>SubmittableSimpleOnceJob</code>实例的映射到缓存<code>onceJobs</code>中。</p>
</li>
<li><p>获取引擎信息</p>
<p>内部创建一个<code>EngineConnOperateAction</code>，通过 linkis 客户端请求，拿到上面的 yarn application id 和 node manger 地址，与 <code>engineConnId</code>、提交用户、ECM 实例 等封装一个 <code>FlinkJobInfo</code> 实例中。保存<code>engineConnId</code>和<code>FlinkJobInfo</code>实例的映射到缓存<code>onceJobIdToJobInfo</code>中。</p>
</li>
<li><p>更新作业状态</p>
<p>类<code>TaskMonitorService</code> 中有个定时任务，从数据库中获取未完成的任务，根据任务的 <code>engineConnId</code>从映射中拿到<code>SubmittableSimpleOnceJob</code>实例，内部会创建一个<code>GetEngineConnAction</code>，有 linkis 客户端发起请求，获取节点信息，里面包含了引擎的状态。</p>
</li>
</ol>
<h2 id="任务执行-API-入口"><a href="#任务执行-API-入口" class="headerlink" title="任务执行 API 入口"></a>任务执行 API 入口</h2><pre class="mermaid">sequenceDiagram
activate JobRestfulApi
JobRestfulApi ->> JobRestfulApi : 判断用户对于作业的权限
JobRestfulApi ->> + TaskService : executeJob()
    TaskService ->> TaskService : 根据 jobId 从表 linkis_stream_job 获取 job
    TaskService ->> TaskService : 根据 jobId 从表 linkis_stream_job_version 获取最新版本 jobVersion
    TaskService ->> TaskService : 创建 task 对象，并保存到表 linkis_stream_task
    Note over TaskService : 找到满足条件的 Builder： <br /> StreamisFlinkTransformJobBuilder <br />调用 build() 方法
    TaskService ->> + AbstractFlinkStreamisTransformJobBuilder : build()：<br /> 调用符合条件的实现类<br />将 StreamJob 转成 StreamisTransformJob
    AbstractFlinkStreamisTransformJobBuilder ->> AbstractFlinkStreamisTransformJobBuilder : 通过 configurationService 获取任务配置<br/>内部按照不同类型将配置分类<br/> 通过 streamJobMapper 获取版本
    Note over FlinkSQLJobContentParser : 不同版本的作业，作业内容可能不一样
    AbstractFlinkStreamisTransformJobBuilder ->> + FlinkSQLJobContentParser : parseTo()：从作业版本中获取作业内容
    Note over FlinkSQLJobContentParser : 根据任务类型获取作业内容 <br /> 1. file 类型调用 getFileContent(...) <br /> 2. bml 类型调用  readFileFromBML(...) 3. sql 类型直接返回 sql 字符串
    FlinkSQLJobContentParser -->> - AbstractFlinkStreamisTransformJobBuilder : 返回 StreamisSqlTransformJobContent 对象<br />封装了任务内容（需要执行的 SQL）
    AbstractFlinkStreamisTransformJobBuilder ->> AbstractFlinkStreamisTransformJobBuilder : 设置 engineConn 类型（flink-1.12.2）和运行类型（sql）
    AbstractFlinkStreamisTransformJobBuilder -->> - TaskService : 返回 transformJob
    Note over TaskService : 接下来，将 transformJob 转成 launchJob
    TaskService ->> + Transform : transform() 
    Note over TaskService, Transform : 这里调用了一个 foldLeft 方法，<br />效果是从数组的最后一个 Transform 依次作用在 transformJob 上面，<br />并将结果作为下次 transform 的输入。
    Transform -->> - TaskService : 返回 launchJob

    Note over TaskService : 接下来通过 linkisJobManager 启动 launchJob<br /><br />通过反射获取 LinkisJobManager 接口的实现类，<br />接口中有个 getName 方法。<br />内部会构建一个 name -> 实现类的映射。<br /> 这里根据 name=simpleFlink 获取到的实现类是 SimpleFlinkJobManager
    
    Note over TaskService : 具体见“启动LaunchJob”

TaskService -->> - JobRestfulApi : 执行结束返回
note over JobRestfulApi : 返回成功消息
deactivate JobRestfulApi</pre>

<p>Transform 的作用是给 launchJob 添加 labels、source、作业配置、launchConfig。</p>
<pre class="mermaid">classDiagram
class Transform {
<<interface>>
+ transform(...)
}
Transform<|..StreamisJobContentTransform
Transform<|..ConfigTransform
Transform<|..LabelsStreamisCodeTransform
Transform<|..SourceTransform
Transform<|..FlinkJarStreamisStartupParamsTransform
Transform<|..LaunchConfigTransform



class StreamisJobContentTransform {
<<interface>>
+ transform(...)
+ transformJobContent(...)
}
StreamisJobContentTransform<|..SqlStreamisJobContentTransform
StreamisJobContentTransform<|..FlinkJarStreamisJobContentTransform

class ConfigTransform {
<<interface>>
+ transform(...)
+ transform(...)
}
ConfigTransform<|..FlinkCheckpointConfigTransform
ConfigTransform<|..ResourceConfigTransform


class ResourceConfigTransform{
+ transform(...)
}
ResourceConfigTransform<|..ExtraConfigTransform</pre>



<h3 id="LaunchJob-示例"><a href="#LaunchJob-示例" class="headerlink" title="LaunchJob 示例"></a>LaunchJob 示例</h3><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">LaunchJob(</span><br><span class="line">	submitUser: hadoop, </span><br><span class="line">	labels: &#123;</span><br><span class="line">		<span class="attribute">userCreator</span>=hadoop-Streamis, <span class="attribute">engineType</span>=flink-1.12.2, <span class="attribute">engineConnMode</span>=once</span><br><span class="line">	&#125;, </span><br><span class="line">	jobContent: &#123;</span><br><span class="line">		<span class="attribute">code</span>=SELECT <span class="string">&#x27;linkis flink engine test!!!&#x27;</span>, <span class="attribute">runType</span>=sql</span><br><span class="line">	&#125;, </span><br><span class="line">	params: &#123;</span><br><span class="line">		configuration=&#123;</span><br><span class="line">			startup=&#123;</span><br><span class="line">				wds.linkis.flink.taskmanager.<span class="attribute">num</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.jobmanager.<span class="attribute">cpus</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.taskmanager.<span class="attribute">memory</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.<span class="attribute">custom</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.taskManager.<span class="attribute">cpus</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.jobmanager.<span class="attribute">memory</span>=<span class="literal">null</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;, </span><br><span class="line">	source: &#123;</span><br><span class="line">		<span class="attribute">workspace</span>=<span class="literal">null</span>, <span class="attribute">project</span>=demo, <span class="attribute">job</span>=demo_flink_00</span><br><span class="line">	&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>launchConfig 由于在 toString 方法中没有实现，所以日志中不会打印</p>
</blockquote>
<h2 id="启动-LaunchJob"><a href="#启动-LaunchJob" class="headerlink" title="启动 LaunchJob"></a>启动 LaunchJob</h2><p>启动 LaunchJob 内部先是构建了一个单次作业 onceJob，再提交</p>
<p><code>com.webank.wedatasphere.streamis.jobmanager.launcher.linkis.manager.FlinkJobManager#launch</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">launch</span></span>(job: <span class="type">LaunchJob</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    job.getLabels.get(<span class="type">LabelKeyUtils</span>.<span class="type">ENGINE_TYPE_LABEL_KEY</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> engineConnType: <span class="type">String</span> =&gt;</span><br><span class="line">        <span class="keyword">if</span>(!engineConnType.toLowerCase.startsWith(<span class="type">FlinkJobManager</span>.<span class="type">FLINK_ENGINE_CONN_TYPE</span>))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">FlinkJobLaunchErrorException</span>(<span class="number">30401</span>, <span class="string">s&quot;Only <span class="subst">$&#123;FlinkJobManager.FLINK_ENGINE_CONN_TYPE&#125;</span> job is supported to be launched to Linkis, but <span class="subst">$engineConnType</span> is found.&quot;</span>)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">FlinkJobLaunchErrorException</span>(<span class="number">30401</span>, <span class="string">s&quot;Not exists <span class="subst">$&#123;LabelKeyUtils.ENGINE_TYPE_LABEL_KEY&#125;</span>, StreamisJob cannot be submitted to Linkis successfully.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> onceJob = buildOnceJob(job)</span><br><span class="line">    onceJob.submit()</span><br><span class="line">    onceJobs synchronized onceJobs.put(onceJob.getId, onceJob)</span><br><span class="line">    <span class="keyword">val</span> linkisJobInfo = <span class="type">Utils</span>.tryCatch(createJobInfo(onceJob, job))&#123; t =&gt;</span><br><span class="line">        error(<span class="string">s&quot;<span class="subst">$&#123;job.getSubmitUser&#125;</span> create jobInfo failed, now stop this EngineConn <span class="subst">$&#123;onceJob.getId&#125;</span>.&quot;</span>)</span><br><span class="line">        stop(onceJob)</span><br><span class="line">        <span class="keyword">throw</span> t</span><br><span class="line">    &#125;</span><br><span class="line">    onceJobs synchronized onceJobIdToJobInfo.put(onceJob.getId, linkisJobInfo)</span><br><span class="line">    onceJob.getId</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="构建单次作业"><a href="#构建单次作业" class="headerlink" title="构建单次作业"></a>构建单次作业</h3><pre class="mermaid">sequenceDiagram
activate TaskService
TaskService ->> + FlinkJobManager : launch()
    
    FlinkJobManager ->> FlinkJobManager : 判断标签 engineType 是否 flink 开头
    FlinkJobManager ->> + SimpleFlinkJobManager : buildOnceJob()：<br />构建单次作业
        
            SimpleFlinkJobManager  ->> SimpleFlinkJobManager : 初始化 SimpleOnceJobBuilder 对象
            rect rgb(121, 216, 206)
            SimpleFlinkJobManager ->> + SimpleOnceJobBuilder : build()
                SimpleOnceJobBuilder ->> SimpleOnceJobBuilder :  <br /> 1. 校验 labels/jobContent 是否为空 <br /> 2. 设置 params/source <br /> 3. 配置启动参数 <br /> 4. TODO 还有些细节要看下
                activate SimpleOnceJobBuilder
                    Note right of SimpleOnceJobBuilder : 调用方法 getOnceExecutorContent()
                    SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 构建 OnceExecutorContent 对象，并初始化
                    SimpleOnceJobBuilder ->> + OnceExecutorContentUtils : contentToMap() ：<br />将 onceExecutorContent <br /> 转成 contentMap
                    OnceExecutorContentUtils -->> - SimpleOnceJobBuilder : 返回 contentMap
                    SimpleOnceJobBuilder ->> + HttpBmlClient : uploadResource()：<br /> json 序列化 contentMap 对象上传
                    HttpBmlClient -->> - SimpleOnceJobBuilder : 返回响应 response 包含资源 ID 和版本
                    SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 将 response 封装成 BmlResource 
                    SimpleOnceJobBuilder ->> + OnceExecutorContentUtils :　resourceToValue() ： <br />将 BmlResource 转成资源 ID
                    OnceExecutorContentUtils -->> - SimpleOnceJobBuilder : 返回资源 ID
                deactivate SimpleOnceJobBuilder
                SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 初始化 CreateEngineConnAction.Builder 对象
                SimpleOnceJobBuilder ->> + CreateEngineConnAction.Builder :　build()
                CreateEngineConnAction.Builder -->> - SimpleOnceJobBuilder : 返回对象 createEngineConnAction
                SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 封装一个 SubmittableSimpleOnceJob 实例
                Note over SimpleOnceJobBuilder : 这个实例中包含有 linkisClient，以及 createEngineConnAction<br/> linkisCLient 内部有个成员 dwsHttpClient 后续与 linkis 接口的交互通过它执行
            SimpleOnceJobBuilder -->> - SimpleFlinkJobManager : 返回实例
            end
    SimpleFlinkJobManager -->> - FlinkJobManager : 返回实例 onceJob
    
    Note over FlinkJobManager : 执行 onceJob.submit() 方法<br />具体见“提交创建引擎”
    FlinkJobManager -->> - TaskService : 返回 linkisJobId
deactivate TaskService</pre>

<p>createEngineConnAction.getRequestPayload</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;createService&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ServiceInstance(streamis-server, 10-177-198-114.ostream-test.dgtest01:9400)&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ignoreTimeout&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;demo for streamis&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.taskmanager.num&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.jobmanager.cpus&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.taskmanager.memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.taskManager.cpus&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.custom&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;label.codeType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sql&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.jobmanager.memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;onceExecutorContent&quot;</span><span class="punctuation">:</span> <span class="string">&quot;resource_036360c3c48-fc89-4a86-bafc-b21a9af830eav000001&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;userCreator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hadoop-Streamis&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;engineType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;flink-1.12.2&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;engineConnMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;once&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timeOut&quot;</span><span class="punctuation">:</span> <span class="number">300000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>接下来，会将上面的 SubmittableSimpleOnceJob 通过 linkis 客户端提交给 linkis，linkis 则会创建一个 EngineConn。</p>
<h3 id="获取引擎信息"><a href="#获取引擎信息" class="headerlink" title="获取引擎信息"></a>获取引擎信息</h3><pre class="mermaid">sequenceDiagram
activate FlinkJobManager
    FlinkJobManager ->> + AbstractSubmittableLinkisJob : submit() 提交 onceJob
    AbstractSubmittableLinkisJob ->> + SubmittableSimpleOnceJob : doSubmit()
    Note over AbstractSubmittableLinkisJob : 这里与之前创建的 SubmittableSimpleOnceJob 对上了
    SubmittableSimpleOnceJob ->> + LinkisManagerClientImpl : createEngineConn()
    Note over LinkisManagerClientImpl : 这个方法内部会发起 http 请求 linkis 接口<br /> 返回一个 CreateEngineConnResult 实例
    LinkisManagerClientImpl -->> - SubmittableSimpleOnceJob : 返回 nodeInfo
    SubmittableSimpleOnceJob ->> SubmittableSimpleOnceJob : 从 nodeInfo 中获取 serviceInstance、ticketId、ecmServiceInstance、lastEngineConnState
    SubmittableSimpleOnceJob ->> SimpleOnceJob :　initOnceOperatorActions()：在数组 operatorActions 中添加一个 action<br /> 这个 action 给 OnceJobOperator 设置 user/ticketId/serviceInstance/linkisManagerClient
 
    Note over SimpleOnceJob : TODO action 的作用
    rect rgb(100, 118, 135)
    alt 调用方法 isCompleted 判断状态 lastEngineConnState 未完成 且 未启动
    Note over SimpleOnceJob : isCompleted() 方法中在对于 engineConn 结束和运行<br/>分别回调 onJobFlinished / onJobRunning
    rect rgb(255, 230, 204)
    loop 状态 未完成 且 未启动
    SubmittableSimpleOnceJob ->> + SimpleOnceJob :　isCompleted
    SimpleOnceJob ->> + OnceJob : getNodeInfo() ：通过 linkis 客户端获取 <br/>ServiceInstance 对应的 EngineConn 信息
    OnceJob -->> - SimpleOnceJob : 返回 EngineConnNode
    SimpleOnceJob -->> - SubmittableSimpleOnceJob : 从 EngineConnNode 获取状态，返回 isCompleted() 结果作为状态
    end
    end
    SubmittableSimpleOnceJob ->> + SimpleOnceJob :　transformToId()
    SimpleOnceJob -->> - SubmittableSimpleOnceJob : 设置 engineConnId
    else
    SubmittableSimpleOnceJob ->> + SimpleOnceJob :　transformToId()
    SimpleOnceJob -->> - SubmittableSimpleOnceJob : 设置 engineConnId
    end
    end
    SubmittableSimpleOnceJob -->> - AbstractSubmittableLinkisJob : 无返回值
    Note over AbstractSubmittableLinkisJob : 具体见“提交创建引擎后回调”
    AbstractSubmittableLinkisJob -->> - FlinkJobManager : 无返回值
    FlinkJobManager ->> FlinkJobManager : 以 engineConnId 作为 key<br/>onceJob 作为 value<br/>放到缓存 onceJobs 中
    Note over FlinkJobManager : 具体见“通知引擎执行作业”
deactivate FlinkJobManager</pre>

<p>这里逻辑就是通过 linkis 客户端向 linkis 发送一个 <code>createEngineConnAction</code> 用于创建一个 <code>engineConn</code>，然后一直等待直到这个 <code>engineConn</code> 结束或者 运行起来。</p>
<p>这里请求的 linkis 接口地址中路径是 <code>linkisManager/createEngineConn</code>，对应的执行逻辑入口是 <code>org.apache.linkis.manager.am.restful.EngineRestfulApi#createEngineConn</code></p>
<p><code>AbstractSubmittableLinkisJob</code> 在提交之后，还有个指标 jobMetrics 和回调（<code>onJobSubmitted()</code>），见下文。</p>
<p>engineConnId 的格式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">engineConnId = <span class="string">s&quot;<span class="subst">$&#123;ticketId.length&#125;</span>_<span class="subst">$&#123;serviceInstance.getApplicationName.length&#125;</span>_<span class="subst">$&#123;ticketId&#125;</span><span class="subst">$&#123;serviceInstance.getApplicationName&#125;</span><span class="subst">$&#123;serviceInstance.getInstance&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//例如：</span></span><br><span class="line"><span class="comment">//36_20_05443308-f227-442e-9b20-8d291e1fa7d1linkis-cg-engineconn10-177-198-114.ostream-test.dgtest01:37244</span></span><br><span class="line"><span class="comment">//其中：</span></span><br><span class="line"><span class="comment">// ticketId = 05443308-f227-442e-9b20-8d291e1fa7d1</span></span><br><span class="line"><span class="comment">// applicationName = linkis-cg-engineconn</span></span><br><span class="line"><span class="comment">// serviceInstance = 10-177-198-114.ostream-test.dgtest01:37244</span></span><br></pre></td></tr></table></figure>

<p>nodeInfo 中包含的信息，看下日志，engineConnId 几乎包含了。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">EngineConn</span> created with status Running, the nodeInfo is &#123;ecmServiceInstance=&#123;instance=<span class="number">10</span>-<span class="number">177</span>-<span class="number">198</span>-<span class="number">114</span>.os</span><br><span class="line"><span class="attribute">tream</span>-test.dgtest01:<span class="number">9112</span>, applicationName=linkis-cg-engineconnmanager&#125;, serviceInstance=&#123;instance=<span class="number">10</span>-<span class="number">177</span>-<span class="number">198</span>-<span class="number">114</span>.ostream-test.dgtest01:<span class="number">40698</span>, applicationName=linkis-cg-engineconn&#125;, nodeStatus=Running, ticketId=a3f69955-<span class="number">02</span>a1-<span class="number">4973</span>-<span class="number">9325</span>-<span class="number">5</span>b</span><br><span class="line"><span class="attribute">f33f78b9d6</span>&#125;.</span><br></pre></td></tr></table></figure>



<h4 id="linkis-客户端请求创建EngineConn"><a href="#linkis-客户端请求创建EngineConn" class="headerlink" title="linkis 客户端请求创建EngineConn"></a>linkis 客户端请求创建EngineConn</h4><p><code>org.apache.linkis.computation.client.once.simple.SubmittableSimpleOnceJob#doSubmit</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    info(<span class="string">s&quot;Ready to create a engineConn: <span class="subst">$&#123;createEngineConnAction.getRequestPayload&#125;</span>.&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> nodeInfo = linkisManagerClient.createEngineConn(createEngineConnAction)</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中<code>linkisManagerClient.createEngineConn(createEngineConnAction)</code>最终会调用到<code>org.apache.linkis.httpclient.AbstractHttpClient#execute</code>，这里的逻辑在源码-linkis 之BML物料上传与下载有做介绍。</p>
<p>请求的接口路径是这样确定的</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareReq</span></span>(requestAction: <span class="type">HttpAction</span>): <span class="type">HttpRequestBase</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> realURL = <span class="string">&quot;&quot;</span></span><br><span class="line">    requestAction <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> serverUrlAction: <span class="type">ServerUrlAction</span> =&gt;</span><br><span class="line">        realURL = connectUrl(serverUrlAction.serverUrl, requestAction.getURL)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        realURL = getRequestUrl(requestAction.getURL, requestAction.getRequestBody)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于<code>CreateEngineConnAction</code>，<code>getURL</code>方法实现是</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DWSHttpAction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> dwsVersion: <span class="type">String</span> = _</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setDWSVersion</span></span>(dwsVersion: <span class="type">String</span>): <span class="type">Unit</span> = <span class="keyword">this</span>.dwsVersion = dwsVersion</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPrefixURL</span></span>: <span class="type">String</span> = <span class="string">&quot;/api/&quot;</span> + getRestType + <span class="string">&quot;/&quot;</span> + dwsVersion</span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getRestType</span></span>: <span class="type">RestType</span> = <span class="type">RestType</span>.<span class="type">JERSEY</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">suffixURLs</span></span>: <span class="type">Array</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getURL</span></span>: <span class="type">String</span> = getPrefixURL + <span class="string">&quot;/&quot;</span> + suffixURLs.mkString(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>url 是由前缀和后缀两部分组成</p>
<ol>
<li><p>前缀是 <code>&quot;/api/&quot; + getRestType + &quot;/&quot; + dwsVersion</code>，其中 restType 是<code>rest_j</code>，<code>dwsVersion</code>由配置项<code>wds.linkis.web.version</code>指定，默认值是<code>v1</code></p>
</li>
<li><p>后缀<code>suffixURLs</code>在<code>CreateEngineConnAction</code>中有重载</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CreateEngineConnAction</span> <span class="keyword">extends</span> <span class="title">POSTAction</span> <span class="keyword">with</span> <span class="title">LinkisManagerAction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRequestPayload</span></span>: <span class="type">String</span> = <span class="type">DWSHttpClient</span>.jacksonJson.writeValueAsString(getRequestPayloads)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">suffixURLs</span></span>: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">&quot;linkisManager&quot;</span>, <span class="string">&quot;createEngineConn&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>综上，请求的接口地址路径是<code>api/rest_j/v1/linkisManager/createEngineConn</code>。这个地址在 linkis 的处理入口是<code>org.apache.linkis.manager.am.restful.EngineRestfulApi#createEngineConn</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping(path = &quot;/linkisManager&quot;, produces = &#123;&quot;application/json&quot;&#125;)</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">EngineRestfulApi</span> &#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    <span class="meta">@RequestMapping(path = &quot;/createEngineConn&quot;, method = RequestMethod.POST)</span></span><br><span class="line">    <span class="keyword">public</span> Message <span class="title function_">createEngineConn</span><span class="params">( HttpServletRequest req, <span class="meta">@RequestBody</span> JsonNode jsonNode)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<p>此外还有些有意思的地方，比如</p>
<ol>
<li><p>响应类型，是根据请求的 url 来确定：在响应对象上定义了一个注解，注解值是对应的请求 url，通过反射获取到 url 和 响应类的映射关系后，从映射关系中获取响应类型。例如，</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@DWSHttpMessageResult</span>(<span class="string">&quot;/api/rest_j/v\\d+/linkisManager/createEngineConn&quot;</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CreateEngineConnResult</span> <span class="keyword">extends</span> <span class="title">GetEngineConnResult</span></span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="提交创建引擎后回调"><a href="#提交创建引擎后回调" class="headerlink" title="提交创建引擎后回调"></a>提交创建引擎后回调</h4><pre class="mermaid">sequenceDiagram 
activate AbstractSubmittableLinkisJob
    AbstractSubmittableLinkisJob ->> + LinkisJobMetrics : 
    Note over TaskService, LinkisJobMetrics : 创建 LinkisJobMetrics 对象， <br/>参数是上面创建的 engineConnId
    LinkisJobMetrics -->> - AbstractSubmittableLinkisJob : 返回 jobMetrics
    AbstractSubmittableLinkisJob ->> + AbstractLinkisJob : getJobListeners()：获取作业监听器
    Note over AbstractLinkisJob : 这里的监听器应该是空的<br /> 方法 addJobListener 貌似没有被调用
    AbstractLinkisJob -->> - AbstractSubmittableLinkisJob : 返回监听器 
    Note over AbstractSubmittableLinkisJob : 调用监听器的 onJobSubmitted 方法
deactivate AbstractSubmittableLinkisJob</pre>

<h2 id="获取引擎信息-1"><a href="#获取引擎信息-1" class="headerlink" title="获取引擎信息"></a>获取引擎信息</h2><pre class="mermaid">sequenceDiagram
activate FlinkJobManager
    FlinkJobManager ->> FlinkJobManager : 将 onceJob 保存到缓存 onceJobs 中
    FlinkJobManager ->> + SimpleFlinkJobManager : createJobInfo()
    
    SimpleFlinkJobManager ->> + OnceJob : getNodeInfo : 获取 engineConn 信息
    OnceJob -->> - SimpleFlinkJobManager : 返回 nodeInfo
    SimpleFlinkJobManager ->> SimpleFlinkJobManager : 创建一个 FlinkJobInfo 对象 jobInfo
    
    activate SimpleFlinkJobManager
    Note right of SimpleFlinkJobManager : 调用方法 fatchApplicationInfo() <br/> 获取 yarn 作业信息 
    Note right of SimpleFlinkJobManager : 根据 operator 名字获取 operator<br/> EngineConnApplicationInfoOperator
    Note right of SimpleFlinkJobManager : 将前面设置的 action 依次作用到<br/>EngineConnApplicationInfoOperator
    rect rgb(121, 216, 206)
    loop 可以重试
    SimpleFlinkJobManager ->> + OnceJobOperator : apply()
    OnceJobOperator ->> OnceJobOperator : 构建 EngineConnOperateAction.Builder 对象
    Note over OnceJobOperator : 从这个 builder 的类型<br/>可以看出它是用来构建 EngineConn 执行操作的构建器
    OnceJobOperator ->> + EngineConnOperateAction.Builder : build()
    EngineConnOperateAction.Builder -->> - OnceJobOperator : 返回 EngineConnOperateAction 实例 <br /> engineConnOperateAction
    
    OnceJobOperator ->> + LinkisManagerClientImpl : executeEngineConnOperation()
    Note over LinkisManagerClientImpl : 通过 linkis 客户端，提交 engineConnOperateAction <br/> 这里引擎开始执行任务
    LinkisManagerClientImpl -->> - OnceJobOperator : 返回执行结果 result，结果对象中有 applicationId，applicationUrl
    
    OnceJobOperator ->> + EngineConnApplicationInfoOperator : resultToObject()：将返回的结果 result 转成 ApplicationInfo
    EngineConnApplicationInfoOperator -->> - OnceJobOperator : 返回 ApplicationInfo 对象（包含：applicationId、applicationUrl、queue）
    
    OnceJobOperator -->> - SimpleFlinkJobManager :返回 applicationInfo 对象
    end
    end
    SimpleFlinkJobManager ->> SimpleFlinkJobManager : 从 applicationInfo 对象中获取 <br/>applcaitionId,applicationUrl 设置 jobInfo
    deactivate SimpleFlinkJobManager
    SimpleFlinkJobManager ->> SimpleFlinkJobManager : jobInfo 设置 resource
    SimpleFlinkJobManager -->> - FlinkJobManager : 返回 jobInfo
    
    FlinkJobManager ->> FlinkJobManager : 存放到缓存 onceJobIdToJobInfo <br /> 返回 engineConnId
deactivate FlinkJobManager</pre>

<p>这里请求的 linkis 接口地址中路径是 <code>linkisManager/executeEngineConnOperation</code>，对应的执行逻辑入口是 <code>org.apache.linkis.manager.am.restful.EngineRestfulApi#executeEngineConnOperation</code></p>
<h2 id="更新作业状态"><a href="#更新作业状态" class="headerlink" title="更新作业状态"></a>更新作业状态</h2><pre class="mermaid">sequenceDiagram

activate TaskService 
    activate TaskService
    Note right of TaskService : 调用 updateStreamTaskStatus()<br/> 这个方法内部会判断 task <br />是否在缓存 onceJobIdToJobInfo 中
    TaskService ->> + FlinkJobManager : getJobInfo() : <br />从缓存 onceJobIdToJobInfo 获取 jobInfo
    FlinkJobManager ->> + SimpleFlinkJobManager : getStatus() : <br /> 从缓存 onceJobs 中获取 simpleOnceJob
    SimpleFlinkJobManager ->> + SimpleOnceJob : isCompleted() ：调用
    Note over SimpleOnceJob :　这个方法内部会调用 linkis 客户端获取 engineConn 状态
    SimpleOnceJob -->> - SimpleFlinkJobManager : 返回状态
    SimpleFlinkJobManager -->> - FlinkJobManager : 返回状态 status
    Note over FlinkJobManager : jobInfo 设置状态 status
    FlinkJobManager -->> - TaskService :  返回 jobInfo
    Note right of TaskService : 更新 task 的 更新时间和状态<br /> task 状态结束，jobInfo 中 completedMsg 非空<br/>task 设置错误消息 errDesc<br/> jobInfo json 序列化保存到 task 的 linkisJobInfo
    deactivate TaskService
    Note right of TaskService : 最后更新数据表 linkis_stream_task
    
deactivate TaskService</pre>

<p>此外类<code>TaskMonitorService</code>中会启动定时任务调用<code>com.webank.wedatasphere.streamis.jobmanager.manager.service.TaskService$#updateStreamTaskStatus</code>来获取状态。</p>
]]></content>
      <tags>
        <tag>streamis</tag>
      </tags>
  </entry>
  <entry>
    <title>日志门面混用下的格式问题</title>
    <url>/2022/11/05/%E6%97%A5%E5%BF%97%E9%97%A8%E9%9D%A2%E6%B7%B7%E7%94%A8%E4%B8%8B%E7%9A%84%E6%A0%BC%E5%BC%8F%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>介绍了日志门面 ACL 和 SLF4J 混用带来的问题和处理过程</p>
<span id="more"></span>

<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>flinkx 1.11 提交中日志中有个问题：</p>
<p><img src="/2022/11/05/%E6%97%A5%E5%BF%97%E9%97%A8%E9%9D%A2%E6%B7%B7%E7%94%A8%E4%B8%8B%E7%9A%84%E6%A0%BC%E5%BC%8F%E9%97%AE%E9%A2%98/image-20221105143311217.png" alt="image-20221105143311217">  </p>
<p>输出日志中既有 log4j 也有 logback。</p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>当初为了验证会使用哪个日志配置文件，在 flinkx-launcher 的资源目录中放了 3 个日志配置文件  </p>
<p><img src="/2022/11/05/%E6%97%A5%E5%BF%97%E9%97%A8%E9%9D%A2%E6%B7%B7%E7%94%A8%E4%B8%8B%E7%9A%84%E6%A0%BC%E5%BC%8F%E9%97%AE%E9%A2%98/image-20221105143644773.png" alt="image-20221105143644773"></p>
<p>其中 log4j 和 logback 的 layout 配置中分别设置了 log4j 和 logback 字样。从上面的 launcher 的启动日志中，发现使用了两个日志配置文件，说明使用了两个日志实现框架，为什么会这样呢？<br>其中 log4j 对应的日志是  </p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="number">2022</span>-<span class="number">07</span>-<span class="number">29</span> <span class="number">15</span>:<span class="number">07</span>:<span class="number">40</span>,<span class="number">903</span> log4j - <span class="number">0</span>    WARN  <span class="selector-attr">[main]</span> org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.util</span><span class="selector-class">.NativeCodeLoader</span>:Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">2022</span>-<span class="number">07</span>-<span class="number">29</span> <span class="number">15</span>:<span class="number">07</span>:<span class="number">43</span>,<span class="number">483</span> log4j - <span class="number">2580</span> INFO  <span class="selector-attr">[main]</span> org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.yarn</span><span class="selector-class">.client</span><span class="selector-class">.api</span><span class="selector-class">.impl</span><span class="selector-class">.YarnClientImpl</span>:Submitted application application_1644979452149_289296</span><br></pre></td></tr></table></figure>
<p>看了类<code>org.apache.hadoop.yarn.client.api.impl.YarnClientImpl</code>中 logger 对象的获取，它是从 Apache Common Logging 中获取的（类<code>org.apache.commons.logging.LogFactory;</code>）。</p>
<blockquote>
<p>关于 ACL 的解释，见文章<a href="https://segmentfault.com/a/1190000041422727">5分钟搞定混乱的Java日志体系</a>：  2002年8月Apache推出了JCL（Jakarta Commons Logging），也就是日志抽象层，支持运行时动态加载日志组件的实现，当然也提供一个默认实现Simple Log（在ClassLoader中进行查找，如果能找到Log4j则默认使用log4j实现，如果没有则使用JUL 实现，再没有则使用JCL内部提供的Simple Log实现）。  </p>
</blockquote>
<p>一句话 ACL 是一个类似 slf4j 的门面，默认使用 log4j。  看到这里，有一个猜测：既然 ACL 默认使用 log4j，类路径中存在 log4j 的 jar 包，上面日志中出现 log4j 就对了。</p>
<h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><p>接下来要做的是：把 flinkx 工程里面的 log4j 全部排掉，只保留 logback，同时引入一个 ACL 到 slf4j 的转换包，用于支持需要使用 ACL 的逻辑。<del>此外：slf4j-log4j12 是 hive 的依赖中引入的，通过 exclude 方法没法排除，考虑使用 shade 插件来做排除。</del>重新打包上传，执行看提交日志</p>
<p><img src="/2022/11/05/%E6%97%A5%E5%BF%97%E9%97%A8%E9%9D%A2%E6%B7%B7%E7%94%A8%E4%B8%8B%E7%9A%84%E6%A0%BC%E5%BC%8F%E9%97%AE%E9%A2%98/image-20221105143819305.png" alt="image-20221105143819305">  </p>
<p>从中可以看到之前的日志中 log4j 字样变成了 logback，说明之前的想法是对的。</p>
]]></content>
      <tags>
        <tag>flinkx</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title>获取对账数据</title>
    <url>/2022/11/05/%E8%8E%B7%E5%8F%96%E5%AF%B9%E8%B4%A6%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p>flinkx 1.11 任务执行过程中记录读写相关的指标数据，这里这里介绍如何将这些指标数据获取并输出到外部系统中。</p>
<span id="more"></span>

<hr>
<p>FlinkX 内部将读写相关的统计数据记录到了Flink的metric 里面。</p>
<p>metric中的数据是如何获取的呢？<code>BaseRichOutputFormat</code> 和<code>BaseRichInputFormat </code> 有一个成员<code>accumulatorCollector</code>，这个成员在<code>open</code>方法里初始化，内部会启动一个线程，定时从metric获取。</p>
<p>现在统计数据可以获取到了，还有一个问题——与这个数据关联的作业是哪个，Flink作业运行的时候是没有这些信息的，要解决这个问题就要将作业相关的信息传给Flink。</p>
<p>作业相关的信息可以有</p>
<ol>
<li>taskId和调度时间</li>
<li>applicationId（datahub 在提交作业时会记录applicationId，通过applicationId可以找到作业）</li>
</ol>
<p>使用方案1的潜在问题是，一个taskId可能不止一个同步任务；</p>
<p>使用方案2的问题是，applicationId是yarn里面的概念，后续将作业运行到k8s上，这个applicationId改如何表示</p>
<p>有了作业信息，在输出数据写完之后，具体就是<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#close</code>方法结束的时候，将统计数据（通过<code>accumulatorCollector</code>获取）和作业信息发送到外部的接口保存起来，就可以用于后续对账了。</p>
<p>无论采用哪个方案，传递的过程是类似的。</p>
<h2 id="如何传递作业信息"><a href="#如何传递作业信息" class="headerlink" title="如何传递作业信息"></a>如何传递作业信息</h2><p>在说明之前，以<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#monitorUrl</code>为例看下FlinkX中是如何传递参数的</p>
<p>首先，方法<code>com.dtstack.flinkx.launcher.Launcher#main</code>中，解析命令行参数，提交到yarn之前</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">argList.add(<span class="string">&quot;-monitor&quot;</span>);</span><br><span class="line">argList.add(<span class="string">&quot;&quot;</span>);</span><br><span class="line">PerJobSubmitter.submit(launcherOptions, <span class="keyword">new</span> <span class="title class_">JobGraph</span>(), argList.toArray(<span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">0</span>]));</span><br></pre></td></tr></table></figure>

<p>这里<code>-monitor</code>参数设置的是空，会在方法<code>com.dtstack.flinkx.launcher.perJob.FlinkPerJobUtil#buildProgram</code>填充</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="string">&quot;-monitor&quot;</span>.equals(args[i])) &#123;</span><br><span class="line">	args[i + <span class="number">1</span>] = monitorUrl;</span><br><span class="line">	<span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后作为<code>com.dtstack.flinkx.Main#main</code>的参数传入，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">monitor</span> <span class="operator">=</span> options.getMonitor();</span><br><span class="line"><span class="type">DataTransferConfig</span> <span class="variable">config</span> <span class="operator">=</span> DataTransferConfig.parse(job);</span><br><span class="line"><span class="keyword">if</span>(StringUtils.isNotEmpty(monitor)) &#123;</span><br><span class="line">    config.setMonitorUrls(monitor);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>com.dtstack.flinkx.Main#main</code>方法中会通过Factory方法创建<code>BaseDataWriter</code>的子类，子类在初始化时，会调用基类<code>com.dtstack.flinkx.writer.BaseDataWriter#BaseDataWriter</code>的构造方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">BaseDataWriter</span><span class="params">(DataTransferConfig config)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.monitorUrls = config.getMonitorUrls();</span><br><span class="line">    <span class="comment">//...    </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>子类创建后，调用子类的<code>writeData</code>方法，就会将上面的<code>monitorUrls</code>通过<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormatBuilder#setMonitorUrls</code>设置到<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#monitorUrl</code>。</p>
<p>可以看到大致的流程是</p>
<pre class="mermaid">graph LR;
    命令行参数-->Launcher;
    Launcher-->填充;
    填充-->Main;
    Main-->XXXWriter;
    Main-->YYYWriter;
    XXXWriter-->BaseRichOutputFormat;    
    YYYWriter-->BaseRichOutputFormat;</pre>

<p>这里的问题是，传递过程经过了<code>BaseDataWriter</code>子类，不同的Writer子类都要在方法<code>writeData</code>中调用一次。有没有办法绕过呢？</p>
<p>有的，可以参照龙哥上次处理jobId为null的做法：</p>
<p>在方法<code>com.dtstack.flinkx.Main#main</code>中，将命令行参数解析到<code>ParameterTool</code>的一个对象中，然后设置到Flink的运行时中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">ParameterTool</span> <span class="variable">parameterTool</span> <span class="operator">=</span> ParameterTool.fromArgs(args);</span><br><span class="line">env.getConfig().setGlobalJobParameters(parameterTool);</span><br></pre></td></tr></table></figure>

<p><code>BaseRichOutputFormat</code>通过继承<code>RichOutputFormat</code>可以获取到运行时，进而获取到参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">context = (StreamingRuntimeContext) getRuntimeContext();</span><br><span class="line"><span class="type">ParameterTool</span> <span class="variable">parameterTool</span> <span class="operator">=</span> (ParameterTool) context.getExecutionConfig().getGlobalJobParameters();</span><br><span class="line">reportPath = parameterTool.get(<span class="string">&quot;reportPath&quot;</span>, <span class="literal">null</span>);</span><br><span class="line">appId = parameterTool.get(<span class="string">&quot;appId&quot;</span>, <span class="literal">null</span>);</span><br></pre></td></tr></table></figure>

<p>这里的两个参数</p>
<ul>
<li><code>reportPath</code> 上报接口地址</li>
<li><code>appId</code> yarn任务ID</li>
</ul>
<pre class="mermaid">graph LR;
    命令行参数-->Launcher;
    Launcher-->填充;
    填充-->Main;
    Main-->BaseRichOutputFormat;</pre>

<h2 id="发送"><a href="#发送" class="headerlink" title="发送"></a>发送</h2><p>在上面两个参数的基础上继续，通过<code>accumulatorCollector</code>获取到指标数据，连同applicationId，发送到指定接口就可以了（下面代码中的日志）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">reportStatistics</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (taskNumber == <span class="number">0</span> &amp;&amp; StringUtils.isNotEmpty(reportPath) &amp;&amp; StringUtils.isNotEmpty(appId)) &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">numReads</span> <span class="operator">=</span> accumulatorCollector.getAccumulatorValue(Metrics.NUM_READS);</span><br><span class="line">        <span class="type">long</span> <span class="variable">numWrites</span> <span class="operator">=</span> accumulatorCollector.getAccumulatorValue(Metrics.NUM_WRITES);</span><br><span class="line">        <span class="type">long</span> <span class="variable">numErrors</span> <span class="operator">=</span> accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS);</span><br><span class="line">        <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> String.format(<span class="string">&quot;&#123;\&quot;numReads\&quot;:%d,\&quot;numWrites\&quot;:%d,\&quot;numErrors\&quot;:%d&#125;&quot;</span>,</span><br><span class="line">                                      numReads, numWrites, numErrors);</span><br><span class="line">        LOG.info(<span class="string">&quot;send result:&#123;&#125; of appId:&#123;&#125; to &#123;&#125;&quot;</span>, result, appId, reportPath);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>提交作业，观察日志</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">2021</span>-<span class="number">08</span>-<span class="number">10</span> <span class="number">20</span>:<span class="number">33</span>:<span class="number">06</span>,<span class="number">221</span> INFO  com.dtstack.flinkx.hdfs.writer.HdfsTextOutputFormat           - send result:&#123;<span class="string">&quot;numReads&quot;</span>:<span class="number">91</span>,<span class="string">&quot;numWrites&quot;</span>:<span class="number">91</span>,<span class="string">&quot;numErrors&quot;</span>:<span class="number">0</span>&#125; of appId:application_1627567033654_0630 to report_path</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>flinkx</tag>
      </tags>
  </entry>
  <entry>
    <title>streamis 中支持批作业的一个方法</title>
    <url>/2022/11/05/streamis-%E4%B8%AD%E6%94%AF%E6%8C%81%E6%89%B9%E4%BD%9C%E4%B8%9A%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>介绍如何在 streamis 中支持 flink 批作业运行。</p>
<span id="more"></span>

<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在 streamis 提交了一个 flink.sql 类型的批作业，执行的 flink sql 是</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="string">&#x27;linkis flink engine test!!!&#x27;</span></span><br></pre></td></tr></table></figure>

<p>执行结果失败</p>
<p><img src="/2022/11/05/streamis-%E4%B8%AD%E6%94%AF%E6%8C%81%E6%89%B9%E4%BD%9C%E4%B8%9A%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B9%E6%B3%95/163829464-96e1e3ab-0448-4bb1-aa08-29b560f8e48f-16504201338512.png" alt="image"></p>
<p>不过对应的 yarn application 的 FinalStatus 是 SUCCEEDED</p>
<p><img src="/2022/11/05/streamis-%E4%B8%AD%E6%94%AF%E6%8C%81%E6%89%B9%E4%BD%9C%E4%B8%9A%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B9%E6%B3%95/163829298-9c2e7b57-17e3-484a-8391-f5819115d461-16504201890874.png" alt="image"></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>streamis 提交作业的流程大致是</p>
<ol>
<li><p>构建一次性作业</p>
<p>访问接口<code>/streamis/streamJobManager/job/execute</code>，从数据库中获取 job 信息，封装到<code>StreamisTransformJob</code>实例 中，再经过一系列的转换（<code>Transform</code>）处理（添加 labels、source、作业配置、launchConfig）后得到一个<code>LaunchJob</code>实例。这个实例，会有 linkis 的 <code>SimpleOnceJobBuilder</code> 转成一个 <code>SubmittableSimpleOnceJob</code> 对象，封装了 linkis 客户端和引擎创建 action：<code>CreateEngineConnAction</code>，此外还会将作业要执行的 sql 作为资源上传到 HDFS 便于引擎执行时获取到</p>
</li>
<li><p>提交创建引擎</p>
<p>提交上文创建的 <code>SubmittableSimpleOnceJob</code>，通过 linkis 客户端执行请求<code>CreateEngineConnAction</code>，然后在一个循环中等待引擎就绪，得到一个<code>engineConnId</code>。linkis 启动引擎的过程中，会创建一个<code>FlinkCodeOnceExecutor</code>实例的执行器，这个执行器内部会存储 yarn application id 和 作业所在 node manager 的地址。保存<code>engineConnId</code>和<code>SubmittableSimpleOnceJob</code>实例的映射到缓存<code>onceJobs</code>中。</p>
</li>
<li><p>获取引擎信息</p>
<p>内部创建一个<code>EngineConnOperateAction</code>，通过 linkis 客户端请求，拿到上面的 yarn application id 和 node manger 地址，与 <code>engineConnId</code>、提交用户、ECM 实例 等封装一个 <code>FlinkJobInfo</code> 实例中。保存<code>engineConnId</code>和<code>FlinkJobInfo</code>实例的映射到缓存<code>onceJobIdToJobInfo</code>中。</p>
</li>
<li><p>更新作业状态</p>
<p>类<code>TaskMonitorService</code> 中有个定时任务，从数据库中获取未完成的任务，根据任务的 <code>engineConnId</code>从映射中拿到<code>SubmittableSimpleOnceJob</code>实例，内部会创建一个<code>GetEngineConnAction</code>，有 linkis 客户端发起请求，获取节点信息，里面包含了引擎的状态。</p>
</li>
</ol>
<p>streamis 的失败日志</p>
<figure class="highlight basic"><table><tr><td class="code"><pre><span class="line"><span class="symbol">24129 </span><span class="number">2022</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">19</span>:<span class="number">35</span>:<span class="number">35.091</span> INFO  [Linkis-Default-Scheduler-Thread-<span class="number">13</span>] <span class="keyword">com</span>.webank.wedatasphere.streamis.jobmanager.manager.service.TaskMonitorService <span class="number">41</span> info - Try <span class="keyword">to</span> update status of StreamJob-demo_flink_00.</span><br><span class="line"><span class="symbol">24130 </span><span class="number">2022</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">19</span>:<span class="number">35</span>:<span class="number">35.203</span> <span class="keyword">ERROR</span> [Linkis-Default-Scheduler-Thread-<span class="number">13</span>] <span class="keyword">com</span>.webank.wedatasphere.streamis.jobmanager.manager.service.TaskMonitorService <span class="number">58</span> <span class="keyword">error</span> - Fetch StreamJob-demo_flink_00 failed, maybe the Linkis cluster is wrong, please be noticed! org.apache.linkis.httpclient.exception.HttpClientResultException: errCode: <span class="number">10905</span> ,desc: URL /api/rest_j/v1/linkisManager/getEngineConn request failed! ResponseBody is &#123;<span class="string">&quot;method&quot;</span>:null,<span class="string">&quot;status&quot;</span>:<span class="number">1</span>,<span class="string">&quot;message&quot;</span>:<span class="string">&quot;errorcode(错误码): 210003, error message(错误信息): Not exists EngineConn(不存在的引擎).&quot;</span>,<span class="string">&quot;data&quot;</span>:&#123;<span class="string">&quot;errorMsg&quot;</span>:&#123;<span class="string">&quot;serviceKind&quot;</span>:<span class="string">&quot;linkis-cg-linkismanager&quot;</span>,<span class="string">&quot;level&quot;</span>:<span class="number">2</span>,<span class="string">&quot;port&quot;</span>:<span class="number">9111</span>,<span class="string">&quot;errCode&quot;</span>:<span class="number">210003</span>,<span class="string">&quot;ip&quot;</span>:<span class="string">&quot;10-177-198-114.ostream-test.dgtest01&quot;</span>, <span class="string">&quot;desc&quot;</span>:<span class="string">&quot;Not exists EngineConn(不存在的引擎)&quot;</span>&#125;&#125;&#125;. ,ip: <span class="number">10</span>-<span class="number">177</span>-<span class="number">198</span>-<span class="number">114</span>.ostream-test.dgtest01 ,port: <span class="number">9400</span> ,serviceKind: streamis-server</span><br></pre></td></tr></table></figure>

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line"><span class="number">2022</span>-<span class="number">04</span>-<span class="number">19</span> <span class="number">10</span>:<span class="number">59</span>:<span class="number">35.091</span> INFO  <span class="literal">[L<span class="identifier">inkis</span>-D<span class="identifier">efault</span>-S<span class="identifier">cheduler</span>-T<span class="identifier">hread</span>-<span class="number">14</span>]</span> com.webank.wedatasphere.streamis.jobmanager.manager.service.TaskMonitorService <span class="number">41</span> info - Try <span class="keyword">to</span> update status <span class="keyword">of</span> StreamJob-demo_flink_00.</span><br><span class="line"><span class="number">2022</span>-<span class="number">04</span>-<span class="number">19</span> <span class="number">10</span>:<span class="number">59</span>:<span class="number">35.117</span> INFO  <span class="literal">[L<span class="identifier">inkis</span>-D<span class="identifier">efault</span>-S<span class="identifier">cheduler</span>-T<span class="identifier">hread</span>-<span class="number">14</span>]</span> com.webank.wedatasphere.streamis.jobmanager.manager.service.TaskMonitorService <span class="number">222</span> updateStreamTaskStatus - StreamJob-demo_flink_00 is Failed <span class="keyword">with</span> <span class="constructor">FlinkJobInfo(<span class="params">id</span>: 36_20_8e315be6-<span class="params">d4f5</span>-4151-<span class="params">a02f</span>-<span class="params">a54daa53c10blinkis</span>-<span class="params">cg</span>-<span class="params">engineconn10</span>-177-198-114.<span class="params">ostream</span>-<span class="params">test</span>.<span class="params">dgtest01</span>:33836, <span class="params">status</span>: Failed, <span class="params">applicationId</span>: <span class="params">application_1644979452149_41155</span>, <span class="params">applicationUrl</span>: <span class="params">http</span>:<span class="operator">/</span><span class="operator">/</span>10-177-198-117.<span class="params">ostream</span>-<span class="params">test</span>.<span class="params">dgtest01</span>:36877, <span class="params">logPath</span>: <span class="params">null</span>)</span>.</span><br></pre></td></tr></table></figure>

<p>这两种日志原因是一个，引擎执行失败。</p>
<p>EngineConnServer 启动过程中会有两个定时任务</p>
<ol>
<li><code>org.apache.linkis.engineconn.acessible.executor.service.DefaultExecutorHeartbeatService</code> 会启动一个定时任务发送心跳<code>nodeHeartbeatMsg</code>，其中包含了执行器的状态，<code>org.apache.linkis.manager.am.service.heartbeat.AMHeartbeatService#heartbeatEventDeal</code>会来处理收到的心跳，将心跳<code>NodeHeartbeatMsg</code>处理成<code>nodeMetrics</code>，持久化到表<code>linkis_cg_manager_service_instance_metrics</code>中</li>
<li><code>org.apache.linkis.manager.am.service.heartbeat.AMHeartbeatService</code>这个定时任务会从数据库中获取EngineNode，检查状态，如果状态是完成（包括：Success&#x2F;Failed&#x2F;ShuttingDown），就会执行清理操作，通过 pid 将 EngineServer kill 掉，最后将 EngineNode 信息从数据库删除</li>
</ol>
<p>streamis 查询的时候，如果数据库中的引擎信息还在，就会返回下面的日志；如果不在了，就是上面的错误。</p>
<p>那为什么引擎会执行失败呢？从 EngineServer 的日志中大致可以看到，失败有两个地方</p>
<ol>
<li><p>FlinkCodeOnceExecutor 启动之后，有个等待启动的环节，这个方法<code>org.apache.linkis.engineconnplugin.flink.executor.FlinkOnceExecutor#waitToRunning</code>，这个方法内部会通过<code>clusterDescriptor.getJobStatus</code>获取状态</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> JobStatus <span class="title function_">getJobStatus</span><span class="params">()</span> <span class="keyword">throws</span> JobExecutionException &#123;</span><br><span class="line">    <span class="keyword">if</span> (jobId == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">JobExecutionException</span>(<span class="string">&quot;No job has been submitted. This is a bug.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> bridgeClientRequest(<span class="built_in">this</span>.executionContext, jobId, () -&gt; clusterClient.getJobStatus(jobId), <span class="literal">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里调用<code>clusterClient.getJobStatus(jobId)</code>会抛出下面的异常</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="number">22</span>/<span class="number">04</span>/<span class="number">19</span> <span class="number">10</span>:<span class="number">58</span>:<span class="number">07</span> ERROR deployment<span class="selector-class">.ClusterDescriptorAdapter</span>: Job: <span class="number">0</span>ea16c3a35df0649d6100c14b18c33c6 operation failed!</span><br><span class="line"><span class="number">22</span>/<span class="number">04</span>/<span class="number">19</span> <span class="number">10</span>:<span class="number">58</span>:<span class="number">07</span> ERROR executor<span class="selector-class">.FlinkCodeOnceExecutor</span>: Fetch job status failed! retried ++<span class="number">1</span>...</span><br><span class="line">LinkisException&#123;errCode=<span class="number">16023</span>, desc=<span class="string">&#x27;Job: 0ea16c3a35df0649d6100c14b18c33c6 operation failed!&#x27;</span>, ip=<span class="string">&#x27;10-177-198-114.test.dgtest01&#x27;</span>, port=<span class="number">33836</span>, serviceKind=<span class="string">&#x27;linkis-cg-engineconn&#x27;</span>&#125;</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.client</span><span class="selector-class">.deployment</span><span class="selector-class">.ClusterDescriptorAdapter</span><span class="selector-class">.bridgeClientRequest</span>(ClusterDescriptorAdapter<span class="selector-class">.java</span>:<span class="number">136</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.client</span><span class="selector-class">.deployment</span><span class="selector-class">.ClusterDescriptorAdapter</span><span class="selector-class">.getJobStatus</span>(ClusterDescriptorAdapter<span class="selector-class">.java</span>:<span class="number">88</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span>$<span class="variable">$anonfun</span>$<span class="number">2</span><span class="selector-class">.apply</span>(FlinkOnceExecutor<span class="selector-class">.scala</span>:<span class="number">87</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span>$<span class="variable">$anonfun</span>$<span class="number">2</span><span class="selector-class">.apply</span>(FlinkOnceExecutor<span class="selector-class">.scala</span>:<span class="number">87</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.common</span><span class="selector-class">.utils</span>.Utils$<span class="selector-class">.tryCatch</span>(Utils<span class="selector-class">.scala</span>:<span class="number">40</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span><span class="selector-class">.run</span>(FlinkOnceExecutor<span class="selector-class">.scala</span>:<span class="number">87</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.Executors<span class="variable">$RunnableAdapter</span><span class="selector-class">.call</span>(Executors<span class="selector-class">.java</span>:<span class="number">511</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.FutureTask</span><span class="selector-class">.runAndReset</span>(FutureTask<span class="selector-class">.java</span>:<span class="number">308</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span>.access$<span class="number">301</span>(ScheduledThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">180</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span><span class="selector-class">.run</span>(ScheduledThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">294</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.ThreadPoolExecutor</span><span class="selector-class">.runWorker</span>(ThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">1149</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.ThreadPoolExecutor<span class="variable">$Worker</span><span class="selector-class">.run</span>(ThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">624</span>)</span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.Thread</span><span class="selector-class">.run</span>(Thread<span class="selector-class">.java</span>:<span class="number">748</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Flink 获取结果超时，<code>org.apache.linkis.engineconnplugin.flink.executor.FlinkCodeOnceExecutor#doSubmit</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(onceExecutorExecutionContext: <span class="type">OnceExecutorExecutionContext</span>,</span><br><span class="line">                      options: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">	<span class="comment">// ...</span></span><br><span class="line">    future = <span class="type">Utils</span>.defaultScheduler.submit(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            info(<span class="string">&quot;Try to execute codes.&quot;</span>)</span><br><span class="line">            <span class="type">RelMetadataQueryBase</span>.<span class="type">THREAD_PROVIDERS</span>.set(<span class="type">JaninoRelMetadataProvider</span>.of(<span class="type">FlinkDefaultRelMetadataProvider</span>.<span class="type">INSTANCE</span>))</span><br><span class="line">            <span class="type">Utils</span>.tryCatch(<span class="type">CodeParserFactory</span>.getCodeParser(<span class="type">CodeType</span>.<span class="type">SQL</span>).parse(codes).filter(<span class="type">StringUtils</span>.isNotBlank).foreach(runCode))&#123; t =&gt;</span><br><span class="line">                error(<span class="string">&quot;Run code failed!&quot;</span>, t)</span><br><span class="line">                setResponse(<span class="type">ErrorExecuteResponse</span>(<span class="string">&quot;Run code failed!&quot;</span>, t))</span><br><span class="line">                tryFailed()</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">            info(<span class="string">&quot;All codes completed, now stop FlinkEngineConn.&quot;</span>)</span><br><span class="line">            trySucceed()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">this</span> synchronized wait()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里在 <code>runCode</code> 中获取结果会超时，在方法<code>tryFailed()</code>将状态修改为 Failed。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="number">22</span>/<span class="number">04</span>/<span class="number">19</span> <span class="number">10</span>:<span class="number">59</span>:<span class="number">13</span> ERROR executor<span class="selector-class">.FlinkCodeOnceExecutor</span>: Run <span class="selector-tag">code</span> failed!</span><br><span class="line">java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.ExecutionException</span>: java<span class="selector-class">.lang</span><span class="selector-class">.RuntimeException</span>: Failed to fetch next result</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.CompletableFuture</span><span class="selector-class">.reportGet</span>(CompletableFuture<span class="selector-class">.java</span>:<span class="number">357</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.CompletableFuture</span><span class="selector-class">.get</span>(CompletableFuture<span class="selector-class">.java</span>:<span class="number">1895</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.table</span><span class="selector-class">.api</span><span class="selector-class">.internal</span><span class="selector-class">.TableResultImpl</span><span class="selector-class">.awaitInternal</span>(TableResultImpl<span class="selector-class">.java</span>:<span class="number">123</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.table</span><span class="selector-class">.api</span><span class="selector-class">.internal</span><span class="selector-class">.TableResultImpl</span><span class="selector-class">.await</span>(TableResultImpl<span class="selector-class">.java</span>:<span class="number">86</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span><span class="selector-class">.FlinkCodeOnceExecutor</span><span class="selector-class">.runCode</span>(FlinkCodeOnceExecutor<span class="selector-class">.scala</span>:<span class="number">122</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkCodeOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span>$<span class="variable">$anonfun</span><span class="variable">$run</span>$<span class="number">1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$mcV</span><span class="variable">$sp</span>$<span class="number">2</span><span class="selector-class">.apply</span>(FlinkCodeOnceExecutor<span class="selector-class">.scala</span>:<span class="number">72</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkCodeOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span>$<span class="variable">$anonfun</span><span class="variable">$run</span>$<span class="number">1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$mcV</span><span class="variable">$sp</span>$<span class="number">2</span><span class="selector-class">.apply</span>(FlinkCodeOnceExecutor<span class="selector-class">.scala</span>:<span class="number">72</span>)</span><br><span class="line">	at scala<span class="selector-class">.collection</span>.IndexedSeqOptimized<span class="variable">$class</span><span class="selector-class">.foreach</span>(IndexedSeqOptimized<span class="selector-class">.scala</span>:<span class="number">33</span>)</span><br><span class="line">	at scala<span class="selector-class">.collection</span><span class="selector-class">.mutable</span>.ArrayOps<span class="variable">$ofRef</span><span class="selector-class">.foreach</span>(ArrayOps<span class="selector-class">.scala</span>:<span class="number">186</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkCodeOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span>$<span class="variable">$anonfun</span><span class="variable">$run</span>$<span class="number">1</span>.apply<span class="variable">$mcV</span><span class="variable">$sp</span>(FlinkCodeOnceExecutor<span class="selector-class">.scala</span>:<span class="number">72</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkCodeOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span>$<span class="variable">$anonfun</span><span class="variable">$run</span>$<span class="number">1</span><span class="selector-class">.apply</span>(FlinkCodeOnceExecutor<span class="selector-class">.scala</span>:<span class="number">72</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkCodeOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span>$<span class="variable">$anonfun</span><span class="variable">$run</span>$<span class="number">1</span><span class="selector-class">.apply</span>(FlinkCodeOnceExecutor<span class="selector-class">.scala</span>:<span class="number">72</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.common</span><span class="selector-class">.utils</span>.Utils$<span class="selector-class">.tryCatch</span>(Utils<span class="selector-class">.scala</span>:<span class="number">40</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.linkis</span><span class="selector-class">.engineconnplugin</span><span class="selector-class">.flink</span><span class="selector-class">.executor</span>.FlinkCodeOnceExecutor$<span class="variable">$anon</span>$<span class="number">1</span><span class="selector-class">.run</span>(FlinkCodeOnceExecutor<span class="selector-class">.scala</span>:<span class="number">72</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.Executors<span class="variable">$RunnableAdapter</span><span class="selector-class">.call</span>(Executors<span class="selector-class">.java</span>:<span class="number">511</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.FutureTask</span><span class="selector-class">.run</span>(FutureTask<span class="selector-class">.java</span>:<span class="number">266</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span>.access$<span class="number">201</span>(ScheduledThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">180</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.ScheduledThreadPoolExecutor<span class="variable">$ScheduledFutureTask</span><span class="selector-class">.run</span>(ScheduledThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">293</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.ThreadPoolExecutor</span><span class="selector-class">.runWorker</span>(ThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">1149</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.ThreadPoolExecutor<span class="variable">$Worker</span><span class="selector-class">.run</span>(ThreadPoolExecutor<span class="selector-class">.java</span>:<span class="number">624</span>)</span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.Thread</span><span class="selector-class">.run</span>(Thread<span class="selector-class">.java</span>:<span class="number">748</span>)</span><br><span class="line">Caused by: java<span class="selector-class">.lang</span><span class="selector-class">.RuntimeException</span>: Failed to fetch next result</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.collect</span><span class="selector-class">.CollectResultIterator</span><span class="selector-class">.nextResultFromFetcher</span>(CollectResultIterator<span class="selector-class">.java</span>:<span class="number">109</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.collect</span><span class="selector-class">.CollectResultIterator</span><span class="selector-class">.hasNext</span>(CollectResultIterator<span class="selector-class">.java</span>:<span class="number">80</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.table</span><span class="selector-class">.planner</span><span class="selector-class">.sinks</span>.SelectTableSinkBase<span class="variable">$RowIteratorWrapper</span><span class="selector-class">.hasNext</span>(SelectTableSinkBase<span class="selector-class">.java</span>:<span class="number">117</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.table</span><span class="selector-class">.api</span><span class="selector-class">.internal</span>.TableResultImpl<span class="variable">$CloseableRowIteratorWrapper</span><span class="selector-class">.hasNext</span>(TableResultImpl<span class="selector-class">.java</span>:<span class="number">350</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.table</span><span class="selector-class">.api</span><span class="selector-class">.internal</span>.TableResultImpl<span class="variable">$CloseableRowIteratorWrapper</span><span class="selector-class">.isFirstRowReady</span>(TableResultImpl<span class="selector-class">.java</span>:<span class="number">363</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.table</span><span class="selector-class">.api</span><span class="selector-class">.internal</span><span class="selector-class">.TableResultImpl</span>.lambda<span class="variable">$awaitInternal</span>$<span class="number">1</span>(TableResultImpl<span class="selector-class">.java</span>:<span class="number">110</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.CompletableFuture<span class="variable">$AsyncRun</span><span class="selector-class">.run</span>(CompletableFuture<span class="selector-class">.java</span>:<span class="number">1626</span>)</span><br><span class="line">	... <span class="number">3</span> more</span><br><span class="line">Caused by: java<span class="selector-class">.io</span><span class="selector-class">.IOException</span>: Failed to fetch job execution result</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.collect</span><span class="selector-class">.CollectResultFetcher</span><span class="selector-class">.getAccumulatorResults</span>(CollectResultFetcher<span class="selector-class">.java</span>:<span class="number">169</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.collect</span><span class="selector-class">.CollectResultFetcher</span><span class="selector-class">.next</span>(CollectResultFetcher<span class="selector-class">.java</span>:<span class="number">118</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.collect</span><span class="selector-class">.CollectResultIterator</span><span class="selector-class">.nextResultFromFetcher</span>(CollectResultIterator<span class="selector-class">.java</span>:<span class="number">106</span>)</span><br><span class="line">	... <span class="number">9</span> more</span><br><span class="line">Caused by: java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.TimeoutException</span></span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.CompletableFuture</span><span class="selector-class">.timedGet</span>(CompletableFuture<span class="selector-class">.java</span>:<span class="number">1771</span>)</span><br><span class="line">	at java<span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.CompletableFuture</span><span class="selector-class">.get</span>(CompletableFuture<span class="selector-class">.java</span>:<span class="number">1915</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.collect</span><span class="selector-class">.CollectResultFetcher</span><span class="selector-class">.getAccumulatorResults</span>(CollectResultFetcher<span class="selector-class">.java</span>:<span class="number">167</span>)</span><br><span class="line">	... <span class="number">11</span> more</span><br></pre></td></tr></table></figure></li>
</ol>
<p>上面的这个问题提了 issue，我个人的理解是：这是与 Flink 的 per-job 模式相关的，per-job 模式的特点的一个任务一个集群，任务运行完毕，集群就被销毁掉了，所以在任务结束后想获取任务状态和结果的行不通的。</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><ol>
<li>Flink per job 模式下，任务执行结束后，通过回调返回状态</li>
<li>streamis 以 session 模式提交</li>
<li>streamis 直接查询 yarn 获取任务状态</li>
</ol>
<p>这三种方案中，</p>
<ul>
<li><p>第一个由于需要修改 flink 的代码，比较复杂，调用方（linkis）也要做相应修改，比如要传一个回调地址给 flink，要有处理回调的逻辑。</p>
</li>
<li><p>第二个，虽然可以解决 per job 模式下，任务运行结束后集群销毁的问题，但是产生更多的问题，比如 session 集群里面，可以执行多个任务，获取某一个任务的执行状态，需要额外的处理；由于多个任务同时运行在一个集群中，资源隔离也是个问题</p>
</li>
<li><p>关于第三个方案，从前面的分析中，在 streamis 提交任务的过程中，会从 linkis 获取一个<code>FlinkJobInfo</code>，这个对象封装了 applicationId 和 applicationUrl，这个两个变量是在方法<code>org.apache.linkis.engineconnplugin.flink.executor.FlinkOnceExecutor#submit</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(onceExecutorExecutionContext: <span class="type">OnceExecutorExecutionContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">ClusterDescriptorAdapterFactory</span>.create(flinkEngineConnContext.getExecutionContext) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> adapter: <span class="type">T</span> =&gt; clusterDescriptor = adapter</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ExecutorInitException</span>(<span class="string">&quot;Not support ClusterDescriptorAdapter for flink application.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> options = onceExecutorExecutionContext.getOnceExecutorContent.getJobContent.map &#123;</span><br><span class="line">        <span class="keyword">case</span> (k, v: <span class="type">String</span>) =&gt; k -&gt; v</span><br><span class="line">        <span class="keyword">case</span> (k, v) <span class="keyword">if</span> v != <span class="literal">null</span> =&gt; k -&gt; v.toString</span><br><span class="line">        <span class="keyword">case</span> (k, _) =&gt; k -&gt; <span class="literal">null</span></span><br><span class="line">    &#125;.toMap</span><br><span class="line">    doSubmit(onceExecutorExecutionContext, options)</span><br><span class="line">    <span class="keyword">if</span>(isCompleted) <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="literal">null</span> == clusterDescriptor.getClusterID)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ExecutorInitException</span>(<span class="string">&quot;The application start failed, since yarn applicationId is null.&quot;</span>)</span><br><span class="line">    setApplicationId(clusterDescriptor.getClusterID.toString)</span><br><span class="line">    setApplicationURL(clusterDescriptor.getWebInterfaceUrl)</span><br><span class="line">    info(<span class="string">s&quot;Application is started, applicationId: <span class="subst">$getApplicationId</span>, applicationURL: <span class="subst">$getApplicationURL</span>.&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span>(clusterDescriptor.getJobId != <span class="literal">null</span>) setJobID(clusterDescriptor.getJobId.toHexString)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从日志中看到 applicationId 和 applicationURL 的取值</p>
<figure class="highlight dns"><table><tr><td class="code"><pre><span class="line">./linkis-cg-engineconnmanager.out:<span class="number">8</span>e315be6-d<span class="number">4f5-4151</span>-a02f-a54daa53c10b:<span class="number">22/04/19 10</span>:<span class="number">56</span>:<span class="number">52</span> INFO executor.FlinkCodeOnceExecutor: Application is started, applicationId: application_1644979<span class="number">452149_41155</span>, applicationURL: http://<span class="number">10-177-198-117</span>.ostream-test.dgtest<span class="number">01:36877</span></span><br></pre></td></tr></table></figure>

<p>这里 applicationURL  是与任务绑定的（端口：36877），问题是任务跑结束后，这个端口的访问不了的。不过，已经有了 applicationId，通过 yarn 的接口就可以知道这个任务运行的结果。yarn 接口地址从哪里获取？</p>
<p>这个方法进来后，先会创建一个<code>clusterDescriptor</code>，在 streamis 这个场景中，会创建一个<code>org.apache.linkis.engineconnplugin.flink.client.deployment.YarnPerJobClusterDescriptorAdapter</code>实例，它的父类<code>org.apache.linkis.engineconnplugin.flink.client.deployment.ClusterDescriptorAdapter</code> 中有一个<code>YarnClusterDescriptor</code>类型的成员，这个类中有一个<code>YarnClient</code>对象<code>yarnClient</code>。</p>
<p><code>yarnClient</code>这个对象的创建会用到配置文件<code>yarn-conf.xml</code>，通过处理可以获取到配置文件中 resource manager 的地址，将这个地址保存到执行器中。</p>
<p>当 streamis 来获取引擎信息的时候，设置到返回消息中，具体是这个方法<code>org.apache.linkis.engineconn.acessible.executor.operator.impl.EngineConnApplicationInfoOperator#apply</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(<span class="keyword">implicit</span> parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Any</span>]): <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Any</span>] = &#123;</span><br><span class="line">    <span class="type">ExecutorManager</span>.getInstance.getReportExecutor <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> yarnExecutor: <span class="type">YarnExecutor</span> =&gt;</span><br><span class="line">        <span class="type">Map</span>(<span class="string">&quot;applicationId&quot;</span> -&gt; yarnExecutor.getApplicationId, <span class="string">&quot;applicationUrl&quot;</span> -&gt; yarnExecutor.getApplicationURL,</span><br><span class="line">            <span class="string">&quot;queue&quot;</span> -&gt; yarnExecutor.getQueue, <span class="string">&quot;yarnMode&quot;</span> -&gt; yarnExecutor.getYarnMode)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="type">EngineConnException</span>(<span class="number">20301</span>, <span class="string">&quot;EngineConn is not a yarn application, cannot fetch applicaiton info.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>streamis 其实有将<code>FlinkJobInfo</code>保存在表<code>linkis_stream_task</code> 中</p>
<p><img src="/2022/11/05/streamis-%E4%B8%AD%E6%94%AF%E6%8C%81%E6%89%B9%E4%BD%9C%E4%B8%9A%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B9%E6%B3%95/image-20220420181541309.png" alt="image"></p>
<p>当<code>TaskMonitorService</code>中的定时任务从表<code>linkis_stream_task</code>获取到需要更新状态的作业的，替换原来更新逻辑为调用保存的 yarn rm 地址查询对应的 applicationId 的状态。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>streamis</tag>
      </tags>
  </entry>
  <entry>
    <title>flinkx 写 s3</title>
    <url>/2022/11/05/flinkx-%E5%86%99-s3/</url>
    <content><![CDATA[<p>介绍 flinkx 1.11 写 aws s3 过程中遇到的问题及处理。</p>
<span id="more"></span>

<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在新加坡部署集成服务，写 hdfs 的作业执行结束后，在 S3 上面看不到数据。</p>
<h2 id="分析-amp-解决"><a href="#分析-amp-解决" class="headerlink" title="分析&amp;解决"></a>分析&amp;解决</h2><p>写 S3 有两种方式</p>
<ol>
<li>使用社区的 <a href="https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html">hadoop-aws</a> 模块，实现一个 HDFS 文件系统：S3AFileSystem</li>
<li>使用 aws 的 <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-fs.html">emrfs</a>，实现一个 HDFS 文件系统：EmrFileSystem</li>
</ol>
<p>考虑到这个问题之前在欧盟出现过，是通过 hadoop-aws 这个模块写的。所以这次先按照这个思路走。</p>
<blockquote>
<p>flinkx 1.7 与 1.11 有些区别，1.7 中使用的是 flink shade 后的 hadoop 包，1.11 中使用的是未 shade 的 hadoop 包。</p>
</blockquote>
<p><img src="/2022/11/05/flinkx-%E5%86%99-s3/image-20220406145535986-16492304138792.png" alt="image-20220406145535986"></p>
<p>从 <code>org.apache.hadoop.fs.FileSystem</code> 可以看到依赖的是 hadoop 2.7.3 版本，所以在 <code>flinkx-hdfs/flinkx-hdfs-writer/pom.xml</code> 中引入依赖 2.7.3 版本的 hadoop-aws</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-aws<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>保持版本一致，否则会出现各种问题</p>
</blockquote>
<p>重新打包，提交任务，任务成功执行。检查 S3 发现没有数据。接下来，打开日志，查看 hadoop-aws 模块的运行。日志打开方法如下：</p>
<p>修改 <code>/home/service/datahub/flink-1.11-ostream/conf</code> 中的 <code>log4j.properties</code></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">log4j<span class="selector-class">.logger</span><span class="selector-class">.org</span><span class="selector-class">.apache</span>.hadoop=DEBUG</span><br></pre></td></tr></table></figure>

<p>重新执行任务，从日志大致看到，hadoop-aws 写文件的过程大致是，先写到 node manager 一个临时的目录，例如<code>/mnt/var/lib/hadoop/tmp/s3a/output-6052406643142588357.tmp</code>，然后在刷新的时候，将文件上传到 S3。</p>
<p>从日志中看到，在执行完<code>com.dtstack.flinkx.outputformat.BaseFileOutputFormat#flushData</code>后，hadoop-aws 将对象上传到 s3 了。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">51</span>,<span class="number">900</span> INFO  com.dtstack.flinkx.hdfs.writer.HdfsTextOutputFormat           - Close current text stream, write data size:[<span class="number">1563</span>]</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">51</span>,<span class="number">900</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - OutputStream for key &#x27;warehouse/compass_2/dataland_test_db.db/test_yl_0325/.data/<span class="number">0</span>.<span class="number">12</span>c6031f80ede3449f01b72916912c0f.<span class="number">0</span>&#x27; closed. Now beginning upload</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">51</span>,<span class="number">900</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Minimum upload part size: <span class="number">104857600</span> threshold <span class="number">2147483647</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">51</span>,<span class="number">946</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Getting path status for /warehouse/compass_2/dataland_test_db.db/test_yl_0325/.data (warehouse/compass_2/dataland_test_db.db/test_yl_0325/.data)</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">241</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Found path as directory (with /): <span class="number">0</span>/<span class="number">1</span></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">241</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Summary: warehouse/compass_2/dataland_test_db.db/test_yl_0325/.data/<span class="number">0</span>.<span class="number">12</span>c6031f80ede3449f01b72916912c0f.<span class="number">0</span> <span class="number">1543</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">241</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Getting path status for /warehouse/compass_2/dataland_test_db.db/test_yl_0325 (warehouse/compass_2/dataland_test_db.db/test_yl_0325)</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">287</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Found file (with /): fake directory</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">287</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Deleting fake directory warehouse/compass_2/dataland_test_db.db/test_yl_0325/</span><br><span class="line"></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">312</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Getting path status for /warehouse/compass_2/dataland_test_db.db (warehouse/compass_2/dataland_test_db.db)</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">384</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Found path as directory (with /): <span class="number">1</span>/<span class="number">0</span></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">384</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Prefix: warehouse/compass_2/dataland_test_db.db/cx_iceberg_sg_1/</span><br><span class="line"></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">384</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Getting path status for /warehouse/compass_2 (warehouse/compass_2)</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">451</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Found path as directory (with /): <span class="number">1</span>/<span class="number">0</span></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">451</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Prefix: warehouse/compass_2/dataland_test_db.db/</span><br><span class="line"></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">451</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Getting path status for /warehouse (warehouse)</span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">533</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Found path as directory (with /): <span class="number">1</span>/<span class="number">0</span></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">534</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - Prefix: warehouse/Strategy_data/</span><br><span class="line"></span><br><span class="line"><span class="attribute">2022</span>-<span class="number">04</span>-<span class="number">02</span> <span class="number">06</span>:<span class="number">42</span>:<span class="number">52</span>,<span class="number">534</span> DEBUG org.apache.hadoop.fs.s3a.S3AFileSystem                        - OutputStream for key &#x27;warehouse/compass_2/dataland_test_db.db/test_yl_0325/.data/<span class="number">0</span>.<span class="number">12</span>c6031f80ede3449f01b72916912c0f.<span class="number">0</span>&#x27; upload complete</span><br></pre></td></tr></table></figure>

<p>但是为什么 S3 上就是没有呢？</p>
<blockquote>
<p>这个问题需要 AWS 帮查看，没有继续看。</p>
</blockquote>
<p>hadoop-aws 这个方法走不通了，那 emrfs 该怎么使用呢？</p>
<p>Flink 对 S3 提供了支持，具体见文档 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.11/zh/ops/filesystems/s3.html">Amazon S3</a>，这里提到了两个文件系统插件的实现，一个是 <code>flink-s3-fs-presto</code> 和 <code>flink-s3-fs-hadoop</code>。插件的使用方式是</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">mkdir .<span class="regexp">/plugins/</span>s3-fs-hadoop</span><br><span class="line">cp .<span class="regexp">/opt/</span>flink-s3-fs-hadoop-<span class="number">1.11</span>.<span class="number">3</span>.jar .<span class="regexp">/plugins/</span>s3-fs-hadoop/</span><br></pre></td></tr></table></figure>

<blockquote>
<p>中间有把<code>flink-s3-fs-hadoop-1.11.3.jar</code> 放到了 <code>lib</code> 下，同时把 <code>flink-shaded-hadoop-2-uber-2.6.5-10.0.jar</code> 删掉了，报错如下</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">java<span class="selector-class">.lang</span><span class="selector-class">.NoClassDefFoundError</span>: Could not initialize class org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.runtime</span><span class="selector-class">.entrypoint</span><span class="selector-class">.parser</span>.CommandLineOptions</span><br></pre></td></tr></table></figure>

<p>来自<a href="https://blog.csdn.net/u011462328/article/details/116199776">Flink ON YARN 报错及解决方案</a></p>
</blockquote>
<p>提交作业，执行失败，报错如下</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">Caused by: java<span class="selector-class">.lang</span><span class="selector-class">.NoSuchMethodError</span>: org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.conf</span><span class="selector-class">.Configuration</span><span class="selector-class">.setAllowNullValueProperties</span>(Z)V</span><br><span class="line">	at com<span class="selector-class">.amazon</span><span class="selector-class">.ws</span><span class="selector-class">.emr</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span><span class="selector-class">.EmrFileSystem</span><span class="selector-class">.initializeConfiguration</span>(EmrFileSystem<span class="selector-class">.java</span>:<span class="number">128</span>)</span><br><span class="line">	at com<span class="selector-class">.amazon</span><span class="selector-class">.ws</span><span class="selector-class">.emr</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span><span class="selector-class">.EmrFileSystem</span><span class="selector-class">.initialize</span>(EmrFileSystem<span class="selector-class">.java</span>:<span class="number">96</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span><span class="selector-class">.FileSystem</span><span class="selector-class">.createFileSystem</span>(FileSystem<span class="selector-class">.java</span>:<span class="number">2598</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span><span class="selector-class">.FileSystem</span>.access$<span class="number">200</span>(FileSystem<span class="selector-class">.java</span>:<span class="number">91</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span>.FileSystem<span class="variable">$Cache</span><span class="selector-class">.getInternal</span>(FileSystem<span class="selector-class">.java</span>:<span class="number">2632</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span>.FileSystem<span class="variable">$Cache</span><span class="selector-class">.get</span>(FileSystem<span class="selector-class">.java</span>:<span class="number">2614</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span><span class="selector-class">.FileSystem</span><span class="selector-class">.get</span>(FileSystem<span class="selector-class">.java</span>:<span class="number">370</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.fs</span><span class="selector-class">.FileSystem</span><span class="selector-class">.get</span>(FileSystem<span class="selector-class">.java</span>:<span class="number">169</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.util</span><span class="selector-class">.FileSystemUtil</span><span class="selector-class">.getFileSystem</span>(FileSystemUtil<span class="selector-class">.java</span>:<span class="number">62</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.hdfs</span><span class="selector-class">.writer</span><span class="selector-class">.BaseHdfsOutputFormat</span><span class="selector-class">.openSource</span>(BaseHdfsOutputFormat<span class="selector-class">.java</span>:<span class="number">354</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.outputformat</span><span class="selector-class">.BaseFileOutputFormat</span><span class="selector-class">.openInternal</span>(BaseFileOutputFormat<span class="selector-class">.java</span>:<span class="number">103</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.hdfs</span><span class="selector-class">.writer</span><span class="selector-class">.BaseHdfsOutputFormat</span><span class="selector-class">.openInternal</span>(BaseHdfsOutputFormat<span class="selector-class">.java</span>:<span class="number">111</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.outputformat</span><span class="selector-class">.BaseRichOutputFormat</span><span class="selector-class">.open</span>(BaseRichOutputFormat<span class="selector-class">.java</span>:<span class="number">258</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.functions</span><span class="selector-class">.sink</span><span class="selector-class">.DtOutputFormatSinkFunction</span><span class="selector-class">.open</span>(DtOutputFormatSinkFunction<span class="selector-class">.java</span>:<span class="number">89</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.api</span><span class="selector-class">.common</span><span class="selector-class">.functions</span><span class="selector-class">.util</span><span class="selector-class">.FunctionUtils</span><span class="selector-class">.openFunction</span>(FunctionUtils<span class="selector-class">.java</span>:<span class="number">36</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.AbstractUdfStreamOperator</span><span class="selector-class">.open</span>(AbstractUdfStreamOperator<span class="selector-class">.java</span>:<span class="number">102</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.StreamSink</span><span class="selector-class">.open</span>(StreamSink<span class="selector-class">.java</span>:<span class="number">48</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span><span class="selector-class">.OperatorChain</span><span class="selector-class">.initializeStateAndOpenOperators</span>(OperatorChain<span class="selector-class">.java</span>:<span class="number">291</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span><span class="selector-class">.StreamTask</span>.lambda<span class="variable">$beforeInvoke</span>$<span class="number">0</span>(StreamTask<span class="selector-class">.java</span>:<span class="number">473</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span>.StreamTaskActionExecutor$<span class="number">1</span><span class="selector-class">.runThrowing</span>(StreamTaskActionExecutor<span class="selector-class">.java</span>:<span class="number">47</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span><span class="selector-class">.StreamTask</span><span class="selector-class">.beforeInvoke</span>(StreamTask<span class="selector-class">.java</span>:<span class="number">469</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span><span class="selector-class">.StreamTask</span><span class="selector-class">.invoke</span>(StreamTask<span class="selector-class">.java</span>:<span class="number">522</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.runtime</span><span class="selector-class">.taskmanager</span><span class="selector-class">.Task</span><span class="selector-class">.doRun</span>(Task<span class="selector-class">.java</span>:<span class="number">721</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.runtime</span><span class="selector-class">.taskmanager</span><span class="selector-class">.Task</span><span class="selector-class">.run</span>(Task<span class="selector-class">.java</span>:<span class="number">546</span>)</span><br><span class="line">	at java<span class="selector-class">.lang</span><span class="selector-class">.Thread</span><span class="selector-class">.run</span>(Thread<span class="selector-class">.java</span>:<span class="number">748</span>)</span><br></pre></td></tr></table></figure>

<p>方法 <code>org.apache.hadoop.conf.Configuration.setAllowNullValueProperties</code> 找不到。</p>
<blockquote>
<p>从这里也看到使用的文件系统是 <code>EmrFileSystem</code></p>
</blockquote>
<p>反编译了 <code>flink-shaded-hadoop-2-uber-2.6.5-10.0.jar</code>，确实没有上述方法</p>
<p><img src="/2022/11/05/flinkx-%E5%86%99-s3/image-20220406153313924-16492304035861.png" alt="image-20220406153313924"></p>
<p>查阅文档 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.11/ops/deployment/hadoop.html#hadoop-integration">Hadoop Integration</a>，flink 从 1.11 版本之后，不再提供官方的 <code>flink-shaded-hadoop-2-uber</code>，不过可以手动从<a href="https://flink.apache.org/downloads.html#additional-components">这里</a>下载</p>
<p>确认 hadoop 版本 , 2.8.5 版本，下载 <code>flink-shaded-hadoop-2-uber-2.8.3-10.0.jar</code>，与此同时，将 flink 改为使用社区提供的 1.11.3 版本，代替我们自己编译的 ostream-1.11 版本。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[amy@ip-10-1-42-72 ~]$ hadoop version</span><br><span class="line">Hadoop 2.8.5-amzn-5</span><br><span class="line">Subversion git@aws157git.com:/pkg/Aws157BigTop -r a3b61461af0d6b4d981c915b0a1f342464987aaa</span><br><span class="line">Compiled by ec2-user on 2019-12-14T09:05Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum fad06c90f0f460226b0a91c6c1926d4</span><br><span class="line">This command was run using /usr/lib/hadoop/hadoop-common-2.8.5-amzn-5.jar</span><br></pre></td></tr></table></figure>

<p>重新提交作业执行成功，S3 也能看到数据。</p>
<p>后面遇到作业看不到日志的问题，解决办法是将原有 flink 目录下的 conf 目录整体拷贝到新目录。</p>
]]></content>
  </entry>
  <entry>
    <title>类冲突导致作业执行失败</title>
    <url>/2022/11/05/%E7%B1%BB%E5%86%B2%E7%AA%81%E5%AF%BC%E8%87%B4%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%A4%B1%E8%B4%A5/</url>
    <content><![CDATA[<p>介绍了 maven 中 exclusion 排包失败，后通过 shade 插件排除成功的例子。</p>
<span id="more"></span>

<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>执行hive到sqlserver任务，配置了限流器，报错方法找不到</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">Caused by: java<span class="selector-class">.lang</span><span class="selector-class">.NoSuchMethodError</span>: shade<span class="selector-class">.core</span><span class="selector-class">.com</span><span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.util</span><span class="selector-class">.concurrent</span><span class="selector-class">.RateLimiter</span><span class="selector-class">.acquire</span>()D</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.reader</span><span class="selector-class">.ByteRateLimiter</span><span class="selector-class">.acquire</span>(ByteRateLimiter<span class="selector-class">.java</span>:<span class="number">79</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.inputformat</span><span class="selector-class">.BaseRichInputFormat</span><span class="selector-class">.nextRecord</span>(BaseRichInputFormat<span class="selector-class">.java</span>:<span class="number">280</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.inputformat</span><span class="selector-class">.BaseRichInputFormat</span><span class="selector-class">.nextRecord</span>(BaseRichInputFormat<span class="selector-class">.java</span>:<span class="number">66</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.functions</span><span class="selector-class">.source</span><span class="selector-class">.DtInputFormatSourceFunction</span><span class="selector-class">.run</span>(DtInputFormatSourceFunction<span class="selector-class">.java</span>:<span class="number">137</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.StreamSource</span><span class="selector-class">.run</span>(StreamSource<span class="selector-class">.java</span>:<span class="number">100</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.StreamSource</span><span class="selector-class">.run</span>(StreamSource<span class="selector-class">.java</span>:<span class="number">63</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span>.SourceStreamTask<span class="variable">$LegacySourceFunctionThread</span><span class="selector-class">.run</span>(SourceStreamTask<span class="selector-class">.java</span>:<span class="number">201</span>)</span><br></pre></td></tr></table></figure>

<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>看下这个方法<code>com.dtstack.flinkx.reader.ByteRateLimiter#acquire</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">acquire</span><span class="params">()</span> &#123;</span><br><span class="line">    rateLimiter.acquire();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个方法调用了<code>com.google.common.util.concurrent.RateLimiter#acquire()</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">double</span> <span class="title function_">acquire</span><span class="params">()</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">this</span>.acquire(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再看下hive reader jar包的情况，也有一个<code>RateLimiter</code>类，但是acquire方法是没有返回值的。</p>
<p><img src="/2022/11/05/%E7%B1%BB%E5%86%B2%E7%AA%81%E5%AF%BC%E8%87%B4%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%A4%B1%E8%B4%A5/image-20210701145006895.png" alt="image-20210701145006895"></p>
<p>这样看来是hive reader这个插件打包的时候，引入了低版本的guava，导致上面方法找不到错误。</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>记录下解决过程</p>
<p>对于类冲突，解决思路是：使用maven helper插件分析出引入冲突的依赖，然后在依赖中通过exclusion排除冲突的jar。</p>
<p>排除之后是这样的</p>
<p><img src="/2022/11/05/%E7%B1%BB%E5%86%B2%E7%AA%81%E5%AF%BC%E8%87%B4%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%A4%B1%E8%B4%A5/image-20210701142900903.png" alt="image-20210701142900903"></p>
<p>重新打包后检查，依然发现旧版的guava包打进来了。</p>
<p><img src="/2022/11/05/%E7%B1%BB%E5%86%B2%E7%AA%81%E5%AF%BC%E8%87%B4%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%A4%B1%E8%B4%A5/image-20210701145006895.png" alt="image-20210701145006895"></p>
<p>上面不是通过exclusion排除掉了么，为什么还是会有呢？这个问题困扰了几天，中间尝试过通过dependencyManagement锁定版本；在maven-shade-plugin中排除guava包。</p>
<p>转机出现在，既然总会引入一个guava，明确引入一个指定版本的guava包，根据最短路径原则是不是就可以了。</p>
<p>打包时有一段告警</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="selector-attr">[WARNING]</span> guava-<span class="number">19.0</span><span class="selector-class">.jar</span>, hive-exec-<span class="number">1.1</span>.<span class="number">1</span><span class="selector-class">.jar</span> define <span class="number">1326</span> overlapping classes: </span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.cache</span>.LocalCache<span class="variable">$KeyIterator</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.collect</span>.ImmutableMapValues$<span class="number">1</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.collect</span>.WellBehavedMap<span class="variable">$EntrySet</span>$<span class="number">1</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.util</span><span class="selector-class">.concurrent</span>.AbstractService$<span class="number">5</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.io</span><span class="selector-class">.LineProcessor</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.io</span>.BaseEncoding<span class="variable">$StandardBaseEncoding</span>$<span class="number">2</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.collect</span>.ComputingConcurrentHashMap<span class="variable">$ComputingSerializationProxy</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.reflect</span><span class="selector-class">.ImmutableTypeToInstanceMap</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.io</span><span class="selector-class">.ByteProcessor</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - com<span class="selector-class">.google</span><span class="selector-class">.common</span><span class="selector-class">.io</span><span class="selector-class">.InputSupplier</span></span><br><span class="line"><span class="selector-attr">[WARNING]</span>   - <span class="number">1316</span> more...</span><br></pre></td></tr></table></figure>

<p>这段告警的意思是说 guava-1.9.0.jar与hive-exec-1.1.1.jar有重叠的类。一下明白了，之前被打进来的guava包来自hive-exec-1.1.1.jar这个包。看样子，通过exclusion并没有排除掉</p>
<p><img src="/2022/11/05/%E7%B1%BB%E5%86%B2%E7%AA%81%E5%AF%BC%E8%87%B4%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%A4%B1%E8%B4%A5/image-20210701161730196.png" alt="image-20210701161730196"></p>
<p>可行的排除方式是利用maven-shade-plugin插件的filter</p>
<p><img src="/2022/11/05/%E7%B1%BB%E5%86%B2%E7%AA%81%E5%AF%BC%E8%87%B4%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%A4%B1%E8%B4%A5/image-20210701161849981.png" alt="image-20210701161849981"></p>
<p>最后看下hive-exec-1.1.1.jar中guava是如何打进来的，原来是通过maven-shade-plugin的include引入的。</p>
<p><img src="/2022/11/05/%E7%B1%BB%E5%86%B2%E7%AA%81%E5%AF%BC%E8%87%B4%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E5%A4%B1%E8%B4%A5/image-20210701162923474.png" alt="image-20210701162923474"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>对于通过maven-shade-plugin插件引入的jar，为什么exclusive就不生效了呢？</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://segmentfault.com/a/1190000023446358">用好这几个技巧，解决Maven Jar包冲突易如反掌</a></p>
<p><a href="https://developer.aliyun.com/article/632130">[Maven]maven-shade-plugin使用指南</a></p>
<p><a href="%5Bhttps://chaycao.github.io/2020/10/31/%E4%B8%80%E6%AC%A1Maven%E4%BE%9D%E8%B5%96%E5%86%B2%E7%AA%81%E9%87%87%E5%9D%91-%E6%8A%8A%E4%BE%9D%E8%B5%96%E8%B0%83%E8%A7%A3-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%BD%BB%E5%BA%95%E6%95%B4%E6%98%8E%E7%99%BD%E4%BA%86.html%5D(https://chaycao.github.io/2020/10/31/%E4%B8%80%E6%AC%A1Maven%E4%BE%9D%E8%B5%96%E5%86%B2%E7%AA%81%E9%87%87%E5%9D%91-%E6%8A%8A%E4%BE%9D%E8%B5%96%E8%B0%83%E8%A7%A3-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%BD%BB%E5%BA%95%E6%95%B4%E6%98%8E%E7%99%BD%E4%BA%86.html)">一次Maven依赖冲突踩坑，把依赖调解、类加载彻底整明白了</a></p>
<p><a href="https://blog.csdn.net/qq_32506245/article/details/113061321">MAVEN shade 插件解决JAR包冲突</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1622207">Maven打包之maven-shade-plugin</a></p>
<p><a href="https://run-zheng.github.io/2019/11/06/maven-shade-plugin/">maven-shade-plugin介绍</a></p>
<p><a href="https://www.playpi.org/2019120101.html">解决 jar 包冲突的神器：maven-shade-plugin</a></p>
]]></content>
  </entry>
</search>
