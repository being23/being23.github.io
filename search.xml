<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PG 读取大量数据因临时空间不足导致任务失败</title>
    <url>/2022/10/30/PG-%E8%AF%BB%E5%8F%96%E5%A4%A7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%9B%A0%E4%B8%B4%E6%97%B6%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%E5%AF%BC%E8%87%B4%E4%BB%BB%E5%8A%A1%E5%A4%B1%E8%B4%A5/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>业务方有一个同步 PG 的作业，数据量级大致是 1.5+ 亿条。任务执行失败，报错</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="number">2022</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">14</span>:<span class="number">31</span>:<span class="number">37.178</span> <span class="selector-attr">[flink-akka.actor.default-dispatcher-21]</span> INFO  org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.runtime</span><span class="selector-class">.executiongraph</span><span class="selector-class">.ExecutionGraph</span>  - Source: postgresqlreader (<span class="number">1</span>/<span class="number">1</span>) (<span class="number">77</span>bd8cb9299189456dceee9609b5d03a) switched from RUNNING to FAILED on org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.runtime</span><span class="selector-class">.jobmaster</span><span class="selector-class">.slotpool</span>.SingleLogicalSlot@<span class="number">55</span>bedd88.</span><br><span class="line">java<span class="selector-class">.lang</span><span class="selector-class">.IllegalArgumentException</span>: <span class="built_in">open</span>() failed<span class="selector-class">.ERROR</span>: temporary file size exceeds temp_file_limit (<span class="number">15728640</span>kB)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.rdb</span><span class="selector-class">.inputformat</span><span class="selector-class">.JdbcInputFormat</span><span class="selector-class">.openInternal</span>(JdbcInputFormat<span class="selector-class">.java</span>:<span class="number">154</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.inputformat</span><span class="selector-class">.BaseRichInputFormat</span><span class="selector-class">.open</span>(BaseRichInputFormat<span class="selector-class">.java</span>:<span class="number">183</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.functions</span><span class="selector-class">.source</span><span class="selector-class">.DtInputFormatSourceFunction</span><span class="selector-class">.run</span>(DtInputFormatSourceFunction<span class="selector-class">.java</span>:<span class="number">124</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.StreamSource</span><span class="selector-class">.run</span>(StreamSource<span class="selector-class">.java</span>:<span class="number">100</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.api</span><span class="selector-class">.operators</span><span class="selector-class">.StreamSource</span><span class="selector-class">.run</span>(StreamSource<span class="selector-class">.java</span>:<span class="number">63</span>)</span><br><span class="line">	at org<span class="selector-class">.apache</span><span class="selector-class">.flink</span><span class="selector-class">.streaming</span><span class="selector-class">.runtime</span><span class="selector-class">.tasks</span>.SourceStreamTask<span class="variable">$LegacySourceFunctionThread</span><span class="selector-class">.run</span>(SourceStreamTask<span class="selector-class">.java</span>:<span class="number">201</span>)</span><br><span class="line">Caused by: org<span class="selector-class">.postgresql</span><span class="selector-class">.util</span><span class="selector-class">.PSQLException</span>: ERROR: temporary file size exceeds temp_file_limit (<span class="number">15728640</span>kB)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.core</span><span class="selector-class">.v3</span><span class="selector-class">.QueryExecutorImpl</span><span class="selector-class">.receiveErrorResponse</span>(QueryExecutorImpl<span class="selector-class">.java</span>:<span class="number">2433</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.core</span><span class="selector-class">.v3</span><span class="selector-class">.QueryExecutorImpl</span><span class="selector-class">.processResults</span>(QueryExecutorImpl<span class="selector-class">.java</span>:<span class="number">2178</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.core</span><span class="selector-class">.v3</span><span class="selector-class">.QueryExecutorImpl</span><span class="selector-class">.execute</span>(QueryExecutorImpl<span class="selector-class">.java</span>:<span class="number">306</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeInternal</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">441</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.execute</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">365</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeWithFlags</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">307</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeCachedSql</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">293</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeWithFlags</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">270</span>)</span><br><span class="line">	at org<span class="selector-class">.postgresql</span><span class="selector-class">.jdbc</span><span class="selector-class">.PgStatement</span><span class="selector-class">.executeQuery</span>(PgStatement<span class="selector-class">.java</span>:<span class="number">224</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.rdb</span><span class="selector-class">.inputformat</span><span class="selector-class">.JdbcInputFormat</span><span class="selector-class">.executeQuery</span>(JdbcInputFormat<span class="selector-class">.java</span>:<span class="number">868</span>)</span><br><span class="line">	at com<span class="selector-class">.dtstack</span><span class="selector-class">.flinkx</span><span class="selector-class">.rdb</span><span class="selector-class">.inputformat</span><span class="selector-class">.JdbcInputFormat</span><span class="selector-class">.openInternal</span>(JdbcInputFormat<span class="selector-class">.java</span>:<span class="number">149</span>)</span><br><span class="line">	... <span class="number">5</span> common frames omitted</span><br></pre></td></tr></table></figure>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>这看起来就是 PG 数据库的问题，需要增加配置<code>temp_file_limit</code>。不过业务说 1.0 上可以执行，不放心用 2.0 的配置在 1.0 上手动提交了一个作业，居然可以。<br>从 2.0 的执行日志中获取了执行的 SQL</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> &quot;ts&quot;,&quot;asm_name&quot;,&quot;sequence&quot;,&quot;process_name&quot;,&quot;thread_name&quot;,&quot;pid&quot;,&quot;tid&quot;,&quot;energy_raw&quot;,&quot;uniform_time_ms&quot;,&quot;curr_time&quot; <span class="keyword">FROM</span> &quot;ctp_metrics27&quot; <span class="keyword">WHERE</span> <span class="number">1</span><span class="operator">=</span><span class="number">1</span>   <span class="keyword">and</span> &quot;curr_time&quot; <span class="operator">&gt;=</span> <span class="string">&#x27;2022-10-27 00:00:00.000000&#x27;</span> <span class="keyword">and</span> &quot;curr_time&quot; <span class="operator">&lt;</span> <span class="string">&#x27;2022-10-28 00:00:00.000000&#x27;</span> <span class="keyword">order</span> <span class="keyword">by</span> curr_time</span><br></pre></td></tr></table></figure>
<p>注意到，这个 sql 的最后多了 <code>order by</code>，这是个多余的操作，而且为了做排序，必定会使用到临时空间，考虑到数据量级，空间不足就是理应会发生的了。<br>看看这个 <code>order by</code> 是在哪里添加的——<br>flinkx 中同步关系型数据库的逻辑，简单的概括就是<code>InputFormat</code>的<code>open</code>方法中创建一个数据库连接，执行 SQL，得到结果集，其中执行的 SQL 的生成分成两步</p>
<ol>
<li>在调用方法<code>com.dtstack.flinkx.rdb.datareader.JdbcDataReader#readData</code>处理配置时，调用<code>com.dtstack.flinkx.rdb.datareader.QuerySqlBuilder#buildSql</code>生成一个 SQL 模板，其中包含了增量&#x2F;恢复、切分条件的占位符</li>
<li>在调用方法<code>com.dtstack.flinkx.rdb.inputformat.JdbcInputFormat#openInternal</code>打开数据库时，调用<code>com.dtstack.flinkx.rdb.inputformat.JdbcInputFormat#buildQuerySql</code>将模板中的占位符替换掉，得到最终的 SQL</li>
</ol>
<p>PG 特殊的地方在于它将父类的方法<code>com.dtstack.flinkx.postgresql.reader.PostgresqlQuerySqlBuilder#buildQuerySql</code>给覆盖掉了，覆盖后的方法与父类的差异多了一行</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> String <span class="title function_">buildQuerySql</span><span class="params">()</span>&#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  sb.append(buildOrderSql()); <span class="comment">// 多了这一行</span></span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//com.dtstack.flinkx.rdb.datareader.QuerySqlBuilder#buildOrderSql</span></span><br><span class="line"><span class="keyword">protected</span> String <span class="title function_">buildOrderSql</span><span class="params">()</span>&#123;</span><br><span class="line">  String column;</span><br><span class="line">  <span class="keyword">if</span>(isIncrement)&#123;</span><br><span class="line">    column = incrementColumn;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span>(isRestore)&#123;</span><br><span class="line">    column = restoreColumn;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    column = orderByColumn;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> StringUtils.isEmpty(column) ? <span class="string">&quot;&quot;</span> : String.format(<span class="string">&quot; order by %s&quot;</span>, column);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，当任务是增量任务时，会添加<code>order by 增量字段</code>。</p>
<h2 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h2><p>这段逻辑是没有必要的，去掉即可。</p>
]]></content>
      <tags>
        <tag>flinkx</tag>
        <tag>问题</tag>
      </tags>
  </entry>
  <entry>
    <title>flinkx 1.7 jar 上传逻辑</title>
    <url>/2022/10/30/flinkx-1-7-jar-%E4%B8%8A%E4%BC%A0%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<p>flinkx 1.7 任务启动过程中要上传的 jar 分两部分</p>
<ol>
<li>flink lib 目录下的 jar，又细分成两部分 flink-dist 和 非 flink-dist，以及<del>具体的 flinkx 插件 jar 包</del></li>
<li>flinkx 插件目录下的 jar，又细分成 flinkx.jar （对应 flinkx-core 模块）和具体插件的 jar</li>
</ol>
<p>在具体的上传过程中，<br>flink-dist 单独上传</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Path</span> <span class="variable">remotePathJar</span> <span class="operator">=</span> setupSingleLocalResource(</span><br><span class="line">  <span class="string">&quot;flink.jar&quot;</span>,</span><br><span class="line">  fs,</span><br><span class="line">  appId,</span><br><span class="line">  flinkJarPath,</span><br><span class="line">  localResources,</span><br><span class="line">  homeDir,</span><br><span class="line">  <span class="string">&quot;&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>其他的 jar 作业 user jar 一起上传</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (jobGraph != <span class="literal">null</span>) &#123;</span><br><span class="line">  <span class="comment">// add the user code jars from the provided JobGraph</span></span><br><span class="line">  <span class="comment">// 这里所说的 user code jars 就是 flinkx plugin 目录下的 flinkx.jar</span></span><br><span class="line">  <span class="keyword">for</span> (org.apache.flink.core.fs.Path path : jobGraph.getUserJars()) &#123;</span><br><span class="line">    userJarFiles.add(<span class="keyword">new</span> <span class="title class_">File</span>(path.toUri()));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) &#123;</span><br><span class="line">  userClassPaths = uploadAndRegisterFiles(</span><br><span class="line">    userJarFiles,</span><br><span class="line">    fs,</span><br><span class="line">    homeDir,</span><br><span class="line">    appId,</span><br><span class="line">    paths,</span><br><span class="line">    localResources,</span><br><span class="line">    envShipFileList);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  userClassPaths = Collections.emptyList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>userJarFiles</code> 的初始化是在<code>com.dtstack.flinkx.launcher.perjob.PerJobClusterClientBuilder#createPerJobClusterDescriptor</code>，这里将 flink lib 目录下除 flink-dist jar 之外的所有 jar 放到了 <code>userJarFiles</code> 中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;URL&gt; classpaths = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"><span class="keyword">if</span> (flinkJarPath != <span class="literal">null</span>) &#123;</span><br><span class="line">  File[] jars = <span class="keyword">new</span> <span class="title class_">File</span>(flinkJarPath).listFiles();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (File file : jars)&#123;</span><br><span class="line">    <span class="keyword">if</span> (file.toURI().toURL().toString().contains(<span class="string">&quot;flink-dist&quot;</span>))&#123;</span><br><span class="line">      clusterDescriptor.setLocalJarPath(<span class="keyword">new</span> <span class="title class_">Path</span>(file.toURI().toURL().toString()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      classpaths.add(file.toURI().toURL());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;The Flink jar path is null&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">clusterDescriptor.setProvidedUserJarFiles(classpaths);</span><br></pre></td></tr></table></figure>

<p>这个过程中存在问题是，所有的插件 jar 在 flink lib 目录下都存在了一份，提交的作业时不管有没有使用到都会上传。</p>
<p>理想的状态是，任务使用了哪些插件，就上传这些插件对应的 jar。</p>
<p>任务使用了哪些插件，可以从任务配置文件中解析得到。这个解析过程，在<code>com.dtstack.flinkx.launcher.Launcher#analyzeUserClasspath</code>方法中已经完成，保存在<code>clusterSpecification.classpaths</code> 中。并且在后续获取 <code>PackagedProgram</code> 和 <code>JobGraph</code> 对象时，又保存到了各自对象的 <code>classpaths</code> 变量中。</p>
<p>一个处理的方法是：</p>
<ol>
<li>将插件包从 flink lib 目录下删除</li>
<li>上面上传 <code>userJarFiles</code> 表示的 jar 包之前，从 <code>jobGraph</code> 中获取了 <code>flinkx.jar</code>，可以做一个类似的操作，即：将 <code>jobGraph.classpath</code> 中的 jar （就是任务需要的插件包）一并添加到 <code>userJarFiles</code> 中，待后续上传。</li>
<li>还有个方法，同事给出的，是在 <code>com.dtstack.flinkx.launcher.perjob.PerJobClusterClientBuilder#createPerJobClusterDescriptor</code>方法中设置 classpaths 时，从 clusterSpefication 中获取解析出的插件包添加到 classpaths 中。</li>
</ol>
<p>如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;URL&gt; classpaths = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">clusterSpecification.getClasspaths().forEach(jar-&gt;classpaths.add(jar));</span><br><span class="line"><span class="keyword">if</span> (flinkJarPath != <span class="literal">null</span>) &#123;</span><br><span class="line">  File[] jars = <span class="keyword">new</span> <span class="title class_">File</span>(flinkJarPath).listFiles();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (File file : jars)&#123;</span><br><span class="line">    <span class="keyword">if</span> (file.toURI().toURL().toString().contains(<span class="string">&quot;flink-dist&quot;</span>))&#123;</span><br><span class="line">      clusterDescriptor.setLocalJarPath(<span class="keyword">new</span> <span class="title class_">Path</span>(file.toURI().toURL().toString()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      classpaths.add(file.toURI().toURL());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;The Flink jar path is null&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">clusterDescriptor.setProvidedUserJarFiles(classpaths);</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>flinkx</tag>
      </tags>
  </entry>
  <entry>
    <title>hive 读取超时</title>
    <url>/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/</url>
    <content><![CDATA[<p>业务反馈 hive 读取作业执行失败<br><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220707143134989_1657247857902_0.png" alt="image-20220707143134989.png"><br>查看了这些失败作业，发现有些许作业是读超时了，异常栈如下</p>
<pre><code>Caused by: java.net.SocketTimeoutException: Read timed out
at java.net.SocketInputStream.socketRead0(Native Method)
at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
at java.net.SocketInputStream.read(SocketInputStream.java:171)
at java.net.SocketInputStream.read(SocketInputStream.java:141)
at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
... 30 more
</code></pre>
<p>方法<code>java.net.SocketInputStream#socketRead0</code>的定义是</p>
<figure class="highlight aspectj"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">native</span> <span class="function"><span class="keyword">int</span> <span class="title">socketRead0</span><span class="params">(FileDescriptor fd,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">byte</span> b[], <span class="keyword">int</span> off, <span class="keyword">int</span> len,</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">int</span> timeout)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException</span>;</span><br></pre></td></tr></table></figure>
<p>其中，参数<code>timeout</code>的定义是<code>the read timeout in ms</code>。<code>timeout</code>在 141 行调用，由下面的方法传入，即<code>impl.getTimeout()</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">read</span><span class="params">(<span class="type">byte</span> b[], <span class="type">int</span> off, <span class="type">int</span> length)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  <span class="keyword">return</span> read(b, off, length, impl.getTimeout());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里<code>impl</code>是类<code>AbstractPlainSocketImpl</code>的一个实例，方法<code>getTimeout()</code>返回成员变量<code>timeout</code>，该成员在方法<code>java.net.AbstractPlainSocketImpl#setOption</code>中设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setOption</span><span class="params">(<span class="type">int</span> opt, Object val)</span> <span class="keyword">throws</span> SocketException &#123;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  <span class="keyword">case</span> SO_TIMEOUT:</span><br><span class="line">  <span class="keyword">if</span> (val == <span class="literal">null</span> || (!(val <span class="keyword">instanceof</span> Integer)))</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">SocketException</span>(<span class="string">&quot;Bad parameter for SO_TIMEOUT&quot;</span>);</span><br><span class="line">  <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> ((Integer) val).intValue();</span><br><span class="line">  <span class="keyword">if</span> (tmp &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(<span class="string">&quot;timeout &lt; 0&quot;</span>);</span><br><span class="line">  timeout = tmp;</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法有 3 处调用<br><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220708105432620_1657264951019_0.png" alt="image-20220708105432620.png"><br>如何知道哪个调用是与 hive 有关系呢？不太好找，因为有太多的调用方了。换个思路，从 hive 入手，知道肯定会执行到这里，所以打个断点，当执行到断点时，查看调用栈，就知道执行过程了。<br><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220708144551916_1657264974671_0.png" alt="image-20220708144551916.png"><br>这里大致流程是<br><code>com.dtstack.flinkx.hive.format.HiveInputFormat#openInternal</code>拿到查询 SQL<code>querySql</code>并执行查询<br>查询是通过方法<code>com.dtstack.flinkx.hive.format.HiveInputFormat#executeQuery</code>实现，首先获取数据库连接<code>dbConn</code><br>数据库连接通过方法<code>com.dtstack.flinkx.rdb.inputformat.JdbcInputFormat#getConnection</code> 获取，内部有个重试，调用<code>DriverManager.getConnection()</code><br><code>DriverManager</code>内部遍历注册的 drivers ，并连接，见方法<code>java.sql.DriverManager#getConnection(java.lang.String, java.util.Properties, java.lang.Class&lt;?&gt;)</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(DriverInfo aDriver : registeredDrivers) &#123;</span><br><span class="line">  <span class="comment">// If the caller does not have permission to load the driver then</span></span><br><span class="line">  <span class="comment">// skip it.</span></span><br><span class="line">  <span class="keyword">if</span>(isDriverAllowed(aDriver.driver, callerCL)) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      println(<span class="string">&quot;    trying &quot;</span> + aDriver.driver.getClass().getName());</span><br><span class="line">      <span class="type">Connection</span> <span class="variable">con</span> <span class="operator">=</span> aDriver.driver.connect(url, info); <span class="comment">//连接</span></span><br><span class="line">      <span class="keyword">if</span> (con != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// Success!</span></span><br><span class="line">        println(<span class="string">&quot;getConnection returning &quot;</span> + aDriver.driver.getClass().getName());</span><br><span class="line">        <span class="keyword">return</span> (con);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SQLException ex) &#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    println(<span class="string">&quot;    skipping: &quot;</span> + aDriver.getClass().getName());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Hive 场景下，注册的 driver 是 <code>HiveDriver</code>，connect 方法中就是创建一个<code>HiveConnection</code> 对象</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Connection <span class="title function_">connect</span><span class="params">(String url, Properties info)</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  <span class="keyword">return</span> acceptsURL(url) ? <span class="keyword">new</span> <span class="title class_">HiveConnection</span>(url, info) : <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对象<code>HiveConnection</code>的构造方法如下</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">HiveConnection</span><span class="params">(String uri, Properties info)</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  setupLoginTimeout(); </span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    connParams = Utils.parseURL(uri);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ZooKeeperHiveClientException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">SQLException</span>(e);</span><br><span class="line">  &#125;</span><br><span class="line">  jdbcUriString = connParams.getJdbcUriString();</span><br><span class="line">  <span class="comment">// extract parsed connection parameters:</span></span><br><span class="line">  <span class="comment">// JDBC URL: jdbc:hive2://&lt;host&gt;:&lt;port&gt;/dbName;sess_var_list?hive_conf_list#hive_var_list</span></span><br><span class="line">  <span class="comment">// each list: &lt;key1&gt;=&lt;val1&gt;;&lt;key2&gt;=&lt;val2&gt; and so on</span></span><br><span class="line">  <span class="comment">// sess_var_list -&gt; sessConfMap</span></span><br><span class="line">  <span class="comment">// hive_conf_list -&gt; hiveConfMap</span></span><br><span class="line">  <span class="comment">// hive_var_list -&gt; hiveVarMap</span></span><br><span class="line">  host = connParams.getHost();</span><br><span class="line">  port = connParams.getPort();</span><br><span class="line">  sessConfMap = connParams.getSessionVars();</span><br><span class="line">  hiveConfMap = connParams.getHiveConfs();</span><br><span class="line"></span><br><span class="line">  hiveVarMap = connParams.getHiveVars();</span><br><span class="line">  <span class="keyword">for</span> (Map.Entry&lt;Object, Object&gt; kv : info.entrySet()) &#123;</span><br><span class="line">    <span class="keyword">if</span> ((kv.getKey() <span class="keyword">instanceof</span> String)) &#123;</span><br><span class="line">      <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> (String) kv.getKey();</span><br><span class="line">      <span class="keyword">if</span> (key.startsWith(HIVE_VAR_PREFIX)) &#123;</span><br><span class="line">        hiveVarMap.put(key.substring(HIVE_VAR_PREFIX.length()), info.getProperty(key));</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.startsWith(HIVE_CONF_PREFIX)) &#123;</span><br><span class="line">        hiveConfMap.put(key.substring(HIVE_CONF_PREFIX.length()), info.getProperty(key));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  isEmbeddedMode = connParams.isEmbeddedMode();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isEmbeddedMode) &#123;</span><br><span class="line">    <span class="type">EmbeddedThriftBinaryCLIService</span> <span class="variable">embeddedClient</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">EmbeddedThriftBinaryCLIService</span>();</span><br><span class="line">    embeddedClient.init(<span class="keyword">new</span> <span class="title class_">HiveConf</span>());</span><br><span class="line">    client = embeddedClient;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// extract user/password from JDBC connection properties if its not supplied in the</span></span><br><span class="line">    <span class="comment">// connection URL</span></span><br><span class="line">    <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_USER)) &#123;</span><br><span class="line">      sessConfMap.put(JdbcConnectionParams.AUTH_USER, info.getProperty(JdbcConnectionParams.AUTH_USER));</span><br><span class="line">      <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_PASSWD)) &#123;</span><br><span class="line">        sessConfMap.put(JdbcConnectionParams.AUTH_PASSWD, info.getProperty(JdbcConnectionParams.AUTH_PASSWD));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_TYPE)) &#123;</span><br><span class="line">      sessConfMap.put(JdbcConnectionParams.AUTH_TYPE, info.getProperty(JdbcConnectionParams.AUTH_TYPE));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// open the client transport</span></span><br><span class="line">    openTransport(); <span class="comment">// 从这里开始</span></span><br><span class="line">    <span class="comment">// set up the client</span></span><br><span class="line">    client = <span class="keyword">new</span> <span class="title class_">TCLIService</span>.Client(<span class="keyword">new</span> <span class="title class_">TBinaryProtocol</span>(transport));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// add supported protocols</span></span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V1);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V2);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V3);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V4);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V5);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V6);</span><br><span class="line">  supportedProtocols.add(TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V7);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// open client session</span></span><br><span class="line">  openSession();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Wrap the client with a thread-safe proxy to serialize the RPC calls</span></span><br><span class="line">  client = newSynchronizedClient(client);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用方法<code>setupLoginTimeout()</code>设置<code>loginTimeout</code>，内部获取的是<code>java.sql.DriverManager#loginTimeout</code>，这个变量是类的 static 成员，就是参考资料中提到的多个 JDBC driver 驱动存在时，会被覆盖导致问题<br>调用<code>org.apache.hive.jdbc.Utils#parseURL</code>解析 jdbc url 连接， <code>jdbc:hive2://&lt;host1&gt;:&lt;port1&gt;,&lt;host2&gt;:&lt;port2&gt;/dbName;sess_var_list?hive_conf_list#hive_var_list</code>，其中几个 list 是以<code>;</code>分隔的键值对，分别对应 URI 的 path、query和fragment，内部也是用 URI 类来提取的各个部分的<br>调用方法<code>org.apache.hive.jdbc.HiveConnection#openTransport</code>打开客户端连接，判断是个二进制协议还是 http 协议，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">openTransport</span><span class="params">()</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  <span class="type">int</span> <span class="variable">numRetries</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> <span class="variable">maxRetries</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    maxRetries = Integer.parseInt(sessConfMap.get(JdbcConnectionParams.RETRIES));</span><br><span class="line">  &#125; <span class="keyword">catch</span>(NumberFormatException e) &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      assumeSubject =</span><br><span class="line">        JdbcConnectionParams.AUTH_KERBEROS_AUTH_TYPE_FROM_SUBJECT.equals(sessConfMap</span><br><span class="line">                                                                         .get(JdbcConnectionParams.AUTH_KERBEROS_AUTH_TYPE));</span><br><span class="line">      transport = isHttpTransportMode() ? createHttpTransport() : createBinaryTransport(); <span class="comment">//判断是个二进制协议还是 http 协议，</span></span><br><span class="line">      <span class="keyword">if</span> (!transport.isOpen()) &#123;</span><br><span class="line">        transport.open();</span><br><span class="line">      &#125;</span><br><span class="line">      logZkDiscoveryMessage(<span class="string">&quot;Connected to &quot;</span> + connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (TTransportException e) &#123;</span><br><span class="line">      <span class="comment">// We&#x27;ll retry till we exhaust all HiveServer2 nodes from ZooKeeper</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里是二进制，调用方法<code>org.apache.hive.jdbc.HiveConnection#createBinaryTransport</code>，内部调用<code>org.apache.hive.jdbc.HiveConnection#createUnderlyingTransport</code>创建底层的 transport</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> TTransport <span class="title function_">createUnderlyingTransport</span><span class="params">()</span> <span class="keyword">throws</span> TTransportException &#123;</span><br><span class="line">  <span class="type">TTransport</span> <span class="variable">transport</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">  <span class="comment">// Note: Thrift returns an SSL socket that is already bound to the specified host:port</span></span><br><span class="line">  <span class="comment">// Therefore an open called on this would be a no-op later</span></span><br><span class="line">  <span class="comment">// Hence, any TTransportException related to connecting with the peer are thrown here.</span></span><br><span class="line">  <span class="comment">// Bubbling them up the call hierarchy so that a retry can happen in openTransport,</span></span><br><span class="line">  <span class="comment">// if dynamic service discovery is configured.</span></span><br><span class="line">  <span class="keyword">if</span> (isSslConnection()) &#123;</span><br><span class="line">    <span class="comment">// get SSL socket</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// get non-SSL socket transport</span></span><br><span class="line">    transport = HiveAuthUtils.getSocketTransport(host, port, loginTimeout);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> transport;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里获取 transport 的时候，传了<code>loginTimeout</code><br>方法<code>org.apache.hadoop.hive.common.auth.HiveAuthUtils#getSocketTransport</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> TTransport <span class="title function_">getSocketTransport</span><span class="params">(String host, <span class="type">int</span> port, <span class="type">int</span> loginTimeout)</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">TSocket</span>(host, port, loginTimeout);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>继续看 <code>TSocket</code> 的初始化</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">TSocket</span><span class="params">(String host, <span class="type">int</span> port, <span class="type">int</span> timeout)</span> &#123;</span><br><span class="line">  <span class="built_in">this</span>.socket_ = <span class="literal">null</span>;</span><br><span class="line">  <span class="built_in">this</span>.host_ = <span class="literal">null</span>;</span><br><span class="line">  <span class="built_in">this</span>.port_ = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">this</span>.timeout_ = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">this</span>.host_ = host;</span><br><span class="line">  <span class="built_in">this</span>.port_ = port;</span><br><span class="line">  <span class="built_in">this</span>.timeout_ = timeout;</span><br><span class="line">  <span class="built_in">this</span>.initSocket(); <span class="comment">//</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">initSocket</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="built_in">this</span>.socket_ = <span class="keyword">new</span> <span class="title class_">Socket</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.socket_.setSoLinger(<span class="literal">false</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">this</span>.socket_.setTcpNoDelay(<span class="literal">true</span>);</span><br><span class="line">    <span class="built_in">this</span>.socket_.setKeepAlive(<span class="literal">true</span>);</span><br><span class="line">    <span class="built_in">this</span>.socket_.setSoTimeout(<span class="built_in">this</span>.timeout_); <span class="comment">//</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> (SocketException var2) &#123;</span><br><span class="line">    LOGGER.error(<span class="string">&quot;Could not configure socket.&quot;</span>, var2);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>java.net.Socket#setSoTimeout</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">setSoTimeout</span><span class="params">(<span class="type">int</span> timeout)</span> <span class="keyword">throws</span> SocketException &#123;</span><br><span class="line">  <span class="keyword">if</span> (isClosed())</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">SocketException</span>(<span class="string">&quot;Socket is closed&quot;</span>);</span><br><span class="line">  <span class="keyword">if</span> (timeout &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(<span class="string">&quot;timeout can&#x27;t be negative&quot;</span>);</span><br><span class="line"></span><br><span class="line">  getImpl().setOption(SocketOptions.SO_TIMEOUT, <span class="keyword">new</span> <span class="title class_">Integer</span>(timeout)); <span class="comment">//</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里设置的是 socket 的读取超时，实际上 timeout_，也用于 socket 连接超时<br>最后看下，<code>java.sql.DriverManager#loginTimeout</code>是如何设置的，类似的，添加断点，发现是在方法<code>com.dtstack.flinkx.util.ClassUtil#forName(java.lang.String, java.lang.ClassLoader)</code>中设置，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">forName</span><span class="params">(String clazz, ClassLoader classLoader)</span>  &#123;</span><br><span class="line">  <span class="keyword">synchronized</span> (LOCK_STR)&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Class.forName(clazz, <span class="literal">true</span>, classLoader);</span><br><span class="line">      DriverManager.setLoginTimeout(<span class="number">10</span>); <span class="comment">//10s</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2022/10/30/hive-%E8%AF%BB%E5%8F%96%E8%B6%85%E6%97%B6/image-20220708110105135_1657265225155_0.png" alt="image-20220708110105135.png"></p>
]]></content>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>mongo 任务执行成功但数据量不对</title>
    <url>/2022/10/30/mongo-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E6%88%90%E5%8A%9F%E4%BD%86%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B8%8D%E5%AF%B9/</url>
    <content><![CDATA[<p>业务反馈了一个同步 hive 数据到 mongo 的作业，同步结束后，mongo 里面的数据量与 hive 的对不上，任务运行日志中有 118 条记录写失败了。<br>初看这个任务有一点奇怪：flinkx 中有一个数据量的检测机制，如果写失败了，错误记录数或者错误记录比例超过一定阈值，作业就失败了。这个作业有记录写失败了，为什么还是成功的呢？带着这个疑惑，重新看了下错误检测机制，分为两个部分：初始化和校验</p>
<p>在<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#open</code> 内部会调用方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#initStatisticsAccumulator</code>初始化累加器（比如错误记录数 <code>errCounter</code>），具体是通过 flink runtime 创建计数器，并将这些计数器添加到指标组中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">initStatisticsAccumulator</span><span class="params">()</span>&#123;</span><br><span class="line">  errCounter = context.getLongCounter(Metrics.NUM_ERRORS);</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  outputMetric = <span class="keyword">new</span> <span class="title class_">BaseMetric</span>(context);</span><br><span class="line">  outputMetric.addMetric(Metrics.NUM_ERRORS, errCounter);</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">  startTime = System.currentTimeMillis();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后调用方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#initAccumulatorCollector</code>初始话计数器收集器<code>accumulatorCollector</code>，内部会启动一个定时任务，执行方法<code>com.dtstack.flinkx.metrics.AccumulatorCollector#collectAccumulatorWithApi</code>从 JM 获取计数器的值。<br>校验过程有两处：每一行数据写之前；以及所有数据写完。<br>写之前是在方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#writeSingleRecord</code>中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">writeSingleRecord</span><span class="params">(Row row)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span>(errorLimiter != <span class="literal">null</span>) &#123;</span><br><span class="line">    errorLimiter.acquire();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    writeSingleRecordInternal(row);</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里先调用方法<code>com.dtstack.flinkx.writer.ErrorLimiter#acquire</code>获取错误的情况，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">acquire</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">String</span> <span class="variable">errorDataStr</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">  <span class="keyword">if</span>(errorData != <span class="literal">null</span>)&#123;</span><br><span class="line">    errorDataStr = errorData.toString() + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">long</span> <span class="variable">errors</span> <span class="operator">=</span> accumulatorCollector.getAccumulatorValue(Metrics.NUM_ERRORS);</span><br><span class="line">  <span class="keyword">if</span>(maxErrors != <span class="literal">null</span> &amp;&amp; !maxErrors.equals(<span class="number">0</span>))&#123;</span><br><span class="line">    Preconditions.checkArgument(errors &lt;= maxErrors, <span class="string">&quot;WritingRecordError: error writing record [&quot;</span> + errors + <span class="string">&quot;] exceed limit [&quot;</span> + maxErrors</span><br><span class="line">                                + <span class="string">&quot;]\n&quot;</span> + errorDataStr + errMsg);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span>(maxErrorRatio != <span class="literal">null</span>)&#123;</span><br><span class="line">    <span class="type">long</span> <span class="variable">numRead</span> <span class="operator">=</span> accumulatorCollector.getAccumulatorValue(Metrics.NUM_READS);</span><br><span class="line">    <span class="keyword">if</span>(numRead &gt;= <span class="number">1</span>) &#123;</span><br><span class="line">      errorRatio = (<span class="type">double</span>) errors / numRead;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Preconditions.checkArgument(errorRatio &lt;= maxErrorRatio, <span class="string">&quot;WritingRecordError: error writing record ratio [&quot;</span> + errorRatio + <span class="string">&quot;] exceed limit [&quot;</span> + maxErrorRatio</span><br><span class="line">                                + <span class="string">&quot;]\n&quot;</span> + errorDataStr + errMsg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从计数器收集器<code>accumulatorCollector</code>中获取错误数<code>errors</code>和错误比例（错误数&#x2F;读取数）<code>errorRatio = (**double**) errors / numRead</code>。如果超过最大错误数<code>maxErrors</code>或者最大错误比例<code>maxErrorRatio</code>，抛出异常。<br>由于上面的检测是在写之前，对于最后一次写，还是有可能失败的，所以写完之后，又有一次检测，也是调用方法<code>com.dtstack.flinkx.writer.ErrorLimiter#acquire</code>。具体是在方法<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#checkErrorLimit</code>中，由<code>com.dtstack.flinkx.outputformat.BaseRichOutputFormat#close</code>方法调用。<br>至此，错误检测机制说明白了。那为什么没有生效呢？可能的原因就是检测条件不成立，即<code>maxErrors</code>为 null 或者 0；<code>maxErrorRatio</code> 为 null。顺着这个思路找到这两个值初始化的源头，<code>com.dtstack.flinkx.mongodb.writer.MongodbWriter#writeData</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> DataStreamSink&lt;?&gt; writeData(List&lt;DataStream&lt;Row&gt;&gt; dataSets) &#123;</span><br><span class="line">  <span class="type">MongodbOutputFormatBuilder</span> <span class="variable">builder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MongodbOutputFormatBuilder</span>();</span><br><span class="line"></span><br><span class="line">  builder.setMongodbConfig(mongodbConfig);</span><br><span class="line">  builder.setColumns(columns);</span><br><span class="line">  builder.setMonitorUrls(monitorUrls);</span><br><span class="line">  builder.setErrors(errors);</span><br><span class="line">  builder.setDirtyPath(dirtyPath);</span><br><span class="line">  builder.setDirtyHadoopConfig(dirtyHadoopConfig);</span><br><span class="line">  builder.setSrcCols(srcCols);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> createOutput(dataSets, builder.finish());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看到这里处理了 <code>errors</code>，没有处理<code>errorRatio</code>。而<code>errors</code>是从南天门给的配置中获取的，取值为 0。如此两个检测条件都不成立，所以任务没有失败。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;errorLimit&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;percentage&quot;</span><span class="punctuation">:</span> <span class="number">0.01</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;record&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>flinkx</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>streamis-作业执行过程源码</title>
    <url>/2022/10/30/streamis-%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E6%BA%90%E7%A0%81/</url>
    <content><![CDATA[<h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><pre class="mermaid">flowchart TD
构建单次作业 --> 提交创建引擎
提交创建引擎 --> 获取引擎信息
获取引擎信息 --> 更新作业状态</pre>

<ol>
<li><p>构建一次性作业</p>
<p>访问接口<code>/streamis/streamJobManager/job/execute</code>，从数据库中获取 job 信息，封装到<code>StreamisTransformJob</code>实例 中，再经过一系列的转换（<code>Transform</code>）处理（添加 labels、source、作业配置、launchConfig）后得到一个<code>LaunchJob</code>实例。这个实例，会有 linkis 的 <code>SimpleOnceJobBuilder</code> 转成一个 <code>SubmittableSimpleOnceJob</code> 对象，封装了 linkis 客户端和引擎创建 action：<code>CreateEngineConnAction</code>，此外还会将作业要执行的 sql 作为资源上传到 HDFS 便于引擎执行时获取。</p>
</li>
<li><p>提交创建引擎</p>
<p>提交上文创建的 <code>SubmittableSimpleOnceJob</code>，通过 linkis 客户端执行请求<code>CreateEngineConnAction</code>，然后在一个循环中等待引擎就绪，得到一个<code>engineConnId</code>。linkis 启动引擎的过程中，会创建一个<code>FlinkCodeOnceExecutor</code>实例的执行器，这个执行器内部会存储 yarn application id 和 作业所在 node manager 的地址。保存<code>engineConnId</code>和<code>SubmittableSimpleOnceJob</code>实例的映射到缓存<code>onceJobs</code>中。</p>
</li>
<li><p>获取引擎信息</p>
<p>内部创建一个<code>EngineConnOperateAction</code>，通过 linkis 客户端请求，拿到上面的 yarn application id 和 node manger 地址，与 <code>engineConnId</code>、提交用户、ECM 实例 等封装一个 <code>FlinkJobInfo</code> 实例中。保存<code>engineConnId</code>和<code>FlinkJobInfo</code>实例的映射到缓存<code>onceJobIdToJobInfo</code>中。</p>
</li>
<li><p>更新作业状态</p>
<p>类<code>TaskMonitorService</code> 中有个定时任务，从数据库中获取未完成的任务，根据任务的 <code>engineConnId</code>从映射中拿到<code>SubmittableSimpleOnceJob</code>实例，内部会创建一个<code>GetEngineConnAction</code>，有 linkis 客户端发起请求，获取节点信息，里面包含了引擎的状态。</p>
</li>
</ol>
<span id="more"></span>



<h2 id="任务执行-API-入口"><a href="#任务执行-API-入口" class="headerlink" title="任务执行 API 入口"></a>任务执行 API 入口</h2><pre class="mermaid">sequenceDiagram
activate JobRestfulApi
JobRestfulApi ->> JobRestfulApi : 判断用户对于作业的权限
JobRestfulApi ->> + TaskService : executeJob()
    TaskService ->> TaskService : 根据 jobId 从表 linkis_stream_job 获取 job
    TaskService ->> TaskService : 根据 jobId 从表 linkis_stream_job_version 获取最新版本 jobVersion
    TaskService ->> TaskService : 创建 task 对象，并保存到表 linkis_stream_task
    Note over TaskService : 找到满足条件的 Builder： <br /> StreamisFlinkTransformJobBuilder <br />调用 build() 方法
    TaskService ->> + AbstractFlinkStreamisTransformJobBuilder : build()：<br /> 调用符合条件的实现类<br />将 StreamJob 转成 StreamisTransformJob
    AbstractFlinkStreamisTransformJobBuilder ->> AbstractFlinkStreamisTransformJobBuilder : 通过 configurationService 获取任务配置<br/>内部按照不同类型将配置分类<br/> 通过 streamJobMapper 获取版本
    Note over FlinkSQLJobContentParser : 不同版本的作业，作业内容可能不一样
    AbstractFlinkStreamisTransformJobBuilder ->> + FlinkSQLJobContentParser : parseTo()：从作业版本中获取作业内容
    Note over FlinkSQLJobContentParser : 根据任务类型获取作业内容 <br /> 1. file 类型调用 getFileContent(...) <br /> 2. bml 类型调用  readFileFromBML(...) 3. sql 类型直接返回 sql 字符串
    FlinkSQLJobContentParser -->> - AbstractFlinkStreamisTransformJobBuilder : 返回 StreamisSqlTransformJobContent 对象<br />封装了任务内容（需要执行的 SQL）
    AbstractFlinkStreamisTransformJobBuilder ->> AbstractFlinkStreamisTransformJobBuilder : 设置 engineConn 类型（flink-1.12.2）和运行类型（sql）
    AbstractFlinkStreamisTransformJobBuilder -->> - TaskService : 返回 transformJob
    Note over TaskService : 接下来，将 transformJob 转成 launchJob
    TaskService ->> + Transform : transform() 
    Note over TaskService, Transform : 这里调用了一个 foldLeft 方法，<br />效果是从数组的最后一个 Transform 依次作用在 transformJob 上面，<br />并将结果作为下次 transform 的输入。
    Transform -->> - TaskService : 返回 launchJob

    Note over TaskService : 接下来通过 linkisJobManager 启动 launchJob<br /><br />通过反射获取 LinkisJobManager 接口的实现类，<br />接口中有个 getName 方法。<br />内部会构建一个 name -> 实现类的映射。<br /> 这里根据 name=simpleFlink 获取到的实现类是 SimpleFlinkJobManager
    
    Note over TaskService : 具体见“启动LaunchJob”

TaskService -->> - JobRestfulApi : 执行结束返回
note over JobRestfulApi : 返回成功消息
deactivate JobRestfulApi</pre>

<p>Transform 的作用是给 launchJob 添加 labels、source、作业配置、launchConfig。</p>
<pre class="mermaid">classDiagram
class Transform {
<<interface>>
+ transform(...)
}
Transform<|..StreamisJobContentTransform
Transform<|..ConfigTransform
Transform<|..LabelsStreamisCodeTransform
Transform<|..SourceTransform
Transform<|..FlinkJarStreamisStartupParamsTransform
Transform<|..LaunchConfigTransform



class StreamisJobContentTransform {
<<interface>>
+ transform(...)
+ transformJobContent(...)
}
StreamisJobContentTransform<|..SqlStreamisJobContentTransform
StreamisJobContentTransform<|..FlinkJarStreamisJobContentTransform

class ConfigTransform {
<<interface>>
+ transform(...)
+ transform(...)
}
ConfigTransform<|..FlinkCheckpointConfigTransform
ConfigTransform<|..ResourceConfigTransform


class ResourceConfigTransform{
+ transform(...)
}
ResourceConfigTransform<|..ExtraConfigTransform</pre>



<h3 id="LaunchJob-示例"><a href="#LaunchJob-示例" class="headerlink" title="LaunchJob 示例"></a>LaunchJob 示例</h3><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">LaunchJob(</span><br><span class="line">	submitUser: hadoop, </span><br><span class="line">	labels: &#123;</span><br><span class="line">		<span class="attribute">userCreator</span>=hadoop-Streamis, <span class="attribute">engineType</span>=flink-1.12.2, <span class="attribute">engineConnMode</span>=once</span><br><span class="line">	&#125;, </span><br><span class="line">	jobContent: &#123;</span><br><span class="line">		<span class="attribute">code</span>=SELECT <span class="string">&#x27;linkis flink engine test!!!&#x27;</span>, <span class="attribute">runType</span>=sql</span><br><span class="line">	&#125;, </span><br><span class="line">	params: &#123;</span><br><span class="line">		configuration=&#123;</span><br><span class="line">			startup=&#123;</span><br><span class="line">				wds.linkis.flink.taskmanager.<span class="attribute">num</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.jobmanager.<span class="attribute">cpus</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.taskmanager.<span class="attribute">memory</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.<span class="attribute">custom</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.taskManager.<span class="attribute">cpus</span>=<span class="literal">null</span>, </span><br><span class="line">				wds.linkis.flink.jobmanager.<span class="attribute">memory</span>=<span class="literal">null</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;, </span><br><span class="line">	source: &#123;</span><br><span class="line">		<span class="attribute">workspace</span>=<span class="literal">null</span>, <span class="attribute">project</span>=demo, <span class="attribute">job</span>=demo_flink_00</span><br><span class="line">	&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>launchConfig 由于在 toString 方法中没有实现，所以日志中不会打印</p>
</blockquote>
<h2 id="启动-LaunchJob"><a href="#启动-LaunchJob" class="headerlink" title="启动 LaunchJob"></a>启动 LaunchJob</h2><p>启动 LaunchJob 内部先是构建了一个单次作业 onceJob，再提交</p>
<p><code>com.webank.wedatasphere.streamis.jobmanager.launcher.linkis.manager.FlinkJobManager#launch</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">launch</span></span>(job: <span class="type">LaunchJob</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    job.getLabels.get(<span class="type">LabelKeyUtils</span>.<span class="type">ENGINE_TYPE_LABEL_KEY</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> engineConnType: <span class="type">String</span> =&gt;</span><br><span class="line">        <span class="keyword">if</span>(!engineConnType.toLowerCase.startsWith(<span class="type">FlinkJobManager</span>.<span class="type">FLINK_ENGINE_CONN_TYPE</span>))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">FlinkJobLaunchErrorException</span>(<span class="number">30401</span>, <span class="string">s&quot;Only <span class="subst">$&#123;FlinkJobManager.FLINK_ENGINE_CONN_TYPE&#125;</span> job is supported to be launched to Linkis, but <span class="subst">$engineConnType</span> is found.&quot;</span>)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">FlinkJobLaunchErrorException</span>(<span class="number">30401</span>, <span class="string">s&quot;Not exists <span class="subst">$&#123;LabelKeyUtils.ENGINE_TYPE_LABEL_KEY&#125;</span>, StreamisJob cannot be submitted to Linkis successfully.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> onceJob = buildOnceJob(job)</span><br><span class="line">    onceJob.submit()</span><br><span class="line">    onceJobs synchronized onceJobs.put(onceJob.getId, onceJob)</span><br><span class="line">    <span class="keyword">val</span> linkisJobInfo = <span class="type">Utils</span>.tryCatch(createJobInfo(onceJob, job))&#123; t =&gt;</span><br><span class="line">        error(<span class="string">s&quot;<span class="subst">$&#123;job.getSubmitUser&#125;</span> create jobInfo failed, now stop this EngineConn <span class="subst">$&#123;onceJob.getId&#125;</span>.&quot;</span>)</span><br><span class="line">        stop(onceJob)</span><br><span class="line">        <span class="keyword">throw</span> t</span><br><span class="line">    &#125;</span><br><span class="line">    onceJobs synchronized onceJobIdToJobInfo.put(onceJob.getId, linkisJobInfo)</span><br><span class="line">    onceJob.getId</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="构建单次作业"><a href="#构建单次作业" class="headerlink" title="构建单次作业"></a>构建单次作业</h3><pre class="mermaid">sequenceDiagram
activate TaskService
TaskService ->> + FlinkJobManager : launch()
    
    FlinkJobManager ->> FlinkJobManager : 判断标签 engineType 是否 flink 开头
    FlinkJobManager ->> + SimpleFlinkJobManager : buildOnceJob()：<br />构建单次作业
        
            SimpleFlinkJobManager  ->> SimpleFlinkJobManager : 初始化 SimpleOnceJobBuilder 对象
            rect rgb(121, 216, 206)
            SimpleFlinkJobManager ->> + SimpleOnceJobBuilder : build()
                SimpleOnceJobBuilder ->> SimpleOnceJobBuilder :  <br /> 1. 校验 labels/jobContent 是否为空 <br /> 2. 设置 params/source <br /> 3. 配置启动参数 <br /> 4. TODO 还有些细节要看下
                activate SimpleOnceJobBuilder
                    Note right of SimpleOnceJobBuilder : 调用方法 getOnceExecutorContent()
                    SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 构建 OnceExecutorContent 对象，并初始化
                    SimpleOnceJobBuilder ->> + OnceExecutorContentUtils : contentToMap() ：<br />将 onceExecutorContent <br /> 转成 contentMap
                    OnceExecutorContentUtils -->> - SimpleOnceJobBuilder : 返回 contentMap
                    SimpleOnceJobBuilder ->> + HttpBmlClient : uploadResource()：<br /> json 序列化 contentMap 对象上传
                    HttpBmlClient -->> - SimpleOnceJobBuilder : 返回响应 response 包含资源 ID 和版本
                    SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 将 response 封装成 BmlResource 
                    SimpleOnceJobBuilder ->> + OnceExecutorContentUtils :　resourceToValue() ： <br />将 BmlResource 转成资源 ID
                    OnceExecutorContentUtils -->> - SimpleOnceJobBuilder : 返回资源 ID
                deactivate SimpleOnceJobBuilder
                SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 初始化 CreateEngineConnAction.Builder 对象
                SimpleOnceJobBuilder ->> + CreateEngineConnAction.Builder :　build()
                CreateEngineConnAction.Builder -->> - SimpleOnceJobBuilder : 返回对象 createEngineConnAction
                SimpleOnceJobBuilder ->> SimpleOnceJobBuilder : 封装一个 SubmittableSimpleOnceJob 实例
                Note over SimpleOnceJobBuilder : 这个实例中包含有 linkisClient，以及 createEngineConnAction<br/> linkisCLient 内部有个成员 dwsHttpClient 后续与 linkis 接口的交互通过它执行
            SimpleOnceJobBuilder -->> - SimpleFlinkJobManager : 返回实例
            end
    SimpleFlinkJobManager -->> - FlinkJobManager : 返回实例 onceJob
    
    Note over FlinkJobManager : 执行 onceJob.submit() 方法<br />具体见“提交创建引擎”
    FlinkJobManager -->> - TaskService : 返回 linkisJobId
deactivate TaskService</pre>

<p>createEngineConnAction.getRequestPayload</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;createService&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ServiceInstance(streamis-server, 10-177-198-114.ostream-test.dgtest01:9400)&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ignoreTimeout&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;demo for streamis&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.taskmanager.num&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.jobmanager.cpus&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.taskmanager.memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.taskManager.cpus&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.custom&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;label.codeType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sql&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;wds.linkis.flink.jobmanager.memory&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;onceExecutorContent&quot;</span><span class="punctuation">:</span> <span class="string">&quot;resource_036360c3c48-fc89-4a86-bafc-b21a9af830eav000001&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;userCreator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hadoop-Streamis&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;engineType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;flink-1.12.2&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;engineConnMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;once&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timeOut&quot;</span><span class="punctuation">:</span> <span class="number">300000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>接下来，会将上面的 SubmittableSimpleOnceJob 通过 linkis 客户端提交给 linkis，linkis 则会创建一个 EngineConn。</p>
<h3 id="获取引擎信息"><a href="#获取引擎信息" class="headerlink" title="获取引擎信息"></a>获取引擎信息</h3><pre class="mermaid">sequenceDiagram
activate FlinkJobManager
    FlinkJobManager ->> + AbstractSubmittableLinkisJob : submit() 提交 onceJob
    AbstractSubmittableLinkisJob ->> + SubmittableSimpleOnceJob : doSubmit()
    Note over AbstractSubmittableLinkisJob : 这里与之前创建的 SubmittableSimpleOnceJob 对上了
    SubmittableSimpleOnceJob ->> + LinkisManagerClientImpl : createEngineConn()
    Note over LinkisManagerClientImpl : 这个方法内部会发起 http 请求 linkis 接口<br /> 返回一个 CreateEngineConnResult 实例
    LinkisManagerClientImpl -->> - SubmittableSimpleOnceJob : 返回 nodeInfo
    SubmittableSimpleOnceJob ->> SubmittableSimpleOnceJob : 从 nodeInfo 中获取 serviceInstance、ticketId、ecmServiceInstance、lastEngineConnState
    SubmittableSimpleOnceJob ->> SimpleOnceJob :　initOnceOperatorActions()：在数组 operatorActions 中添加一个 action<br /> 这个 action 给 OnceJobOperator 设置 user/ticketId/serviceInstance/linkisManagerClient
 
    Note over SimpleOnceJob : TODO action 的作用
    rect rgb(100, 118, 135)
    alt 调用方法 isCompleted 判断状态 lastEngineConnState 未完成 且 未启动
    Note over SimpleOnceJob : isCompleted() 方法中在对于 engineConn 结束和运行<br/>分别回调 onJobFlinished / onJobRunning
    rect rgb(255, 230, 204)
    loop 状态 未完成 且 未启动
    SubmittableSimpleOnceJob ->> + SimpleOnceJob :　isCompleted
    SimpleOnceJob ->> + OnceJob : getNodeInfo() ：通过 linkis 客户端获取 <br/>ServiceInstance 对应的 EngineConn 信息
    OnceJob -->> - SimpleOnceJob : 返回 EngineConnNode
    SimpleOnceJob -->> - SubmittableSimpleOnceJob : 从 EngineConnNode 获取状态，返回 isCompleted() 结果作为状态
    end
    end
    SubmittableSimpleOnceJob ->> + SimpleOnceJob :　transformToId()
    SimpleOnceJob -->> - SubmittableSimpleOnceJob : 设置 engineConnId
    else
    SubmittableSimpleOnceJob ->> + SimpleOnceJob :　transformToId()
    SimpleOnceJob -->> - SubmittableSimpleOnceJob : 设置 engineConnId
    end
    end
    SubmittableSimpleOnceJob -->> - AbstractSubmittableLinkisJob : 无返回值
    Note over AbstractSubmittableLinkisJob : 具体见“提交创建引擎后回调”
    AbstractSubmittableLinkisJob -->> - FlinkJobManager : 无返回值
    FlinkJobManager ->> FlinkJobManager : 以 engineConnId 作为 key<br/>onceJob 作为 value<br/>放到缓存 onceJobs 中
    Note over FlinkJobManager : 具体见“通知引擎执行作业”
deactivate FlinkJobManager</pre>

<p>这里逻辑就是通过 linkis 客户端向 linkis 发送一个 <code>createEngineConnAction</code> 用于创建一个 <code>engineConn</code>，然后一直等待直到这个 <code>engineConn</code> 结束或者 运行起来。</p>
<p>这里请求的 linkis 接口地址中路径是 <code>linkisManager/createEngineConn</code>，对应的执行逻辑入口是 <code>org.apache.linkis.manager.am.restful.EngineRestfulApi#createEngineConn</code></p>
<p><code>AbstractSubmittableLinkisJob</code> 在提交之后，还有个指标 jobMetrics 和回调（<code>onJobSubmitted()</code>），见下文。</p>
<p>engineConnId 的格式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">engineConnId = <span class="string">s&quot;<span class="subst">$&#123;ticketId.length&#125;</span>_<span class="subst">$&#123;serviceInstance.getApplicationName.length&#125;</span>_<span class="subst">$&#123;ticketId&#125;</span><span class="subst">$&#123;serviceInstance.getApplicationName&#125;</span><span class="subst">$&#123;serviceInstance.getInstance&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//例如：</span></span><br><span class="line"><span class="comment">//36_20_05443308-f227-442e-9b20-8d291e1fa7d1linkis-cg-engineconn10-177-198-114.ostream-test.dgtest01:37244</span></span><br><span class="line"><span class="comment">//其中：</span></span><br><span class="line"><span class="comment">// ticketId = 05443308-f227-442e-9b20-8d291e1fa7d1</span></span><br><span class="line"><span class="comment">// applicationName = linkis-cg-engineconn</span></span><br><span class="line"><span class="comment">// serviceInstance = 10-177-198-114.ostream-test.dgtest01:37244</span></span><br></pre></td></tr></table></figure>

<p>nodeInfo 中包含的信息，看下日志，engineConnId 几乎包含了。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">EngineConn</span> created with status Running, the nodeInfo is &#123;ecmServiceInstance=&#123;instance=<span class="number">10</span>-<span class="number">177</span>-<span class="number">198</span>-<span class="number">114</span>.os</span><br><span class="line"><span class="attribute">tream</span>-test.dgtest01:<span class="number">9112</span>, applicationName=linkis-cg-engineconnmanager&#125;, serviceInstance=&#123;instance=<span class="number">10</span>-<span class="number">177</span>-<span class="number">198</span>-<span class="number">114</span>.ostream-test.dgtest01:<span class="number">40698</span>, applicationName=linkis-cg-engineconn&#125;, nodeStatus=Running, ticketId=a3f69955-<span class="number">02</span>a1-<span class="number">4973</span>-<span class="number">9325</span>-<span class="number">5</span>b</span><br><span class="line"><span class="attribute">f33f78b9d6</span>&#125;.</span><br></pre></td></tr></table></figure>



<h4 id="linkis-客户端请求创建EngineConn"><a href="#linkis-客户端请求创建EngineConn" class="headerlink" title="linkis 客户端请求创建EngineConn"></a>linkis 客户端请求创建EngineConn</h4><p><code>org.apache.linkis.computation.client.once.simple.SubmittableSimpleOnceJob#doSubmit</code></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    info(<span class="string">s&quot;Ready to create a engineConn: <span class="subst">$&#123;createEngineConnAction.getRequestPayload&#125;</span>.&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> nodeInfo = linkisManagerClient.createEngineConn(createEngineConnAction)</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中<code>linkisManagerClient.createEngineConn(createEngineConnAction)</code>最终会调用到<code>org.apache.linkis.httpclient.AbstractHttpClient#execute</code>，这里的逻辑在 [源码-linkis 之BML物料上传与下载.md](源码-linkis 之BML物料上传与下载.md) 有做介绍。</p>
<p>请求的接口路径是这样确定的</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareReq</span></span>(requestAction: <span class="type">HttpAction</span>): <span class="type">HttpRequestBase</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> realURL = <span class="string">&quot;&quot;</span></span><br><span class="line">    requestAction <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> serverUrlAction: <span class="type">ServerUrlAction</span> =&gt;</span><br><span class="line">        realURL = connectUrl(serverUrlAction.serverUrl, requestAction.getURL)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        realURL = getRequestUrl(requestAction.getURL, requestAction.getRequestBody)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于<code>CreateEngineConnAction</code>，<code>getURL</code>方法实现是</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DWSHttpAction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> dwsVersion: <span class="type">String</span> = _</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setDWSVersion</span></span>(dwsVersion: <span class="type">String</span>): <span class="type">Unit</span> = <span class="keyword">this</span>.dwsVersion = dwsVersion</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPrefixURL</span></span>: <span class="type">String</span> = <span class="string">&quot;/api/&quot;</span> + getRestType + <span class="string">&quot;/&quot;</span> + dwsVersion</span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getRestType</span></span>: <span class="type">RestType</span> = <span class="type">RestType</span>.<span class="type">JERSEY</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">suffixURLs</span></span>: <span class="type">Array</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getURL</span></span>: <span class="type">String</span> = getPrefixURL + <span class="string">&quot;/&quot;</span> + suffixURLs.mkString(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>url 是由前缀和后缀两部分组成</p>
<ol>
<li><p>前缀是 <code>&quot;/api/&quot; + getRestType + &quot;/&quot; + dwsVersion</code>，其中 restType 是<code>rest_j</code>，<code>dwsVersion</code>由配置项<code>wds.linkis.web.version</code>指定，默认值是<code>v1</code></p>
</li>
<li><p>后缀<code>suffixURLs</code>在<code>CreateEngineConnAction</code>中有重载</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CreateEngineConnAction</span> <span class="keyword">extends</span> <span class="title">POSTAction</span> <span class="keyword">with</span> <span class="title">LinkisManagerAction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRequestPayload</span></span>: <span class="type">String</span> = <span class="type">DWSHttpClient</span>.jacksonJson.writeValueAsString(getRequestPayloads)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">suffixURLs</span></span>: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">&quot;linkisManager&quot;</span>, <span class="string">&quot;createEngineConn&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>综上，请求的接口地址路径是<code>api/rest_j/v1/linkisManager/createEngineConn</code>。这个地址在 linkis 的处理入口是<code>org.apache.linkis.manager.am.restful.EngineRestfulApi#createEngineConn</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@RequestMapping(path = &quot;/linkisManager&quot;, produces = &#123;&quot;application/json&quot;&#125;)</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">EngineRestfulApi</span> &#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    <span class="meta">@RequestMapping(path = &quot;/createEngineConn&quot;, method = RequestMethod.POST)</span></span><br><span class="line">    <span class="keyword">public</span> Message <span class="title function_">createEngineConn</span><span class="params">( HttpServletRequest req, <span class="meta">@RequestBody</span> JsonNode jsonNode)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">//...</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<p>此外还有些有意思的地方，比如</p>
<ol>
<li><p>响应类型，是根据请求的 url 来确定：在响应对象上定义了一个注解，注解值是对应的请求 url，通过反射获取到 url 和 响应类的映射关系后，从映射关系中获取响应类型。例如，</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="meta">@DWSHttpMessageResult</span>(<span class="string">&quot;/api/rest_j/v\\d+/linkisManager/createEngineConn&quot;</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CreateEngineConnResult</span> <span class="keyword">extends</span> <span class="title">GetEngineConnResult</span></span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="提交创建引擎后回调"><a href="#提交创建引擎后回调" class="headerlink" title="提交创建引擎后回调"></a>提交创建引擎后回调</h4><pre class="mermaid">sequenceDiagram 
activate AbstractSubmittableLinkisJob
    AbstractSubmittableLinkisJob ->> + LinkisJobMetrics : 
    Note over TaskService, LinkisJobMetrics : 创建 LinkisJobMetrics 对象， <br/>参数是上面创建的 engineConnId
    LinkisJobMetrics -->> - AbstractSubmittableLinkisJob : 返回 jobMetrics
    AbstractSubmittableLinkisJob ->> + AbstractLinkisJob : getJobListeners()：获取作业监听器
    Note over AbstractLinkisJob : 这里的监听器应该是空的<br /> 方法 addJobListener 貌似没有被调用
    AbstractLinkisJob -->> - AbstractSubmittableLinkisJob : 返回监听器 
    Note over AbstractSubmittableLinkisJob : 调用监听器的 onJobSubmitted 方法
deactivate AbstractSubmittableLinkisJob</pre>

<h2 id="获取引擎信息-1"><a href="#获取引擎信息-1" class="headerlink" title="获取引擎信息"></a>获取引擎信息</h2><pre class="mermaid">sequenceDiagram
activate FlinkJobManager
    FlinkJobManager ->> FlinkJobManager : 将 onceJob 保存到缓存 onceJobs 中
    FlinkJobManager ->> + SimpleFlinkJobManager : createJobInfo()
    
    SimpleFlinkJobManager ->> + OnceJob : getNodeInfo : 获取 engineConn 信息
    OnceJob -->> - SimpleFlinkJobManager : 返回 nodeInfo
    SimpleFlinkJobManager ->> SimpleFlinkJobManager : 创建一个 FlinkJobInfo 对象 jobInfo
    
    activate SimpleFlinkJobManager
    Note right of SimpleFlinkJobManager : 调用方法 fatchApplicationInfo() <br/> 获取 yarn 作业信息 
    Note right of SimpleFlinkJobManager : 根据 operator 名字获取 operator<br/> EngineConnApplicationInfoOperator
    Note right of SimpleFlinkJobManager : 将前面设置的 action 依次作用到<br/>EngineConnApplicationInfoOperator
    rect rgb(121, 216, 206)
    loop 可以重试
    SimpleFlinkJobManager ->> + OnceJobOperator : apply()
    OnceJobOperator ->> OnceJobOperator : 构建 EngineConnOperateAction.Builder 对象
    Note over OnceJobOperator : 从这个 builder 的类型<br/>可以看出它是用来构建 EngineConn 执行操作的构建器
    OnceJobOperator ->> + EngineConnOperateAction.Builder : build()
    EngineConnOperateAction.Builder -->> - OnceJobOperator : 返回 EngineConnOperateAction 实例 <br /> engineConnOperateAction
    
    OnceJobOperator ->> + LinkisManagerClientImpl : executeEngineConnOperation()
    Note over LinkisManagerClientImpl : 通过 linkis 客户端，提交 engineConnOperateAction <br/> 这里引擎开始执行任务
    LinkisManagerClientImpl -->> - OnceJobOperator : 返回执行结果 result，结果对象中有 applicationId，applicationUrl
    
    OnceJobOperator ->> + EngineConnApplicationInfoOperator : resultToObject()：将返回的结果 result 转成 ApplicationInfo
    EngineConnApplicationInfoOperator -->> - OnceJobOperator : 返回 ApplicationInfo 对象（包含：applicationId、applicationUrl、queue）
    
    OnceJobOperator -->> - SimpleFlinkJobManager :返回 applicationInfo 对象
    end
    end
    SimpleFlinkJobManager ->> SimpleFlinkJobManager : 从 applicationInfo 对象中获取 <br/>applcaitionId,applicationUrl 设置 jobInfo
    deactivate SimpleFlinkJobManager
    SimpleFlinkJobManager ->> SimpleFlinkJobManager : jobInfo 设置 resource
    SimpleFlinkJobManager -->> - FlinkJobManager : 返回 jobInfo
    
    FlinkJobManager ->> FlinkJobManager : 存放到缓存 onceJobIdToJobInfo <br /> 返回 engineConnId
deactivate FlinkJobManager</pre>

<p>这里请求的 linkis 接口地址中路径是 <code>linkisManager/executeEngineConnOperation</code>，对应的执行逻辑入口是 <code>org.apache.linkis.manager.am.restful.EngineRestfulApi#executeEngineConnOperation</code></p>
<h2 id="更新作业状态"><a href="#更新作业状态" class="headerlink" title="更新作业状态"></a>更新作业状态</h2><pre class="mermaid">sequenceDiagram

activate TaskService 
    activate TaskService
    Note right of TaskService : 调用 updateStreamTaskStatus()<br/> 这个方法内部会判断 task <br />是否在缓存 onceJobIdToJobInfo 中
    TaskService ->> + FlinkJobManager : getJobInfo() : <br />从缓存 onceJobIdToJobInfo 获取 jobInfo
    FlinkJobManager ->> + SimpleFlinkJobManager : getStatus() : <br /> 从缓存 onceJobs 中获取 simpleOnceJob
    SimpleFlinkJobManager ->> + SimpleOnceJob : isCompleted() ：调用
    Note over SimpleOnceJob :　这个方法内部会调用 linkis 客户端获取 engineConn 状态
    SimpleOnceJob -->> - SimpleFlinkJobManager : 返回状态
    SimpleFlinkJobManager -->> - FlinkJobManager : 返回状态 status
    Note over FlinkJobManager : jobInfo 设置状态 status
    FlinkJobManager -->> - TaskService :  返回 jobInfo
    Note right of TaskService : 更新 task 的 更新时间和状态<br /> task 状态结束，jobInfo 中 completedMsg 非空<br/>task 设置错误消息 errDesc<br/> jobInfo json 序列化保存到 task 的 linkisJobInfo
    deactivate TaskService
    Note right of TaskService : 最后更新数据表 linkis_stream_task
    
deactivate TaskService</pre>

<p>此外类<code>TaskMonitorService</code>中会启动定时任务调用<code>com.webank.wedatasphere.streamis.jobmanager.manager.service.TaskService$#updateStreamTaskStatus</code>来获取状态。</p>
]]></content>
      <tags>
        <tag>streamis</tag>
      </tags>
  </entry>
  <entry>
    <title>连不上 HS2</title>
    <url>/2022/10/30/%E8%BF%9E%E4%B8%8D%E4%B8%8A-HS2/</url>
    <content><![CDATA[<p>作业执行失败报错如下  </p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">Tried <span class="attribute">all</span> existing HiveServer2 uris <span class="selector-tag">from</span> ZooKeeper.</span><br></pre></td></tr></table></figure>

<p>目前 HS2 通过 ZK 做了 HA，从报错日志看，将 ZK 里面配置的 HS2 依次连接了一遍，都失败了。过往的经验是有两个可能的问题</p>
<p>jdbc 连接串（ <code>jdbc:hive2://10.1.183.246,10.1.178.113,10.1.166.18:2181/usercenter;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sgp -com-hive;hive.server2.proxy.user=sg_ib_dw?mapred.job.queue.name=root.ib.sgoffline</code> ）有问题<br>HS2 地址中使用的是主机名，从主机名解析获取 IP 地址失败</p>
<p>第一个问题通过 beeline 连接给定的 jdbc 连接串验证，可以连上，说明不是这个问题；  </p>
<p>第二个问题可以从 ZK 中获取 HS2 地址，通过 ping 或者 telnet 命令验证，验证通过，说明也不是这个问题  </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取 HS2 地址</span></span><br><span class="line">[zk: 10.1.166.18:2181(CONNECTED) 0] ls /sgp-com-hive</span><br><span class="line">[serverUri=10-94-12-217.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn-1;sequence=0000001812, serverUri=10-94-104-44.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn-1;sequence=0000001795, serverUri=10-94-106-212.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn-1;sequence=0000001771]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">telnet 验证</span></span><br><span class="line">[service@10-94-48-163.datahub-api.sgp lib]$ telnet 10-94-12-217.sgp-com-hiveserver2.sgp 10000</span><br><span class="line">Trying 10.94.12.217...</span><br><span class="line">Connected to 10-94-12-217.sgp-com-hiveserver2.sgp.</span><br><span class="line">Escape character is &#x27;^]&#x27;.</span><br><span class="line">^]</span><br><span class="line"><span class="meta prompt_">telnet&gt; </span><span class="language-bash">quit</span></span><br><span class="line">Connection closed.</span><br></pre></td></tr></table></figure>

<p>接下来，看下 hive jdbc 是如何创建连接的，从异常栈中是看到相关的类是  <code>org.apache.hive.jdbc.HiveConnection#HiveConnection</code> ，之前在hive-读取超时中有描述 jdbc 打开 hive 连接的大致流程，接下来看下其中遍历 ZK 中配置的 HS2，依次尝试连接，直到成功或者失败的过程。  </p>
<p>从方法 <code>org.apache.hive.jdbc.Utils#parseURL</code> 开始解析 jdbc url 连接， <code>jdbc:hive2://&lt;host1&gt;:&lt;port1&gt;,&lt;host2&gt;:&lt;port2&gt;/dbName;sess_var_list?hive_conf_list#hive_var_list</code> ，其中几个 list 是以 <code>;</code> 分隔的键值对，分别对应 URI 的 path、query和fragment，内部也是用 URI 类来提取的各个部分的。最后会调用 <code>org.apache.hive.jdbc.Utils#configureConnParams</code>  处理 <code>connParams</code> ，从 ZK 中获取真实的主机和端口，并替换 <code>dummyAuthorityString</code> 。  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> JdbcConnectionParams <span class="title function_">parseURL</span><span class="params">(String uri)</span> <span class="keyword">throws</span> JdbcUriParseException,</span><br><span class="line">SQLException, ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="type">JdbcConnectionParams</span> <span class="variable">connParams</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JdbcConnectionParams</span>();</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="comment">// Extract host, port</span></span><br><span class="line">  <span class="keyword">if</span> (connParams.isEmbeddedMode()) &#123;</span><br><span class="line">    <span class="comment">// In case of embedded mode we were supplied with an empty authority.</span></span><br><span class="line">    <span class="comment">// So we never substituted the authority with a dummy one.</span></span><br><span class="line">    connParams.setHost(jdbcURI.getHost());</span><br><span class="line">    connParams.setPort(jdbcURI.getPort());</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Configure host, port and params from ZooKeeper if used,</span></span><br><span class="line">    <span class="comment">// and substitute the dummy authority with a resolved one</span></span><br><span class="line">    configureConnParams(connParams); <span class="comment">//就是这里</span></span><br><span class="line">    <span class="comment">// We check for invalid host, port while configuring connParams with configureConnParams()</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">authorityStr</span> <span class="operator">=</span> connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort();</span><br><span class="line">    LOG.info(<span class="string">&quot;Resolved authority: &quot;</span> + authorityStr);</span><br><span class="line">    uri = uri.replace(dummyAuthorityString, authorityStr);</span><br><span class="line">    connParams.setJdbcUriString(uri);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> connParams;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法 <code>org.apache.hive.jdbc.Utils#configureConnParams</code> 内部会判断服务发现模式是不是 <code>zooKeeper</code> （在 jdbc url 的 session var 中有给出，即： <code>serviceDiscoveryMode=zooKeeper;</code> ），执行 <code>org.apache.hive.jdbc.ZooKeeperHiveClientHelper#configureConnParams</code> 从 ZK 获取配置  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">configureConnParams</span><span class="params">(JdbcConnectionParams connParams)</span></span><br><span class="line">  <span class="keyword">throws</span> JdbcUriParseException, ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="type">String</span> <span class="variable">serviceDiscoveryMode</span> <span class="operator">=</span></span><br><span class="line">    connParams.getSessionVars().get(JdbcConnectionParams.SERVICE_DISCOVERY_MODE);</span><br><span class="line">  <span class="keyword">if</span> ((serviceDiscoveryMode != <span class="literal">null</span>)</span><br><span class="line">      &amp;&amp; (JdbcConnectionParams.SERVICE_DISCOVERY_MODE_ZOOKEEPER</span><br><span class="line">          .equalsIgnoreCase(serviceDiscoveryMode))) &#123;</span><br><span class="line">    <span class="comment">// Set ZooKeeper ensemble in connParams for later use</span></span><br><span class="line">    connParams.setZooKeeperEnsemble(joinStringArray(connParams.getAuthorityList(), <span class="string">&quot;,&quot;</span>));</span><br><span class="line">    <span class="comment">// Configure using ZooKeeper</span></span><br><span class="line">    ZooKeeperHiveClientHelper.configureConnParams(connParams);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看方法 <code>org.apache.hive.jdbc.ZooKeeperHiveClientHelper#configureConnParams</code> 内部实现<br>从指定空间（在 jdbc url 的 session var 中指定：即 <code>zooKeeperNamespace=sgp-com-hive</code> ）中获取 HS2 列表 <code>serverHosts</code><br>将 <code>serverHosts</code> 中已经尝试连接过的 HS2 剔除掉（连接失败的 HS2 会记录到^^拒绝连接列表^^中，下文中有描述）<br>如果没有可用的 HS2，就抛出前面任务失败的异常 <code>Tried all existing HiveServer2 uris from ZooKeeper.</code><br>从可用的 HS2 随机选择一个，获取对应 znode 内容  <code>serverConfStr</code> ，即对应 HS2 的配置：</p>
<figure class="highlight subunit"><table><tr><td class="code"><pre><span class="line">[zk: 10.1.166.18:2181(CONNECTED) 0] ls /sgp-com-hive</span><br><span class="line">[serverUri=10<span class="string">-94</span><span class="string">-12</span><span class="string">-217</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001812, serverUri=10<span class="string">-94</span><span class="string">-104</span><span class="string">-44</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001795, serverUri=10<span class="string">-94</span><span class="string">-106</span><span class="string">-212</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001771]</span><br><span class="line">[zk: 10.1.166.18:2181(CONNECTED) 1] get /sgp-com-hive/serverUri=10<span class="string">-94</span><span class="string">-12</span><span class="string">-217</span>.sgp-com-hiveserver2.sgp:10000;version=2.3.7-amzn<span class="string">-1</span>;sequence=0000001812</span><br><span class="line">hive.server2.authentication=CUSTOM;hive.server2.transport.mode=binary;hive.server2.thrift.sasl.qop=auth;hive.server2.thrift.bind.host=10<span class="string">-94</span><span class="string">-12</span><span class="string">-217</span>.sgp-com-hiveserver2.sgp;hive.server2.thrift.port=10000;hive.server2.use.SSL=false</span><br></pre></td></tr></table></figure>

<p>重新整理后如下  </p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">hive.server2.authentication</span>=CUSTOM<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.transport.mode</span>=binary<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.thrift.sasl.qop</span>=auth<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.thrift.bind.host</span>=<span class="number">10</span>-<span class="number">94</span>-<span class="number">12</span>-<span class="number">217</span>.sgp-com-hiveserver2.sgp<span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.thrift.port</span>=<span class="number">10000</span><span class="comment">;</span></span><br><span class="line"><span class="attr">hive.server2.use.SSL</span>=<span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p>调用方法 <code>applyConfs</code>  应用配置 <code>serverConfStr</code> ，即将上面的配置依次设置到 <code>connParams</code> 中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">configureConnParams</span><span class="params">(JdbcConnectionParams connParams)</span></span><br><span class="line">  <span class="keyword">throws</span> ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="comment">//省略...</span></span><br><span class="line">  serverHosts = zooKeeperClient.getChildren().forPath(<span class="string">&quot;/&quot;</span> + zooKeeperNamespace);</span><br><span class="line">  <span class="comment">// Remove the znodes we&#x27;ve already tried from this list</span></span><br><span class="line">  serverHosts.removeAll(connParams.getRejectedHostZnodePaths());</span><br><span class="line">  <span class="keyword">if</span> (serverHosts.isEmpty()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">ZooKeeperHiveClientException</span>(</span><br><span class="line">      <span class="string">&quot;Tried all existing HiveServer2 uris from ZooKeeper.&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Now pick a server node randomly</span></span><br><span class="line">  serverNode = serverHosts.get(randomizer.nextInt(serverHosts.size()));</span><br><span class="line">  connParams.setCurrentHostZnodePath(serverNode);</span><br><span class="line">  <span class="comment">// Read config string from the znode for this server node</span></span><br><span class="line">  <span class="type">String</span> <span class="variable">serverConfStr</span> <span class="operator">=</span></span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">String</span>(</span><br><span class="line">    zooKeeperClient.getData().forPath(<span class="string">&quot;/&quot;</span> + zooKeeperNamespace + <span class="string">&quot;/&quot;</span> + serverNode),</span><br><span class="line">    Charset.forName(<span class="string">&quot;UTF-8&quot;</span>));</span><br><span class="line">  applyConfs(serverConfStr, connParams);</span><br><span class="line">  <span class="comment">//省略 ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>支持连接参数 <code>connParams</code> 初始化完成，接下来调用方法 <code>org.apache.hive.jdbc.HiveConnection#openTransport</code> 打开与 ThriftServer 的连接，在一个 while true 的循环中尝试连接，处理异常。看下其中处理打开异常的部分：   </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">openTransport</span><span class="params">()</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      transport = isHttpTransportMode() ? createHttpTransport() : createBinaryTransport();</span><br><span class="line">      <span class="keyword">if</span> (!transport.isOpen()) &#123;</span><br><span class="line">        transport.open();</span><br><span class="line">      &#125;</span><br><span class="line">      logZkDiscoveryMessage(<span class="string">&quot;Connected to &quot;</span> + connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort());</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (TTransportException e) &#123;</span><br><span class="line">      <span class="comment">// We&#x27;ll retry till we exhaust all HiveServer2 nodes from ZooKeeper</span></span><br><span class="line">      <span class="keyword">if</span> (isZkDynamicDiscoveryMode()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          Utils.updateConnParamsFromZooKeeper(connParams); <span class="comment">//这里</span></span><br><span class="line">          <span class="comment">//...</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在通过 ZK 做动态发现模式下（方法 <code>isZkDynamicDiscoveryMode()</code> ）,调用方法 <code>org.apache.hive.jdbc.Utils#updateConnParamsFromZooKeeper</code> 重新获取一个 HS2<br>首先，将当前尝试连接的节点节点添加到^^拒绝节点列表^^中<br>暂存当前使用 HS2 的 host 和 port<br>调用前面介绍的方法 <code>org.apache.hive.jdbc.ZooKeeperHiveClientHelper#configureConnParams</code> 重新获取一个 HS2<br>使用新获取的 HS2 的 host 和 port 替换旧的 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">updateConnParamsFromZooKeeper</span><span class="params">(JdbcConnectionParams connParams)</span></span><br><span class="line">  <span class="keyword">throws</span> ZooKeeperHiveClientException &#123;</span><br><span class="line">  <span class="comment">// Add current host to the rejected list</span></span><br><span class="line">  connParams.getRejectedHostZnodePaths().add(connParams.getCurrentHostZnodePath());</span><br><span class="line">  <span class="type">String</span> <span class="variable">oldServerHost</span> <span class="operator">=</span> connParams.getHost();</span><br><span class="line">  <span class="type">int</span> <span class="variable">oldServerPort</span> <span class="operator">=</span> connParams.getPort();</span><br><span class="line">  <span class="comment">// Update connection params (including host, port) from ZooKeeper</span></span><br><span class="line">  ZooKeeperHiveClientHelper.configureConnParams(connParams);</span><br><span class="line">  connParams.setJdbcUriString(connParams.getJdbcUriString().replace(</span><br><span class="line">    oldServerHost + <span class="string">&quot;:&quot;</span> + oldServerPort, connParams.getHost() + <span class="string">&quot;:&quot;</span> + connParams.getPort()));</span><br><span class="line">  LOG.info(<span class="string">&quot;Selected HiveServer2 instance with uri: &quot;</span> + connParams.getJdbcUriString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，通过 ZK 来实现 HS2 高可用的流程就结束了。  </p>
<p>看来问题还是在连接 HS2 出现了异常，即 <code>transport.open();</code> 这里，内部的代码没有细看，大致用了 SASL 做认证，其中会从  <code>sessConfMap</code>  中获取用户和密码，会不会是认证出现了问题了？  </p>
<p>用户名和密码是在创建对象 <code>HiveConnection</code> 是，从传入变量 <code>info</code> 中拿到后设置到 <code>sessConfMap</code> 中的  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_USER)) &#123;</span><br><span class="line">  sessConfMap.put(JdbcConnectionParams.AUTH_USER, info.getProperty(JdbcConnectionParams.AUTH_USER));</span><br><span class="line">  <span class="keyword">if</span> (info.containsKey(JdbcConnectionParams.AUTH_PASSWD)) &#123;</span><br><span class="line">    sessConfMap.put(JdbcConnectionParams.AUTH_PASSWD, info.getProperty(JdbcConnectionParams.AUTH_PASSWD));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结合前面的异常栈，这个 <code>info</code> 就是 <code>com.dtstack.flinkx.hive.format.HiveInputFormat#executeQuery</code> 这里的 <code>properties</code> ，这个 <code>properties</code> 是从配置文件中获取的，检查后，发现配置是 null  </p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;increColumn&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;properties&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;splitPk&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;startLocation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<p>这些问题大致明白了——没有获取到用户名和密码导致 HS2 连接失败。  </p>
]]></content>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>flinkx 1.11 中使用远程 jar 遇到的问题与处理</title>
    <url>/2022/10/30/flinkx-1-11-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%A8%8B-jar-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<p>随着任务不断迁移，datahub api 需要提交的任务也越来越多，带来的问题包括但不限于如下</p>
<ol>
<li>客户端压力大，导致作业提交失败，之前发生过提交子进程内存耗尽</li>
<li>大量任务同时上传文件到 HDFS，导致上传变慢，失败后重试，甚至上传失败</li>
</ol>
<p>之前优化了 jar 重复上传的问题，上周例会上龙哥也做了一件类似的事，将 flink lib 的目录下的 jar 上传到 hdfs，任务使用远程 jar ，这样可以省去本地上传的过程。由于 flinkx 作业几乎不会改动到 flink 的 jar 包，使用远程 jar 可以节省至少一半上传数据量。<br>接下来记录下验证过程中碰到的一个问题<br>虽然配置了远程的 jar 但是要把本地的 lib 目录清空了才不会上传。但是清空之后有个问题，任务启动失败报错有个类找不到。挨个尝试，最终发现要在本地 lib 目录中，保留以 Hadoop shaded jar 才行。<br><img src="/2022/10/30/flinkx-1-11-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%A8%8B-jar-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%84%E7%90%86/image_1666956868228_0.png" alt="image.png"><br>之所以会上传是因为 flinkx 在创建 yarndescriptor 时候，将 flink lib 目录中的 jar 添加到 ship files 中了，在执行 start app master 这个方法的时候 会把 ship files 中的文件上传。<br>后面发现添加配置项 <code>yarn.per-job-cluster.include-user-jar : FIRST</code> 后任务可以执行了，带来的变化是<br><img src="/2022/10/30/flinkx-1-11-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%A8%8B-jar-%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%84%E7%90%86/image_1666956956724_0.png" alt="image.png"><br>但是这里有个问题，按类路径里的顺序 往后不是也能从 flinkx-release_1.11.0.jar 这个包里找到需要的类么？<br>第二天过来，按照类加载的顺序，看了这两个 jar ：<code>flinkx-hdfs-writer-release_1.11.0.jar</code> 和 <code>flinkx-release_1.11.0.jar</code>，发现问题了</p>
<ol>
<li>不设置 FIRST ，会从 <code>flinkx-hdfs-writer-release_1.11.0.jar</code> 这个 jar 中加载到类，<code>org.apache.commons.cli.Option</code>。 这个类是 commons-cli 1.2 版本的，它的实现中没有 builder 这个方法。</li>
<li>设置了 FIRST，会从 <code>flinkx-release_1.11.0.jar</code> 这个 jar 中加载 Option 类。这个 jar 中的 commons-cli 是 1.3.1 版本的，是有这个方法的。</li>
</ol>
<p>之前为什么会 “挨个尝试，最终发现要在本地 lib 目录中，保留 flink-shaded-hadoop-2-uber-2.6.5-10.0.jar 这个 jar” 也是这个原因，这个 jar 里面的 Option 类是对的。本地 lib 中 jar 会出现在 classpath 的前面，如下，所以会加载到正确的类。</p>
]]></content>
      <tags>
        <tag>flinkx</tag>
      </tags>
  </entry>
</search>
